<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Samuel Bernard" />
  <meta name="author" content="Laurent Pujo-Menjouet" />
  <title>Elements of Maths for Biology</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Elements of Maths for Biology</h1>
<p class="author">Samuel Bernard<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p class="author">Laurent Pujo-Menjouet<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p class="date">V2021.10.15</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#readme">README</a></li>
<li><a href="#fonctions-maps">Fonctions, maps</a>
<ul>
<li><a href="#usual-maps">Usual maps</a></li>
<li><a href="#exercises-on-functions">Exercises on functions</a></li>
</ul></li>
<li><a href="#derivatives">Derivatives</a>
<ul>
<li><a href="#list-of-common-derivatives">List of common derivatives</a></li>
<li><a href="#exercises-on-derivatives">Exercises on derivatives</a></li>
</ul></li>
<li><a href="#taylor-series-and-truncated-expansions">Taylor series and truncated expansions</a>
<ul>
<li><a href="#expansion-in-two-variables">Expansion in two variables</a></li>
<li><a href="#expansion-of-a-function-from-mathbbr2-to-mathbbr2">Expansion of a function from <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span></a></li>
</ul></li>
<li><a href="#integrals-and-primitives">Integrals and primitives</a>
<ul>
<li><a href="#primitives">Primitives</a></li>
<li><a href="#integrals">Integrals</a></li>
</ul></li>
<li><a href="#differential-equations-in-1d">Differential equations in 1D</a>
<ul>
<li><a href="#finding-solutions-of-differential-equations">Finding solutions of differential equations</a></li>
</ul></li>
<li><a href="#complex-numbers">Complex numbers</a>
<ul>
<li><a href="#roots-of-a-complex-number">Roots of a complex number</a></li>
<li><a href="#exercises-on-complex-numbers">Exercises on complex numbers</a></li>
</ul></li>
<li><a href="#matrices-in-dimension-2">Matrices in dimension 2</a>
<ul>
<li><a href="#eigenvalues-of-a-2-times-2-matrix">Eigenvalues of a <span class="math inline">\(2 \times 2\)</span> matrix</a>
<ul>
<li><a href="#exercises-on-eigenvalues">Exercises on eigenvalues</a></li>
</ul></li>
<li><a href="#matrix-vector-operations">Matrix-vector operations </a>
<ul>
<li><a href="#exercises-on-matrix-vector-and-matrix-matrix-operations">Exercises on Matrix-vector and matrix-matrix operations</a></li>
</ul></li>
</ul></li>
<li><a href="#eigenvalue-decomposition">Eigenvalue decomposition</a>
<ul>
<li><a href="#eigenvectors">Eigenvectors</a></li>
<li><a href="#exercises-on-eigenvalues-decomposition">Exercises on eigenvalues decomposition</a></li>
</ul></li>
<li><a href="#linearisation-of-functions-mathbbr2-to-mathbbr2">Linearisation of functions <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span></a>
<ul>
<li><a href="#exercises-on-linearisation">Exercises on linearisation </a></li>
</ul></li>
<li><a href="#solution-of-systems-of-linear-differential-equations-in-dimension-2">Solution of systems of linear differential equations in dimension 2</a></li>
<li><a href="#glossary">Glossary</a></li>
</ul>
</nav>
<h1 id="readme">README</h1>
<p>This document is intended to serve both for training and for future reference. As a reference document, you may find it useful for the first biomaths class (3BS, Fall semester) and for linear algebra (3BIM, Winter Semester).</p>
<p>When important concepts are encoutered for the first time, they highlighted in <strong>bold</strong> next to their definition. Exercises are important, they can introduce theory or techniques that will be prove useful. We tried to make the examples as complete as possible. This means that they are long, you could probably solve them faster.</p>
<h1 id="fonctions-maps">Fonctions, maps</h1>
<p>A <strong>function</strong> is a relation, denoted in general <span class="math inline">\(f\)</span>, that associate an element <span class="math inline">\(x\)</span> belonging to a <strong>domain</strong> <span class="math inline">\(I\)</span>, and at most an element <span class="math inline">\(y\)</span> of the <strong>image</strong> <span class="math inline">\(J\)</span>. The domain <span class="math inline">\(I\)</span> and <span class="math inline">\(J\)</span> are sets, usually <span class="math inline">\(I, J \in \mathbb{R}\)</span>.</p>
<figure>
<img src="htmltmp/tikzblocks_0.pdf.png" id="f_functions" alt="Functions. (A) Function f. (B) Not a function. " /><figcaption aria-hidden="true">Functions. (A) Function <span class="math inline">\(f\)</span>. (B) Not a function. </figcaption>
</figure>
<p>A <strong>map</strong> is a relation that associate <em>each</em> element of its domain to exactly one element of its image. Maps and functions are related but slightly different concepts. A function <span class="math inline">\(f\)</span> is a map if it is defined for all elements of of its domain <span class="math inline">\(I\)</span>. A map is always a function, but the term can also be used when the domain or the image are not numbers (Figure <a href="#f_functions" data-reference-type="ref" data-reference="f_functions">1</a>).</p>
<p>The <strong>graph</strong> of a function <span class="math inline">\(f\)</span>, denoted <span class="math inline">\(\mathcal{G}(f)\)</span> is the set of all pairs <span class="math inline">\((x, f(x))\)</span> in the <span class="math inline">\(I \times J\)</span> plane. For real-valued functions, the graph is represented in the Cartesian plane.</p>
<p>Functions are not numbers. Do not confuse</p>
<ul>
<li><p><span class="math inline">\(f\)</span> the function</p></li>
<li><p><span class="math inline">\(f(x)\)</span> the evaluation of <span class="math inline">\(f\)</span> at element <span class="math inline">\(x\)</span>; <span class="math inline">\(f(x)\)</span> is an element of the image (usually a number)</p></li>
<li><p><span class="math inline">\(\mathcal{G}(f)\)</span> the graph of <span class="math inline">\(f\)</span>.</p></li>
</ul>
<p>Consequently, never write</p>
<ul>
<li><p><span class="math inline">\(f(x)\)</span> is increasing... but write <span class="math inline">\(f\)</span> is increasing...</p></li>
<li><p><span class="math inline">\(f(x)\)</span> is decreasing... but write <span class="math inline">\(f\)</span> is decreasing...</p></li>
<li><p><span class="math inline">\(f(x)\)</span> is continuous... but write <span class="math inline">\(f\)</span> continuous...</p></li>
</ul>
<h2 id="usual-maps">Usual maps</h2>
<ul>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to k\)</span>, <span class="math inline">\(k \in \mathbb{R}\)</span> constant; <span class="math inline">\(x \to x\)</span>, identity map.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_1.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_2.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}\backslash\{0\} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \frac 1x\)</span>, inverse.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_3.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to x^2\)</span>, parabola; <span class="math inline">\(x \to x^3\)</span>, cubic map.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_4.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_5.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}^+ \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \sqrt{x} = x^{\frac 12}\)</span>, square root; more generally with <span class="math inline">\(x \to x^{\frac pq} = {}^q\sqrt{x^p}\)</span>, fractional power.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_6.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_7.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}\backslash\{-d/c\} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \frac{ax + b}{cx + d}\)</span>.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_8.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \exp(x)\)</span>, exponential.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_9.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}^+\backslash\{0\} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \ln(x)\)</span>, natural logarithm.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_10.pdf.png" alt="image" /></p>
</div>
<p>On logarithms: For <span class="math inline">\(a, b &gt;0\)</span>, <span class="math inline">\(n\)</span> positive integer, <span class="math inline">\(\ln(ab) = \ln(a) + \ln(b)\)</span>, <span class="math inline">\(\ln(a^n) = n \ln(a)\)</span>, <span class="math inline">\(\ln(a/b) = \ln(a) - \ln(b)\)</span>.</p></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \cos(x)\)</span>, cosine; <span class="math inline">\(x \to \sin(x)\)</span>, sine; <span class="math inline">\(x \to \tan(x)\)</span>, tangent.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_11.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_12.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_13.pdf.png" alt="image" /></p>
</div>
<p>On trigonometric functions. In the diagram below is shown the relationship between sine, cosine and tangent, of a angle <span class="math inline">\(\theta\)</span>.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_14.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \cosh(x) = \frac 12 \bigl( e^{x} + e^{-x} \bigr)\)</span>, hyperbolic cosine; <span class="math inline">\(x \to \sinh(x) = \frac 12 \bigl( e^{x} - e^{-x} \bigr)\)</span>, hyperbolic sine; <span class="math inline">\(x \to \tanh(x) = \frac{\sinh(x)}{\cosh(x)}\)</span>, hyperbolic tangent.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_15.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_16.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_17.pdf.png" alt="image" /></p>
</div></li>
</ul>
<h2 id="exercises-on-functions">Exercises on functions</h2>
<h1 id="derivatives">Derivatives</h1>
<p>We call the <strong>derivative</strong> of the function <span class="math inline">\(f:I \to J\)</span> (<span class="math inline">\(I,J \subset \mathbb{R}),\)</span>, at point <span class="math inline">\(a \in I\)</span> the limit, if it exists, <span class="math display">\[\lim_{x \to a} \frac{f(x) - f(a)}{x - a}.\]</span> The derivative is denoted <span class="math inline">\(f&#39;(a)\)</span>. An alternative representation of the limit is obtained by setting <span class="math inline">\(h = x - a\)</span>, <span class="math display">\[f&#39;(a) = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h}.\]</span> If the derivative exists for all elements <span class="math inline">\(a \in I\)</span>, we say that <strong>differentiable</strong> on <span class="math inline">\(I\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(f\)</span> is differentiable on <span class="math inline">\(I\)</span>, and <span class="math inline">\(f&#39;(x) &gt; 0\)</span>, then <span class="math inline">\(f\)</span> is strictly increasing on <span class="math inline">\(I\)</span>.</p></li>
<li><p>If <span class="math inline">\(f\)</span> is differentiable on <span class="math inline">\(I\)</span>, and <span class="math inline">\(f&#39;(x) &lt; 0\)</span>, then <span class="math inline">\(f\)</span> is strictly decreasing on <span class="math inline">\(I\)</span>.</p></li>
</ul>
<p>However, if <span class="math inline">\(f\)</span> is strictly increasing, it does not mean that <span class="math inline">\(f&#39;(x) &gt; 0\)</span>. For example the function <span class="math inline">\(f\)</span> with <span class="math inline">\(f(x) = x^3\)</span> is strictly increasing on <span class="math inline">\(\mathbb{R}\)</span>, but <span class="math inline">\(f&#39;(0) = 0\)</span>. Where the derivative exists, we can define the derivative function <span class="math inline">\(f&#39;:I \to \mathbb{R}\)</span> of <span class="math inline">\(f\)</span>.</p>
<p>The <strong>second derivative</strong> of a function <span class="math inline">\(f\)</span>, denoted <span class="math inline">\(f&#39;&#39;\)</span> is the derivative of <span class="math inline">\(f&#39;\)</span>, where defined. If <span class="math inline">\(f&#39;&#39;(x)\)</span> exists and <span class="math inline">\(f&#39;&#39;(x) &gt; 0\)</span> for all <span class="math inline">\(x \in I\)</span>, we say that <span class="math inline">\(f\)</span> is <strong>convex</strong> (U-shaped). If <span class="math inline">\(f&#39;(x) = 0\)</span> and <span class="math inline">\(f&#39;&#39;(x) &gt; 0\)</span>, the point <span class="math inline">\(x\)</span> is a <strong>minimum</strong>. If <span class="math inline">\(f&#39;(x)\)</span> and <span class="math inline">\(f&#39;&#39;(x) &lt; 0\)</span>, the point <span class="math inline">\(x\)</span> is a <strong>maxmimum</strong>. Maxima and minima are <strong>extrema</strong>. If <span class="math inline">\(f&#39;&#39;(0) = 0\)</span>, the point <span class="math inline">\(x\)</span> is an <strong>inflection point</strong> (Figure <a href="#f_extrema" data-reference-type="ref" data-reference="f_extrema">2</a>).</p>
<figure>
<img src="htmltmp/tikzblocks_18.pdf.png" id="f_extrema" alt="Extrema, inflection points of the polynomial f(x) = (x+0.8)(x-0.5)(x-0.8). " /><figcaption aria-hidden="true">Extrema, inflection points of the polynomial <span class="math inline">\(f(x) = (x+0.8)(x-0.5)(x-0.8).\)</span> </figcaption>
</figure>
<h2 id="list-of-common-derivatives">List of common derivatives</h2>
<p>The derivative is linear. If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are differentiable on <span class="math inline">\(I\)</span>, and <span class="math inline">\(a \in \mathbb{R}\)</span>,</p>
<ul>
<li><p><span class="math inline">\((f + g)&#39; = f&#39; + g&#39;\)</span>.</p></li>
<li><p><span class="math inline">\((af)&#39; = a (f&#39;)\)</span>.</p></li>
<li><p><span class="math inline">\((af + g)&#39;  = a (f&#39;) + g&#39;\)</span>.</p></li>
</ul>
<p>The derivative follow the <strong>rule of composed functions</strong>. If <span class="math inline">\(g:I \to J\)</span> and <span class="math inline">\(f:J \to K\)</span>, then <span class="math inline">\(f \circ g\)</span> is function <span class="math inline">\(x \to f(g(x))\)</span>. If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are differentiable, the derivative <span class="math display">\[\bigl( f \circ g \bigr)&#39;(x) = f&#39;(g(x))g&#39;(x).\]</span></p>
<p><strong>Example </strong> Let <span class="math inline">\(f:x \to x^2\)</span> and <span class="math inline">\(g:x \to 3x + 1\)</span>, two differentiable functions, with <span class="math inline">\(f&#39;(x) = 2x\)</span> and <span class="math inline">\(g&#39;(x) = 3\)</span>. The derivative of the composed function <span class="math inline">\(f \circ g\)</span> at <span class="math inline">\(x\)</span> is <span class="math display">\[f&#39;(g(x))g&#39;(x) = f&#39;(3x+1)g&#39;(x) = 2(3x+1) \cdot 3 = 6(3x+1) = 18x + 6.\]</span> The derivative could have been obtained by computing the composed function <span class="math inline">\(f(g(x)) = (3x+1)^2 =
9 x^2 + 6x + 1\)</span>.</p>
<p><strong>Example </strong> Compute the derivative of <span class="math inline">\(f: x \to \sin(1/x)\)</span>. The function <span class="math inline">\(f\)</span> is composed of a sine and an inverse function. To compute the derivative, we decomposed the function <span class="math inline">\(f\)</span> as <span class="math inline">\(f(x) = g(h(x))\)</span> with <span class="math inline">\(g(x) = \sin(x)\)</span> and <span class="math inline">\(h(x) = 1/x\)</span>. The derivatives <span class="math inline">\(g&#39;(x) = \cos(x)\)</span> and <span class="math inline">\(h&#39;(x) = -1/x^2\)</span>. <span class="math display">\[f&#39;(x) = g&#39;(h(x))h&#39;(x) = \cos(1/x) \Bigl( \frac{-1}{x^2} \Bigr) = - \frac{\cos(1/x)}{x^2}.\]</span></p>
<p><strong>Example </strong> A function <span class="math inline">\(f\)</span> is bijective (invertible) if there exists a function, denoted <span class="math inline">\(f^{-1}\)</span>, such that <span class="math inline">\(f \circ f^{-1} = f^{-1} \circ f\)</span> is the identity map. If <span class="math inline">\(f\)</span> is differentiable and invertible, what is the derivative of <span class="math inline">\(f^{-1}\)</span>?</p>
<p>We apply the derivative to <span class="math inline">\(f(f^{-1})\)</span>. Given that <span class="math inline">\(f(f^{-1}(x)) = x\)</span> by definition, we have <span class="math inline">\(\bigl( f(f^{-1}) \bigr)&#39; =  1,\)</span> and <span class="math display">\[\begin{aligned}
  \bigl( f(f^{-1}) \bigr)&#39;(x) &amp; = f&#39;(f^{-1}(x))(f^{-1})&#39;(x), \\
                              &amp; = 1, \\
            (f^{-1})&#39;(x)      &amp; = \frac{1}{f&#39;(f^{-1}(x))}.\end{aligned}\]</span> Take for instance <span class="math inline">\(f(x) = x^2\)</span> on <span class="math inline">\(x \in (0,1]\)</span>. The inverse is <span class="math inline">\(f^{-1}(x) = \sqrt{x}\)</span>. The derivative of <span class="math inline">\(f\)</span> is <span class="math inline">\(f(x) = 2x\)</span> and the derivative <span class="math display">\[f^{-1}(x) = \frac{1}{f&#39;(f^{-1}(x))} = \frac{1}{2(\sqrt{x})}.\]</span></p>
<p><strong>Example </strong> Derivative when the independant variable is in the exponent. To compute the derivative of <span class="math inline">\(f: x \to 2^x\)</span>, we need to re-express the function in terms fo the natural base <span class="math inline">\(e\)</span>. To do that, we use to properties of the natural logarithm</p>
<ul>
<li><p>For any nonzero expression <span class="math inline">\(y\)</span>, <span class="math inline">\(y = e^{ \ln y }\)</span> (<span class="math inline">\(\ln\)</span> is the inverse of the exponential function).</p></li>
<li><p><span class="math inline">\(\ln (a^b) = b \ln a.\)</span></p></li>
</ul>
<p>Then <span class="math inline">\(2^x = e^{ \ln (2^x) } = e^{ x \ln 2 }\)</span>. The derivative is <span class="math inline">\(\ln 2 e^{ x \ln 2 }\)</span>. Re-writing in term of base 2: <span class="math inline">\(\ln 2 2^x\)</span>.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Derivative</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(x^a\)</span></td>
<td style="text-align: left;"><span class="math inline">\(ax^{a-1}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(a \in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\frac{1}{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{-1}{x^2}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(x^{\frac 12}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{2x^{\frac 12}}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\ln(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac 1x\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\cosh(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sinh(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\sinh(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\cosh(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\cos(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(-\sin(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\sin(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\cos(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\tan(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(1 + \tan^2(x) = \frac{1}{\cos^2(x)}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\dfrac{u(x)}{v(x)}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dfrac{v(x)u&#39;(x) - u(x)v&#39;(x)}{v^2(x)}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(u(x) v(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(u&#39;(x)v(x) + u(x)v&#39;(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h2 id="exercises-on-derivatives">Exercises on derivatives</h2>
<p><strong>Exercice </strong> Compute the derivatives of the following functions</p>
<ul>
<li><p><span class="math inline">\(f_1 : x \to \sqrt{\cos x}.\)</span></p></li>
<li><p><span class="math inline">\(f_2 : x \to \sin(3x + 2).\)</span></p></li>
<li><p><span class="math inline">\(f_3 : x \to e^{\cos x}.\)</span></p></li>
<li><p><span class="math inline">\(f_4 : x \to \ln\bigl(\sqrt{x} \bigr).\)</span></p></li>
<li><p><span class="math inline">\(f_5 : x \to 2^{\ln x}.\)</span></p></li>
</ul>
<h1 id="taylor-series-and-truncated-expansions">Taylor series and truncated expansions</h1>
<p>If a function <span class="math inline">\(f\)</span> is infinitely differentiable (i.e. <span class="math inline">\(f^{(k)}\)</span> is continuous for any integer <span class="math inline">\(k\geq 0\)</span>, the Taylor series of <span class="math inline">\(f\)</span> at point <span class="math inline">\(a\)</span> is the series <span class="math display">\[\begin{aligned}
 \label{eq_taylor}
  f(a) + \frac{f&#39;(a)}{1!}(x - a) + \frac{f&#39;&#39;(a)}{2!} (x-a)^2 + \frac{f&#39;&#39;&#39;(a)}{3!} (x - a)^3 + ...\end{aligned}\]</span></p>
<p>A function is <strong>analytic</strong> on an open interval <span class="math inline">\(I\)</span> if and only if its Taylor series converges pointwise to the value of the function. Polynomials, exponential and trigonometric function are analytic over all real points. The square root function is not.</p>
<p>The partial sums of the series (or truncated expansion) can be used to approximate a function, and to evaluate it numerically. The <span class="math inline">\(k\)</span>th-order expansion of a function <span class="math inline">\(f\)</span> is the polynomial <span class="math display">\[\begin{aligned}
 \label{eq_taylor}
  f(a) + \frac{f&#39;(a)}{1!}(x - a) + \frac{f&#39;&#39;(a)}{2!} (x-a)^2 +  ... + \frac{f^{(k)}(a)}{k!} (x - a)^k.\end{aligned}\]</span></p>
<p><strong>Example </strong> The Taylor series of the sine function at point <span class="math inline">\(a = 0\)</span> is <span class="math display">\[\sum_{n=0}^\infty \frac{1}{n!}\frac{d^n\sin(x)}{dx^n} x^n = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + ...\]</span> The 3rd-order expansion is the cubic polynomial <span class="math display">\[x - \frac{x^3}{3!}.\]</span> How good this cubic polynomial approximation to the original sine function? The error is the remainder of the terms of the Taylor series <span class="math display">\[\begin{aligned}
  \Bigl| - \frac{x^7}{7!} + \frac{x^9}{9!} - ... \Bigr|. \end{aligned}\]</span> For <span class="math inline">\(|x| &lt; 1\)</span>, the error is bounded by <span class="math inline">\(|x|^7/7!\)</span>. Given that <span class="math inline">\(7! = 5040\)</span>, the error is less than <span class="math inline">\(1/5040 \approx 0.0002\)</span>. Truncated expansions are never good approximations when <span class="math inline">\(|x|\)</span> becomes large, because polynomials are not bounded, but the approximation can be quite good over small intervals around the point at which the Taylor series is computed.</p>
<figure>
<img src="htmltmp/tikzblocks_19.pdf.png" alt="First, third and fifth order expansion of f(x) = \sin(x)." /><figcaption aria-hidden="true">First, third and fifth order expansion of <span class="math inline">\(f(x) = \sin(x)\)</span>.</figcaption>
</figure>
<p>It is important to note that even if a function has a Taylor series, the series may not converge to the function.</p>
<p><strong>Example </strong> Let the function <span class="math display">\[f(x) = \begin{cases} 
              0  &amp; x \leq 0 \\
              e^{-\frac 1x} &amp; x &gt; 0.
          \end{cases}\]</span> Around 0, all the derivatives exist and are equal to 0. The Taylor series of <span class="math inline">\(f\)</span> at point <span class="math inline">\(x = 0\)</span> is 0, but the function is different from zero.</p>
<h2 id="expansion-in-two-variables">Expansion in two variables</h2>
<p>For functions of two variables, the Taylor expansion at point <span class="math inline">\((x_0,y_0)^t \in \mathbb{R}^2\)</span> is <span class="math display">\[\begin{aligned}
  f(x_0,y_0) = f(x_0,y_0) &amp; + \frac{\partial f(x_0,y_0)}{\partial x} (x - x_0) + \frac{\partial f(x_0,y_0)}{\partial y} (y - y_0) \\
 &amp;  + \frac{1}{2!} \frac{\partial^2 f(x_0,y_0)}{\partial x^2} (x - x_0)^2 \\
 &amp;  + \frac{1}{2!} \frac{\partial^2 f(x_0,y_0)}{\partial y^2} (y - y_0)^2 \\
 &amp;  + \frac{2}{2!} \frac{\partial^2 f(x_0,y_0)}{\partial x \partial y} (x - x_0) (y - y_0)
   + ...\end{aligned}\]</span></p>
<p><strong>Example </strong> The second-order truncated expansion of the function <span class="math inline">\(f(x,y) = y e^{-x}\)</span> at <span class="math inline">\((0,0)^t\)</span> is <span class="math display">\[y - xy\]</span></p>
<h2 id="expansion-of-a-function-from-mathbbr2-to-mathbbr2">Expansion of a function from <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span></h2>
<p>Functions <span class="math inline">\(f: \mathbb{R}^2 \to \mathbb{R}^2\)</span> have the form <span class="math display">\[f(x,y) = \begin{pmatrix} f_1(x,y) \\ f_2(x,y) \end{pmatrix}.\]</span> To compute the Taylor series, we need to compute the Taylor series of each function <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>. For a first-order expansion at <span class="math inline">\((x_0,y_0)^t\)</span>, we obtain the expansion <span class="math display">\[\begin{aligned}
  \begin{pmatrix} f_1(x_0,y_0) \\ f_2(x_0,y_0) \end{pmatrix} + 
    \begin{pmatrix}
  \frac{\partial f_1(x_0,y_0)}{\partial x} (x - x_0) + \frac{\partial f_1(x_0,y_0)}{\partial y} (y - y_0) \\
  \frac{\partial f_2(x_0,y_0)}{\partial x} (x - x_0) + \frac{\partial f_2(x_0,y_0)}{\partial y} (y - y_0)
    \end{pmatrix}\end{aligned}\]</span> Using vector notation <span class="math display">\[\begin{aligned}
  \boldsymbol{f}(\boldsymbol{x}) &amp; = \begin{pmatrix} f_1(x,y) \\ f_2(x,y) \end{pmatrix}, \\
    D\boldsymbol{f} &amp; =   
    \begin{pmatrix}
      \frac{\partial f_1}{\partial x}  &amp; \frac{\partial f_1}{\partial y}  \\
      \frac{\partial f_2}{\partial x} &amp; \frac{\partial f_2}{\partial y} 
    \end{pmatrix},\end{aligned}\]</span> we can write a first-order (or linear) approximation of a function <span class="math inline">\(\boldsymbol{f}\)</span> compactly <span class="math display">\[\boldsymbol{f}(\boldsymbol{x}) \approx \boldsymbol{f}(\boldsymbol{a}) + D\boldsymbol{f}(\boldsymbol{a}) (\boldsymbol{x} - \boldsymbol{a}).\]</span> The approximation is valid only in a neighbourhood of the point <span class="math inline">\(\boldsymbol{a}\)</span>.</p>
<h1 id="integrals-and-primitives">Integrals and primitives</h1>
<h2 id="primitives">Primitives</h2>
<p>Let <span class="math inline">\(f:I \to \mathbb{R}\)</span>, (<span class="math inline">\(I \subset \mathbb{R}\)</span>). A <strong>primitive</strong> <span class="math inline">\(F\)</span> of <span class="math inline">\(f\)</span> on <span class="math inline">\(I\)</span> is a differentiable map such that <span class="math inline">\(F&#39;(x) = f(x)\)</span>, <span class="math inline">\(x \in I\)</span>.</p>
<p><strong>Example </strong> If <span class="math inline">\(f(x) = x^2\)</span>, we look for <span class="math inline">\(F(x)\)</span> such that <span class="math inline">\(F&#39;(x) = x^2\)</span>. Take <span class="math inline">\(F(x) = x^3/3\)</span>, then <span class="math inline">\(F&#39;(x) = 3x^2/3 = x^2.\)</span> It also work for <span class="math inline">\(F(x) = x^3/3 + C\)</span>, for any constant <span class="math inline">\(C \in \mathbb{R}\)</span>.</p>
<p>We denote the primitive as <span class="math inline">\(F(x) = \int f(x) dx\)</span>. Primitives are unique up to a constant: if <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span> are primitives of <span class="math inline">\(f\)</span>, then <span class="math inline">\(F_1&#39;(x) = f(x)\)</span> and <span class="math inline">\(F_2&#39;(x) = f(x)\)</span>, which implies that <span class="math inline">\(F_1&#39;(x) - F_2&#39;(x) = 0\)</span>. Therefore the difference between <span class="math inline">\(F_1\)</span> and <span class="math inline">\(F_2\)</span>, <span class="math inline">\(G : x \to F_1(x) - F_2(x)\)</span> has a zero derivative: <span class="math inline">\(G&#39;(x) = F_1&#39;(x) - F_2&#39;(x) = 0\)</span>. This means that <span class="math inline">\(G\)</span> is a constant.</p>
<p>If <span class="math inline">\(F\)</span> is a primitive of <span class="math inline">\(f\)</span> and <span class="math inline">\(G\)</span> a primitive of <span class="math inline">\(g\)</span>, then <span class="math inline">\(F+G\)</span> is a primitive of <span class="math inline">\(f+g\)</span>.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Primitive</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(0\)</span></td>
<td style="text-align: left;"><span class="math inline">\(C\)</span></td>
<td style="text-align: left;"><span class="math inline">\(C \in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(a\)</span></td>
<td style="text-align: left;"><span class="math inline">\(ax + C\)</span></td>
<td style="text-align: left;"><span class="math inline">\(a \in \mathbb{R}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(x^a\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{x^{a+1}}{a+1}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(a \neq -1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(x^{-1} = \frac{1}{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\ln |x| + C\)</span></td>
<td style="text-align: left;">(<span class="math inline">\(a = -1\)</span>), <span class="math inline">\(x \neq 0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^x + C\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\cos(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sin(x) + C\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\sin(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(-\cos(x) + C\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(f&#39;(g(x))g&#39;(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(f(g(x)) + C\)</span></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p><strong>Exercice </strong> Compute <span class="math display">\[\begin{aligned}
  \int { \frac{1}{\sqrt{3x + 5}} } dx\end{aligned}\]</span></p>
<h2 id="integrals">Integrals</h2>
<p>Let <span class="math inline">\(f:[a,b] \to \mathbb{R}\)</span>, and <span class="math inline">\(F\)</span> a primitive of <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>. Then the <strong>definite integral</strong> is the value <span class="math inline">\(F(b) - F(a)\)</span>, and is denoted <span class="math display">\[\begin{aligned}
  \int_a^b { f(x) dx } = \bigl[ F(x) \bigr]_a^b . \end{aligned}\]</span></p>
<p>The most important interpretation of the integral is the <strong>area under the curve</strong> (AUC). If <span class="math inline">\(f(x) \geq 0\)</span>, <span class="math inline">\(x \in [a,b]\)</span>, then <span class="math display">\[\int_a^b { f(x) dx }\]</span> is the area under delimited by <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(x\)</span>-axis and the axes <span class="math inline">\(x = a\)</span> and <span class="math inline">\(x = b\)</span>. Negative values integrate as negative areas.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_20.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_21.pdf.png" alt="image" /></p>
</div>
<p>If <span class="math inline">\(f\)</span> is a rate (unit in <span><code>something per time</code></span>), and <span class="math inline">\(x\)</span> is <span><code>time</code></span>, then the integral of <span class="math inline">\(f\)</span> has unit <span><code>something</code></span>. This applies to speed (integral: displacement), production or degradation (integral: concentration), etc.</p>
<p>The integral has the following properties</p>
<ul>
<li><p><strong>linearity</strong>. The integral of a sum is the sum of the integrals. <span class="math display">\[\int_a^b { (f(x) + g(x))dx } = \int_a^b f(x) dx + \int_a^b g(x) dx.\]</span></p></li>
<li><p>Negative intervals <span class="math display">\[\int_a^b { f(x) dx } = - \int_b^a { f(x) dx }.\]</span></p></li>
<li><p>Midpoint rule <span class="math display">\[\int_a^b { f(x)dx } = \int_a^c f(x)dx + \int_c^b f(x) dx.\]</span> Notice that this works even if <span class="math inline">\(c\)</span> is not between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p></li>
<li><p>The expression <span class="math display">\[\int_a^x { f(t) dt } = F(x) - F(a) = F_a(x)\]</span> is a function of <span class="math inline">\(x\)</span>, and is the primitive of <span class="math inline">\(f\)</span> that takes the value 0 at <span class="math inline">\(x=a\)</span> (remember that a primitive <span class="math inline">\(F\)</span> has an arbitrary constant, so <span class="math inline">\(F(a)\)</span> can be set to any value). Therefore, for any integrable function <span class="math inline">\(f\)</span> <span class="math display">\[\Bigl( \int_a^x f(x) dx \Bigl)&#39; = f(x).\]</span> This is the <strong>fundamental theorem of calculus</strong>.</p></li>
<li><p><strong>integration by parts</strong> <span class="math display">\[\int_a^b f&#39;(x)g(x)dx = f(x)g(x)|_a^b - \int_a^b f(x)g&#39;(x)dx\]</span></p></li>
</ul>
<p><strong>Exercice </strong> Compute the integral <span class="math display">\[\int_0^1 x e^x dx\]</span></p>
<p><strong>Exercice </strong> Compute the integral <span class="math display">\[\int_0^\pi \cos x  e^x dx\]</span></p>
<p><strong>Exercice </strong> Compute the integral <span class="math display">\[\int_0^1 {  \sinh x e^x dx }\]</span></p>
<h1 id="differential-equations-in-1d">Differential equations in 1D</h1>
<p>An <strong>ordinary differential equation</strong> (ODE) of <strong>order</strong> <span class="math inline">\(n\)</span> is a relation (i.e. an equation) between a real variable <span class="math inline">\(t \in I \subset \mathbb{R}\)</span>, an unknown function <span class="math inline">\(x: t \to x(t)\)</span> and its derivatives <span class="math inline">\(x&#39;,x&#39;&#39;,x&#39;&#39;&#39;, ..., x^{(n)}\)</span> at point <span class="math inline">\(t\)</span> defined by <span class="math display">\[F\bigl(t,x,x&#39;,x&#39;&#39;,...,x^{(n)}\bigr) = 0,\]</span> where <span class="math inline">\(F\)</span> <em>depends</em> on <span class="math inline">\(x^{(n)}\)</span>. In general, <span class="math inline">\(x(t)\)</span> takes values in <span class="math inline">\(\mathbb{R}^N\)</span>, i.e. it is a vector. We say that the equation is a <strong>scalar differential equation</strong> if <span class="math inline">\(N = 1\)</span>. (The expression <span class="math inline">\(x^{(i)}\)</span> stands for the <span class="math inline">\(i\)</span>-th derivative, not the <span class="math inline">\(i\)</span>-th power.)</p>
<p>The <strong>normal form</strong> of a differential equation of order <span class="math inline">\(n\)</span> is <span class="math display">\[x^{(n)} = f\bigl(t,x,x&#39;,x&#39;&#39;,...,x^{(n-1)}\bigr).\]</span></p>
<p>A differential equation is <strong>autonomous</strong> if it does not depend on <span class="math inline">\(t\)</span>, i.e. <span class="math inline">\(F\)</span> has the form <span class="math display">\[F\bigl(x,x&#39;,x&#39;&#39;,...,x^{(n)}\bigr) = 0,\]</span></p>
<p>A <strong>linear differential equation</strong> is a differential equation for which <span class="math inline">\(F\)</span> is a linear function in <span class="math inline">\(x,x&#39;,...x^{(n)}\)</span>. It can be expressed as <span class="math display">\[a_n(t) x^{(n)} + a_{n-1}(t)x^{(n-1)} + ... a_1(t)x&#39; + a_0(t)x = g(t),\]</span> where the coefficients <span class="math inline">\(a_j(t)\)</span> may depend on <span class="math inline">\(t\)</span>, and <span class="math inline">\(x\)</span> and its derivatives <span class="math inline">\(x^{(i)}\)</span> appear only as monones of degree 1 (that is, linearly). If all coefficients are constants, including <span class="math inline">\(g\)</span>, the linear differential equation is autonomous.</p>
<p><strong>Exercice </strong> For the following differential equations, give the order <span class="math inline">\(n\)</span>, and determine whether they are autonomous, linear, and whether they are expressed under their normal form.</p>
<ol>
<li><p><span class="math inline">\(x - t + 4 t x&#39; = 0.\)</span></p></li>
<li><p><span class="math inline">\(\bigl( x&#39;&#39; \bigr)^2 - 2 x&#39; t x = 0.\)</span></p></li>
<li><p><span class="math inline">\(x^{(3)} + \sin(x&#39;) = -5 x.\)</span></p></li>
<li><p><span class="math inline">\(x^{(4)} - x    x&#39;  =  0  .\)</span></p></li>
<li><p><span class="math inline">\(3x&#39;&#39; - 4 x&#39; + 6 x = 2.\)</span></p></li>
<li><p><span class="math inline">\(\ln x&#39; + 3 t x = \sqrt{x}.\)</span></p></li>
</ol>
<p>A <strong>solution or integral of a differential equation</strong> of order <span class="math inline">\(n\)</span> for <span class="math inline">\(t\)</span> in an interval <span class="math inline">\(I \subset \mathbb{R}\)</span>, is a map <span class="math inline">\(x: I -&gt; \mathbb{R}\)</span> that is <span class="math inline">\(n\)</span> times differentiable for all <span class="math inline">\(t \in I\)</span> and satisfies the differential equation.</p>
<p>A <strong>integral curve (or chronic)</strong> is the set of points <span class="math inline">\((t,x(t))\)</span> for <span class="math inline">\(t \in I\)</span>. If <span class="math inline">\(x(t) \in \mathbb{R}^N\)</span>, the integral curve is in <span class="math inline">\(\mathbb{R}^{N+1}\)</span>. A <strong>trajectory or orbit</strong> is the set of points <span class="math inline">\(x(t)\)</span> for <span class="math inline">\(t \in I\)</span>. This is a set in <span class="math inline">\(\mathbb{R}^N\)</span>. The space that contains the trajectories is called the <strong>phase space</strong>. The set of all trajectories is called the <strong>phase portrait</strong>.</p>
<p>It is always possible to write an differential equation of order <span class="math inline">\(n\)</span> as a differential equation of order 1, by defining extra variables for the higher-order derivatives.</p>
<p><strong>Example </strong> We consider the differential equation <span class="math inline">\(a(t) x&#39;&#39; + b(t) x&#39; + c(t) x = d(t)\)</span>. This is a equation of order 2. To reduce it to order 1, let <span class="math inline">\(z_1 = x\)</span> and <span class="math inline">\(z_2 = x&#39;\)</span>. Then <span class="math inline">\(x&#39;&#39; = z_2&#39;\)</span> and <span class="math inline">\(z_1&#39; = z_2\)</span>. The second-order differential equation can be re-expressed as two first order equations: <span class="math display">\[z_1&#39; = z_2, \quad a(t)z_2&#39; + b(t)z_2 + c(t) z_1 = d(t).\]</span> Often, the system of first order equations can be re-expressed in normal form, by isolating the variables <span class="math inline">\(z_1&#39;\)</span> and <span class="math inline">\(z_2&#39;\)</span>, <span class="math display">\[z_1&#39; = z_2, \text{ and } z_2&#39; = \frac{d(t) - b(t)z_2 - c(t) z}{a(t)}.\]</span> (We assume that <span class="math inline">\(a(t) \neq 0\)</span>.) In general, for the differential equation of order <span class="math inline">\(n\)</span> <span class="math inline">\(F(t,x,x&#39;,...,x^{(n)}) = 0,\)</span> with <span class="math inline">\(x : I \to \mathbb{R}^m\)</span>, we make a change of variables: <span class="math inline">\(z_1 = x, z_2 = x&#39;, ..., 
z_i = x^{(i-1)}\)</span> until <span class="math inline">\(z_n = x^{(n-1)}\)</span>. Each variable <span class="math inline">\(z_i(t) \in \mathbb{R}^m\)</span>, so the new vector <span class="math inline">\(z = \bigl( z_1, z_2, ..., z_n \bigr)^t\)</span> is in <span class="math inline">\(\mathbb{R}^{mn}\)</span>. With the change of variables, the differential equation now reads <span class="math display">\[\begin{aligned}
  z_1&#39; &amp; = z_2, \\
  z_2&#39; &amp; = z_3, \\
       &amp; ..., \\
  z_i&#39; &amp; = z_{i+1}, \\
       &amp; ..., \\
  F(t,&amp;z_1,z_2,...,z_n,z_n&#39;) = 0. \\\end{aligned}\]</span></p>
<hr />
<p><strong>Tips on Ordinary differential equations</strong></p>
<ul>
<li><p>The most frequently used differential equations are order 1, and they usually are represented in their normal form: <span class="math inline">\(x&#39; = f(x)\)</span> for autonomous equations, and <span class="math inline">\(x&#39; = f(t,x)\)</span> for non-autonomous equations</p></li>
<li><p>For a scalar, autonomous differential equation <span class="math inline">\(x = f(x)\)</span> with <span class="math inline">\(x(t) \in \mathbb{R}\)</span>, the trajectories are monotonous: if <span class="math inline">\(x\)</span> is a solution, then <span class="math inline">\(x\)</span> is either increasing, decreasing, or constant.</p></li>
</ul>
<hr />
<h2 id="finding-solutions-of-differential-equations">Finding solutions of differential equations</h2>
<p>We consider a <strong>first order scalar differential equation</strong>, <span class="math display">\[a(t) x&#39; + b(t) x = d(t),\]</span> <span class="math inline">\(t \in I\)</span> and <span class="math inline">\(a(t) \neq 0\)</span> on <span class="math inline">\(I\)</span>, <span class="math inline">\(a(t)\)</span> and <span class="math inline">\(b(t)\)</span> continuous on <span class="math inline">\(I\)</span>. If <span class="math inline">\(d(t) = 0\)</span>, we call the equation homogeneous, and <span class="math display">\[a(t) x&#39; + b(t) x = 0.\]</span></p>
<p><em>Homogeneous case, first method.</em> Write the equation in normal form, <span class="math display">\[x&#39; - \frac{b(t)}{a(t)} x.\]</span> The solution <span class="math inline">\(x\)</span> is either the constant <span class="math inline">\(x = 0\)</span>, or <span class="math inline">\(x(t) \neq 0\)</span> for all <span class="math inline">\(t\in I\)</span>. We know that because of uniqueness of solutions, which implies that trajectories cannot cross. If <span class="math inline">\(x = 0\)</span> we are done, so we can assume that <span class="math inline">\(x(t) \neq 0\)</span>. Dividing the equation by <span class="math inline">\(x\)</span> <span class="math display">\[\frac{x&#39;}{x} = -\frac{b(t)}{a(t)}.\]</span> The terms on both sides are functions of <span class="math inline">\(t\)</span>. We can compute their primitives <span class="math display">\[\int {\frac{x&#39;}{x} dt } = - \int { \frac{b(t)}{a(t)} dt}.\]</span> The integrand of the left-hand side is <span class="math inline">\(x&#39;/x\)</span>. This is a very common form called <strong>log-derivative</strong> and admit a primitve <span class="math inline">\(\ln |x|\)</span>. The right-hand side does not necessarily have a close form, and we leave it as it is. With the inegration constant, we have <span class="math display">\[\ln |x| = - \int { \frac{b(t)}{a(t)} dt } + K.\]</span> We would like an explicit solution <span class="math inline">\(x\)</span>, <span class="math display">\[\begin{aligned}
  |x| &amp; = e^{- \int { \frac{b(t)}{a(t)} dt } + K}.\end{aligned}\]</span> Therefore, <span class="math display">\[\begin{aligned}
  x &amp; = \pm e^K e^{- \int { \frac{b(t)}{a(t)} dt }}.\end{aligned}\]</span> Notice that <span class="math inline">\(t\)</span> is a variable of integration, and does not exist outside the integral, and can be replaced by any other variable. To have <span class="math inline">\(x\)</span> as as function of <span class="math inline">\(t\)</span>, we must define the bounds of the integral. When the domain of definition of <span class="math inline">\(t\)</span> is the interval <span class="math inline">\(I = [t_0,t_1], t_0 &lt; t_1,\)</span> the solution <span class="math inline">\(x(t)\)</span> is obtained by integrating from <span class="math inline">\(t_0\)</span> to <span class="math inline">\(t\)</span>, <span class="math display">\[x = \pm e^K e^{- \int_{t_0}^t { \frac{b(u)}{a(u)} du }},\]</span> where we have replaced the variable of integration by <span class="math inline">\(u\)</span>. Then the integral is function of <span class="math inline">\(t\)</span>. The constant <span class="math inline">\(K\)</span> is determined by the <strong>initial condition</strong> on <span class="math inline">\(x\)</span> at <span class="math inline">\(t_0\)</span>, <span class="math inline">\(x(t_0) = x_0 \in \mathbb{R},\)</span> <span class="math display">\[x(t_0) = \pm e^K e^{- \int_{t_0}^{t_0} { \frac{b(u)}{a(u)} du }} = \pm e^K = x_0.\]</span> The constant <span class="math inline">\(K = \mathrm{sign} \, x_0  \ln x_0.\)</span> The fixed bound at <span class="math inline">\(t_0\)</span> is arbitrary, we could have chosen any other time of reference. However, it is very common to consider differential equations for which we know the value of the solution at the initial time <span class="math inline">\(t_0\)</span>. The problem of solving a differential equation with a given initial condition is called <strong>initial value problem</strong> (IVP) or <strong>Cauchy problem</strong>.</p>
<p><em>Homogeneous case, second method.</em> Assume that <span class="math inline">\(a(t) \neq 0\)</span>, and write the equation as <span class="math display">\[x&#39; + \frac{b(t)}{a(t)} x = 0.\]</span> Multiply the equation by the term <span class="math inline">\(e^{\int { \frac{b(t)}{a(t)} dt }}\)</span>, <span class="math display">\[x&#39; e^{\int { \frac{b(t)}{a(t)} dt }} + \frac{b(t)}{a(t)} x e^{\int { \frac{b(t)}{a(t)} dt }} = 0.\]</span> Notice that the first term has the form <span class="math inline">\(x&#39; f\)</span> and the second term the form <span class="math inline">\(x f&#39;\)</span>, with <span class="math display">\[f = e^{\int { \frac{b(t)}{a(t)} dt }}.\]</span> The left-hand-side of the resulting equation, <span class="math inline">\(x&#39; f + x f&#39;\)</span>, is the derivative of the product <span class="math inline">\(xf\)</span>, so the differential equation can be integrated, <span class="math display">\[\begin{aligned}
  x e^{\int { \frac{b(t)}{a(t)} dt }} &amp; = K, \\
  x &amp; = K e^{ - \int { \frac{b(t)}{a(t)} dt }}.\end{aligned}\]</span> The constant <span class="math inline">\(K\)</span> is determined by a condition set on the solution, as in the first method.</p>
<p><strong>Exercice </strong> Solve the equation <span class="math display">\[2x&#39; + 6x = 0, \quad x(0) = 1.\]</span></p>
<p><strong>Exercice </strong> Solve the equation <span class="math display">\[x&#39; + \frac 1t x = 0, \quad x(1) = 1.\]</span></p>
<p><em>Heterogenous case.</em> We now consider the more general differential equation <span class="math display">\[a(t) x&#39; + b(t) x = d(t).\]</span> Using the strategy from the second method for the homogeneous case above, we divide the equation by <span class="math inline">\(a(t)\)</span>, again assuming that <span class="math inline">\(a(t) \neq 0\)</span>, and then multiply by <span class="math inline">\(e^{ \int { \frac{b(t)}{a(t)} dt} }.\)</span> The resulting equation now has a non-zero right-hand-side, <span class="math display">\[x&#39;e^{ \int { \frac{b(t)}{a(t)} dt} } + \frac{b(t)}{a(t)} x e^{ \int { \frac{b(t)}{a(t)} dt} } = \frac{d(t)}{a(t)} e^{ \int { \frac{b(t)}{a(t)} dt} }.\]</span> Nevertheless, the left-hand-side is still of the form <span class="math inline">\(x&#39; f + x f&#39;\)</span>, and the right-hand-side only depends on <span class="math inline">\(t\)</span>. By integrating both sides, we obtain <span class="math display">\[x e^{ \int { \frac{b(t)}{a(t)} dt} } = \int { \frac{d(t)}{a(t)} e^{ \int { \frac{b(t)}{a(t)} dt}} dt } + K.\]</span> The solution for <span class="math inline">\(x\)</span> is then <span class="math display">\[x = \Bigl( \int { \frac{d(t)}{a(t)} e^{ \int { \frac{b(t)}{a(t)} dt}} dt } + K \Bigr) e^{ - \int { \frac{b(t)}{a(t)} dt} }.\]</span></p>
<p><strong>Example </strong> We consider the differential equation <span class="math display">\[\begin{aligned}
  x&#39; + 3 x = 1+\sin(t), \quad x(0) = x_0 &gt; 0. \label{eq_hetero}\end{aligned}\]</span> The coefficients <span class="math inline">\(a(t) = 1, b(t) = 3, d(t) = 1 + \sin(t).\)</span> The term <span class="math inline">\(e^{ \int { \frac{b(t)}{a(t)} dt}} = e^{ \int { 3 dt } } = e^{3t}\)</span>. The general solution is <span class="math display">\[\begin{aligned}
  x(t)  &amp; = \Bigl( \int { (1+\sin(t)) e^{3t} dt } + K \Bigr)e^{ - 3t } , \\
        &amp; = \frac 13 + \frac {1}{10} \bigl( \sin(t) - \cos(t) \bigr) + Ke^{-3t}. \end{aligned}\]</span> At <span class="math inline">\(t = 0\)</span>, the equation <span class="math inline">\(x(0) = \frac 13 + \frac{1}{10} (0 - 1) + K = x_0\)</span> solve in <span class="math inline">\(K\)</span> as <span class="math inline">\(K = x_0 - \frac{7}{30}\)</span>. The solution <span class="math inline">\(x(t)\)</span> is <span class="math display">\[x(t) = \frac 13 + \frac {1}{10} \bigl( 3 \sin(t) - \cos(t) \bigr) + \Bigl( x_0 - \frac{7}{30} \Bigr) e^{-3t}.\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_22.pdf.png" alt="Solution of the initial value problem for the heterogenous differential equation ([eq_hetero])." /><figcaption aria-hidden="true">Solution of the initial value problem for the heterogenous differential equation (<a href="#eq_hetero" data-reference-type="ref" data-reference="eq_hetero">[eq_hetero]</a>).</figcaption>
</figure>
<h1 id="complex-numbers">Complex numbers</h1>
<p>A complex number is a number that can be expressed in the form <span class="math inline">\(a + i b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are real numbers, and the symbol <span class="math inline">\(i\)</span> is called <strong>imaginary unit</strong>. The imaginary unit satisfies the equation <span class="math inline">\(i^2 = -1\)</span>. Because no real number satisfies this equation, this number is called <em>imaginary</em>.</p>
<p>For the complex number <span class="math inline">\(z = a + i b\)</span>, <span class="math inline">\(a\)</span> is called the <strong>real part</strong> and <span class="math inline">\(b\)</span> is called the <strong>imaginary part</strong>. The real part of <span class="math inline">\(z\)</span> is denoted <span class="math inline">\(\Re(z)\)</span> (<code>\Re</code> in LaTeX) or just <span class="math inline">\(\mathrm{Re}(z).\)</span> The imaginary part of <span class="math inline">\(z\)</span> denoted <span class="math inline">\(\Im(z)\)</span> (<code>\Im</code> in LaTex) or just <span class="math inline">\(\mathrm{Im}(z)\)</span>. The set of all complex numbers is denoted <span class="math inline">\(\mathbb{C}\)</span> (<code>\mathbb{C}</code> in LaTeX).</p>
<p>We need complex numbers for solving polynomial equations. The fundamental theorem of algebra asserts that a polynomial equation of with real or complex coefficients has complex solutions. These polynomial equations arise when trying to compute the eigenvalues of matrices, something we need to do to solve linear differential equations for instance.</p>
<p>Arithmetic rules that apply on real numbers also apply on complex numbers, by using the rule <span class="math inline">\(i^2 = -1\)</span>: addition, subtraction, multiplication and division are associative, commutative and distributive.</p>
<p>Let <span class="math inline">\(u = a + i b\)</span> and <span class="math inline">\(v = c + i d\)</span> two complex numbers, with real coefficients <span class="math inline">\(a,b,c,d\)</span>. Then</p>
<ul>
<li><p><span class="math inline">\(u + v = a + i b + c + i d = (a+c) + i (b+d)\)</span>.</p></li>
<li><p><span class="math inline">\(uv = (a + i b)(c + i d) = ac + i a d + i b c + i^2 b d = ac - bd + i(ad + bc)\)</span>.</p></li>
<li><p><span class="math inline">\(\frac{1}{v} =  \frac{1}{c + i d} = \frac{c - id}{(c - id)(c + id)} = \frac{c - id}{c^2 + d^2} =
  \frac{c}{c^2 + d^2} - i \frac{d}{c^2+d^2}\)</span>.</p></li>
<li><p><span class="math inline">\(u = v\)</span> if and only if <span class="math inline">\(a = c\)</span> and <span class="math inline">\(b = d\)</span>.</p>
<p>It follows from the rule on <span class="math inline">\(i\)</span> that</p></li>
<li><p><span class="math inline">\(\frac 1i = -i.\)</span> (Proof: <span class="math inline">\(\frac 1i = \frac{i}{i^2} = \frac{i}{-1} = -i\)</span>.)</p></li>
</ul>
<p>Multiplying by the imaginary unit <span class="math inline">\(i\)</span> is equivalent to a counterclockwise rotation by <span class="math inline">\(\pi/2\)</span> (Figure <a href="#f_complex_rotation" data-reference-type="ref" data-reference="f_complex_rotation">3</a>) <span class="math display">\[ui = (a + ib)i = ia + i^2 b = -b + ia.\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_23.pdf.png" id="f_complex_rotation" alt="Rotation in the complex plane. Multiplication by i is a 90 degree counterclockwise rotation. " /><figcaption aria-hidden="true">Rotation in the complex plane. Multiplication by <span class="math inline">\(i\)</span> is a 90 degree counterclockwise rotation. </figcaption>
</figure>
<p>Let <span class="math inline">\(z = a + ib\)</span> a complex number with real <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The <strong>conjugate</strong> of <span class="math inline">\(z\)</span>, denoted <span class="math inline">\(\bar z\)</span>, is <span class="math inline">\(a - ib\)</span>. The conjugate of the conjugate of <span class="math inline">\(z\)</span> is <span class="math inline">\(z\)</span> (<em>reflection</em>, Figure <a href="#f_complex_rotation" data-reference-type="ref" data-reference="f_complex_rotation">3</a>). The <strong>modulus</strong> of <span class="math inline">\(z\)</span>, denoted <span class="math inline">\(|z|\)</span> is <span class="math inline">\(\sqrt{z \bar z}\)</span>. The product <span class="math inline">\(z \bar z = (a+ib)(a-ib)=a^2 + b^2 + i(-ab + ab) = a^2 + b^2\)</span>. The modulus is the complex version of the absolute value, for if <span class="math inline">\(z\)</span> (i.e. <span class="math inline">\(b = 0\)</span>), <span class="math inline">\(|z| = \sqrt{a^2} = |a|\)</span>. It is always a real, positive number, and <span class="math inline">\(|z| = 0\)</span> if and only if <span class="math inline">\(z = 0\)</span>. The modulus also has the property of being the <em>length</em> of the complex number <span class="math inline">\(z\)</span>, if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the sides of a rectangular triangle, then <span class="math inline">\(|z|\)</span> is the hypotenuse.</p>
<p>When simplifying a ratio involving a complex <span class="math inline">\(v\)</span> at the denominator, it is important to convert it to a real number by multiplying the ratio by <span class="math inline">\(\bar v/ \bar v\)</span>. For instance, if <span class="math inline">\(v \neq 0\)</span>, <span class="math display">\[\frac{u}{v} = \frac{u \bar v}{u \bar v} = \frac{u \bar v}{|v|^2}.\]</span> The denominator <span class="math inline">\(|v|^2\)</span> is always a positive real number.</p>
<p>By allowing complex values, nonlinear functions of real numbers like exponentials, logarithms and trigonometric functions can have their domain extended to all real and complex numbers. The most useful extension is the exponential function. Recall that the exponential function <span class="math inline">\(e^x\)</span>, where <span class="math inline">\(e \approx 2.71828\)</span> is Euler’s constant, satisfies the relation <span class="math inline">\(e^{x + y} = e^{x} e^{y}\)</span>. This remains true for complex numbers. The <strong>Euler’s formula</strong> relates the exponential of a imaginary number with trigonometric functions. For a real number <span class="math inline">\(y\)</span>, <span class="math display">\[e^{i y} = \cos(y) + i \sin(y).\]</span> Therefore, for any complex number <span class="math inline">\(z = a + i b\)</span>, the exponential <span class="math display">\[e^{z} = e^{a + ib} = e^a e^{ib} = e^a \bigl( \cos(b) + i \sin(b) \bigr).\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_24.pdf.png" id="f_complex_plane" alt="Complex plane" /><figcaption aria-hidden="true">Complex plane</figcaption>
</figure>
<hr />
<p><strong>Tips on complex numbers</strong></p>
<ul>
<li><p>If <span class="math inline">\(x\)</span> is real, <span class="math inline">\(ix\)</span> is pure imaginary. If <span class="math inline">\(y\)</span> is imaginary, <span class="math inline">\(iy\)</span> is real.</p></li>
<li><p><span class="math inline">\(|i| = 1\)</span>. For any real <span class="math inline">\(\theta\)</span>, <span class="math inline">\(|e^{i \theta}| = 1\)</span>.</p></li>
<li><p><span class="math inline">\(|z_1 z_2| = |z_1| |z_2|\)</span>.</p></li>
<li><p>In particular, <span class="math inline">\(|iz| = |i||z| = |z|\)</span>. (Multpliying by <span class="math inline">\(i\)</span> is a rotation in the complex plane, it does not change the modulus.)</p></li>
</ul>
<hr />
<h2 id="roots-of-a-complex-number">Roots of a complex number</h2>
<p>For complex numbers, the equation <span class="math inline">\(z^n = 1\)</span> has <span class="math inline">\(n\)</span> solutions. They are called the <strong>root of unity</strong>. For <span class="math inline">\(n=2\)</span>, we have the well-known roots <span class="math inline">\(z = \pm 1\)</span>, which are real. What are the roots of <span class="math inline">\(z^3 = 1\)</span>? To find them, we express <span class="math inline">\(z\)</span> in polar coordinates: <span class="math inline">\(z = r e^{i\theta}\)</span>. Then <span class="math display">\[z^3 = (r e^{i\theta})^3 = r^3 e^{i 3\theta} = 1.\]</span> The equation implies that <span class="math inline">\(z\)</span> has modulus 1, so <span class="math inline">\(r = 1\)</span>. The remaining term <span class="math inline">\(e^{i 3\theta} = 1\)</span> implies that <span class="math inline">\(3 \theta\)</span> is a multiple of <span class="math inline">\(2 \pi\)</span> because <span class="math inline">\(e^{i \omega} = 1\)</span> if and only if <span class="math inline">\(\omega = 2 k \pi\)</span>, for some integer <span class="math inline">\(k\)</span>. Therefore <span class="math inline">\(\theta = \frac{2}{3} k \pi\)</span>, for <span class="math inline">\(k = 0, 1, 2, ...\)</span>. How many distinct points do we have? Clearly, <span class="math inline">\(k = 3\)</span> is equivalent to <span class="math inline">\(k = 0\)</span>: <span class="math inline">\(e^{i \frac{2}{3} 3 \pi} = e^{i 2 \pi} = e{i 0}\)</span>. In the same way <span class="math inline">\(k = 4\)</span> is equivalent to <span class="math inline">\(k = 1\)</span>, and so on. Therefore, there are exactly three distinct solutions for <span class="math inline">\(\theta\)</span>: <span class="math inline">\(0, \frac{2}{3} \pi, \frac{4}{3} \pi\)</span> (Figure <a href="#f_roots_unity" data-reference-type="ref" data-reference="f_roots_unity">5</a>).</p>
<figure>
<img src="htmltmp/tikzblocks_25.pdf.png" id="f_roots_unity" alt="The roots of z^3 - 1." /><figcaption aria-hidden="true">The roots of <span class="math inline">\(z^3 - 1\)</span>.</figcaption>
</figure>
<h2 id="exercises-on-complex-numbers">Exercises on complex numbers</h2>
<p><strong>Exercice </strong> Let the complex number <span class="math inline">\(z = 2 + 3 i\)</span>. Compute <span class="math inline">\(\bar z\)</span>, <span class="math inline">\(|z|\)</span>, <span class="math inline">\(|\bar z|\)</span> (compare with <span class="math inline">\(|z|\)</span>), <span class="math inline">\(z^2\)</span>, <span class="math inline">\(\Re(\bar z)\)</span> , <span class="math inline">\(\Im(\bar z)\)</span>, <span class="math inline">\(\frac{z + \bar z}{2}\)</span> , <span class="math inline">\(\frac{z - \bar z}{2}\)</span> , <span class="math inline">\(-z\)</span>, <span class="math inline">\(iz\)</span>.</p>
<p><strong>Exercice </strong> Any complex number can be represented in <strong>polar form</strong>: <span class="math inline">\(z = r ( \cos(\theta) + i \sin(\theta) )\)</span>.</p>
<ul>
<li><p>Show that <span class="math inline">\(|z| = r\)</span></p></li>
<li><p>Show that <span class="math inline">\(z = r e^{i \theta}\)</span></p></li>
<li><p>Conclude that for any complex number <span class="math inline">\(z\)</span>, <span class="math inline">\(|z| = 1\)</span> if and only if <span class="math inline">\(z\)</span> can be expressed as <span class="math inline">\(z = e^{i \theta}\)</span> for a real <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p><strong>Exercice </strong> Using Euler’s formula, show that <span class="math inline">\(\cos(a)\cos(b) - \sin(a)\sin(b) = \cos(a+b)\)</span>. <em>(Use the property that <span class="math inline">\(e^{ia + ib} = e^{ia} e^{ib}\)</span> and apply Euler’s Formula)</em>.</p>
<p><strong>Exercice </strong> Show Euler’s identity: <span class="math inline">\(e^{i \pi} = -1\)</span>.</p>
<p><strong>Exercice </strong> What are the roots of the equation <span class="math inline">\(z^6 = 1\)</span>?</p>
<p><strong>Exercice </strong> For a complex <span class="math inline">\(z\)</span>, find necessary and sufficient conditions for <span class="math inline">\(e^{z t}\)</span>, <span class="math inline">\(t &gt; 0\)</span>, to converge to 0.</p>
<p><strong>Exercice </strong> Let the complex number <span class="math inline">\(z = a + ib\)</span> with real <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Compute <span class="math inline">\(\sqrt{z}\)</span> (that is, express <span class="math inline">\(s = \sqrt{z}\)</span> as <span class="math inline">\(s = \alpha + i \beta\)</span>, with real <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>)</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_26.pdf.png" alt="image" /></p>
</div>
<h1 id="matrices-in-dimension-2">Matrices in dimension 2</h1>
<h2 id="eigenvalues-of-a-2-times-2-matrix">Eigenvalues of a <span class="math inline">\(2 \times 2\)</span> matrix</h2>
<p>A <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(A\)</span> is an array with 2 rows and 2 columns: <span class="math display">\[A = \begin{pmatrix}
a &amp; b \\
c &amp; d 
\end{pmatrix}.\]</span> Usually, the <strong>coefficients</strong> <span class="math inline">\(a, b, c, d\)</span> are real numbers. The <strong>identity</strong> matrix is the matrix <span class="math display">\[I = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 
\end{pmatrix}.\]</span></p>
<p>The <strong>determinant</strong> of <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(\det A\)</span> or <span class="math inline">\(|A|\)</span> is the number <span class="math inline">\(ad - bc\)</span>. The <strong>trace</strong> of <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(\mathrm{tr}\,A\)</span>, is the sum of the main <strong>diagonal</strong> of <span class="math inline">\(A\)</span>: <span class="math inline">\(a + d\)</span>.</p>
<p>The <strong>characteristic polynomial</strong> of <span class="math inline">\(A\)</span> is the second order polynomial in <span class="math inline">\(\lambda\)</span> obtained by computing the determinant of the matrix <span class="math inline">\(A - \lambda I\)</span> (Figure <a href="#f_characteristic_polynomial" data-reference-type="ref" data-reference="f_characteristic_polynomial">6</a>), <span class="math display">\[\det ( A - \lambda I ) = (a-\lambda)(d-\lambda) - bc = ad - bc - \lambda ( a + d ) + \lambda^2.\]</span> The characteristic polynomial <span class="math inline">\(p_A(\lambda)\)</span> of <span class="math inline">\(A\)</span> can be expressed in terms of its determinant and trace: <span class="math display">\[p_A(\lambda) = \det A - \mathrm{tr}\,A \lambda + \lambda^2.\]</span> The <strong>eigenvalues</strong> of <span class="math inline">\(A\)</span> are the roots of the characteristic polynomial. By the fundamental theorem of algebra, we know that the characteristic polynomial has exactly two roots, counting multiple roots. These roots can be real, or complex. The eigenvalues of <span class="math inline">\(A\)</span> are calculated using the quadratic formula: <span class="math display">\[\lambda_{1,2} = \frac{1}{2} \Bigl( \mathrm{tr}\,A \pm \sqrt{ (\mathrm{tr}\,A)^2 - 4 \det A } \Bigr).\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_27.pdf.png" id="f_characteristic_polynomial" alt="Graph of the characteristic polynomial. (a, green) polynomial with two negative roots, p(\lambda) = 0.08 + 0.8\lambda + \lambda^2. (b, gray) polynomial with two complex roots, p(\lambda) = 0.1  - 0.3\lambda + \lambda^2. (c, red) polynomial with two positive roots, p(\lambda) = 0.05 + 0.5\lambda + \lambda^2. (d, purple) polynomial with a negative and a positive root, p(\lambda) = -0.1 - 0.3\lambda + \lambda^2. " /><figcaption aria-hidden="true">Graph of the characteristic polynomial. (<em>a, green</em>) polynomial with two negative roots, <span class="math inline">\(p(\lambda) = 0.08 + 0.8\lambda + \lambda^2\)</span>. (<em>b, gray</em>) polynomial with two complex roots, <span class="math inline">\(p(\lambda) = 0.1  - 0.3\lambda + \lambda^2\)</span>. (<em>c, red</em>) polynomial with two positive roots, <span class="math inline">\(p(\lambda) = 0.05 + 0.5\lambda + \lambda^2\)</span>. (<em>d, purple</em>) polynomial with a negative and a positive root, <span class="math inline">\(p(\lambda) = -0.1 - 0.3\lambda + \lambda^2\)</span>. </figcaption>
</figure>
<p>From this formula, we can classify the eigenvalues of <span class="math inline">\(A\)</span>. Let <span class="math display">\[\Delta = (\mathrm{tr}\,A)^2 - 4 \det A\]</span> the <strong>discriminant</strong> of the quadratic formula. The two eigenvalues of <span class="math inline">\(A\)</span> are real if and only if <span class="math inline">\(\Delta \geq 0\)</span>, i.e. <span class="math inline">\(\mathrm{tr}\,A)^2 \geq 4 \det A\)</span> Then we have the following properties (Figure <a href="#f_eigenvalues" data-reference-type="ref" data-reference="f_eigenvalues">7</a>):</p>
<ol>
<li><p><span class="math inline">\(\Delta &lt; 0\)</span>, complex eigenvalues</p>
<ul>
<li><p>The two eigenvalues are complex conjugate: <span class="math inline">\(\lambda_1 = \bar \lambda_2\)</span></p></li>
<li><p>Their real part <span class="math inline">\(\Re(\lambda) = \frac{1}{2} \mathrm{tr}\,A\)</span>.</p></li>
</ul></li>
<li><p><span class="math inline">\(\Delta = 0\)</span>, there is a single root of multiplicity 2: <span class="math inline">\(\lambda = \frac{1}{2} \mathrm{tr}\,A\)</span>.</p></li>
<li><p><span class="math inline">\(\Delta &gt; 0, \det A &gt; 0\)</span>, real, distinct eigenvalues of the same sign.</p>
<ul>
<li><p><span class="math inline">\(\mathrm{tr}\,A &gt; 0\)</span> and <span class="math inline">\(\det A &gt; 0\)</span>. Then <span class="math inline">\(\lambda_{1,2}\)</span> are distinct and positive.</p></li>
<li><p><span class="math inline">\(\mathrm{tr}\,A &lt; 0\)</span> and <span class="math inline">\(\det A &gt; 0\)</span>. Then <span class="math inline">\(\lambda_{1,2}\)</span> are distinct and negative.</p></li>
</ul></li>
<li><p><span class="math inline">\(\det A &lt; 0\)</span>, real distinct eigenvalues of opposite sign.</p>
<ul>
<li><p><span class="math inline">\(\lambda_1 &lt; 0 &lt; \lambda_2\)</span>.</p></li>
</ul></li>
<li><p><span class="math inline">\(\det A = 0\)</span> one of the eigenvalue is zero, the other eigenvalue is <span class="math inline">\(\mathrm{tr}\,A\)</span>.</p></li>
</ol>
<figure>
<img src="htmltmp/tikzblocks_28.pdf.png" id="f_eigenvalues" alt="Properties of the eigenvalues of a 2 \times 2 matrix A with respect to \det A and \mathrm{tr}\,A." /><figcaption aria-hidden="true">Properties of the eigenvalues of a <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(A\)</span> with respect to <span class="math inline">\(\det A\)</span> and <span class="math inline">\(\mathrm{tr}\,A\)</span>.</figcaption>
</figure>
<h3 id="exercises-on-eigenvalues">Exercises on eigenvalues</h3>
<p><strong>Exercice </strong> Properties of the eigenvalues of <span class="math inline">\(2 \times 2\)</span> matrices. For each <span class="math inline">\(2 \times 2\)</span> matrix, compute the determinant, the trace, and the discriminant, and determine whether the eigenvalues are real, complex, distinct, and the sign (negative, positive, or zero) of the real parts.</p>
<p><span class="math display">\[A_1 = \begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}, \quad
A_2 = \begin{pmatrix}
-2 &amp;  1 \\
 1 &amp; -2 
\end{pmatrix}, \quad
A_3 = \begin{pmatrix}
 1 &amp; -2 \\
 0 &amp;  1 
\end{pmatrix}, \quad
A_4 = \begin{pmatrix}
-1 &amp;  2 \\
1/2&amp;  2 
\end{pmatrix}.\]</span></p>
<h2 id="matrix-vector-operations">Matrix-vector operations </h2>
<p>A matrix defines a linear transformation between vector spaces. Given a vector <span class="math inline">\(x\)</span>, the product <span class="math inline">\(Ax\)</span> is vector composed of linear combinations of the coefficients of <span class="math inline">\(x\)</span>. For a matrix <span class="math inline">\(2 \times 2\)</span>, the vector <span class="math inline">\(x\)</span> must be a vector of size 2, and the product <span class="math inline">\(Ax\)</span> is a vector of size two. If <span class="math inline">\(x = (x_1, x2)^t\)</span> (the <span class="math inline">\({}^t\)</span> stands for the transpose, because <span class="math inline">\(x\)</span> must be a column vector), and <span class="math inline">\(A = [ a_{ij} ]_{i=1,2, \, j=1,2}\)</span>, then <span class="math display">\[Ax = 
\begin{pmatrix}
a_{11} &amp;  a_{12} \\
a_{21} &amp;  a_{22} 
\end{pmatrix}
\begin{pmatrix}
x_{1}  \\
x_{2}  
\end{pmatrix}
=
\begin{pmatrix}
a_{11} x_{1} + a_{12} x_2  \\
a_{21} x_{1} + a_{22} x_2 
\end{pmatrix}.\]</span> Successive linear transformations can be accomplished by applying several matrices. Given two matrices <span class="math inline">\(A, B\)</span>, the matrix product <span class="math inline">\(C = AB\)</span> is also a matrix. The matrix <span class="math inline">\(C\)</span> is the linear transformation that first applies <span class="math inline">\(B\)</span>, then <span class="math inline">\(A\)</span>. Matrix product is <em>not</em> commutative is general: <span class="math inline">\(AB \neq BA\)</span>. <em>(If <span class="math inline">\(B\)</span> means ’put on socks’ and <span class="math inline">\(A\)</span> means ’put on shoes’, then <span class="math inline">\(BA\)</span> does not have the expected result.)</em> The product of two matrices <span class="math inline">\(A = [ a_{ij} ]_{i=1,2, \, j=1,2}\)</span> and <span class="math inline">\(B = [ b_{ij} ]_{i=1,2, \, j=1,2}\)</span> is <span class="math display">\[AB = 
\begin{pmatrix}
a_{11} &amp;  a_{12} \\
a_{21} &amp;  a_{22} 
\end{pmatrix}
\begin{pmatrix}
b_{11} &amp;  b_{12} \\
b_{21} &amp;  b_{22} 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} b_{11} + a_{12}b_{21} &amp; a_{11}b_{12} + a_{12}b_{22} \\
a_{21} b_{11} + a_{22}b_{21} &amp; a_{21}b_{12} + a_{22}b_{22}
\end{pmatrix}.\]</span> The <strong>sum of two matrices</strong> <span class="math inline">\(A+B\)</span> is performed element-wise: <span class="math inline">\(A+B = [a_{ij} + b_{ij}]_{i=1,2, \, j=1,2}\)</span>. The <strong>sum of two vectors</strong> is defined similarly. Addition is commutative. Matrix operations are associative and distributive. <span class="math display">\[\begin{aligned}
A + B &amp; = B + A, \\
A(B + C) &amp; = AB + BC, \\
A(BC)  &amp; = (AB)C. \end{aligned}\]</span> Matrices and vectors can be multiplied by a scalar value (real or complex). <strong>Multiplication by a scalar</strong> is associative, distributive, and commutative. The result of the multiplication by a scalar is to multiply each coefficient of the matrix or vector by the scalar. For example, if <span class="math inline">\(\lambda, \mu\)</span> are scalars, <span class="math display">\[\begin{aligned}
\lambda A &amp; = A (\lambda I) = A \lambda, \\
\lambda ( A + B ) &amp; = \lambda A + \lambda B, \\
(\lambda A) B &amp; = \lambda (AB) = A (\lambda B), \\
(\mu + \lambda) A &amp; = \mu A + \lambda A, \\
\mu (\lambda A) &amp; = (\mu \lambda) A, ...\end{aligned}\]</span> The product between two column vectors is not defined, because the sizes do not match. However, we can define the <strong>scalar product</strong> between two column vectors <span class="math inline">\(x, y\)</span> in the same way matrix product is defined: <span class="math display">\[x^ty \equiv x_1 y_1 + x_2 y_2.\]</span></p>
<p>If the vectors are complex-valued, we need also to conjugate the transposed vector <span class="math inline">\(x^t\)</span>. The conjugate-transpose is called the <strong>adjoint</strong> and is denoted <span class="math inline">\({}^*\)</span>. Thus, if <span class="math inline">\(x\)</span> is complex-valued, the adjoint <span class="math inline">\(x^*\)</span> is the row vector <span class="math inline">\((\bar x_1, \bar x_2)\)</span>. The scalar product for complex-valued vectors is denoted <span class="math inline">\(x^*y\)</span>. Since this notation also works for real-valued vector, we will used most of the time.</p>
<p>Two vectors are <strong>orthogonal</strong> if their scalar product is 0. In the plane, this means that they are oriented at 90 degree apart. Orthogonal vectors are super important because they can be used to build orthogonal bases that are necessary for solving all sorts of <em>linear problems</em>.</p>
<h3 id="exercises-on-matrix-vector-and-matrix-matrix-operations">Exercises on Matrix-vector and matrix-matrix operations</h3>
<p><strong>Exercice </strong> Compute matrix-vector product <span class="math display">\[\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}.\]</span> What is the transformation given by this matrix.</p>
<p><strong>Exercice </strong> Compute the matrix-matrix product <span class="math display">\[\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}
\begin{pmatrix}
1 &amp;  0 \\
0 &amp; -1 
\end{pmatrix}.\]</span> Can you tell what transformation this is?</p>
<p><strong>Exercice </strong> Now compute the product of the same matrices, but in the inverse order <span class="math display">\[\begin{pmatrix}
1 &amp;  0 \\
0 &amp; -1 
\end{pmatrix}
\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}.\]</span> Compare with the solution found in the previous exercise. What is this transformation?</p>
<p><strong>Exercice </strong> Find the matrix that takes a vector <span class="math inline">\(x = (x_1,x_2)^t\)</span> and returns <span class="math inline">\((a x_1, b x_2)^t\)</span>.</p>
<p><strong>Exercice </strong> Find the matrix that takes a vector <span class="math inline">\(x = (x_1,x_2)^t\)</span> and returns <span class="math inline">\((x_2, x_1)^t\)</span>.</p>
<p><strong>Exercice </strong> Find the matrix that takes a vector <span class="math inline">\(x = (x_1,x_2)^t\)</span> and returns <span class="math inline">\((x_2, 0)^t\)</span>.</p>
<p><strong>Exercice </strong> Compute the successive powers <span class="math inline">\(A, A^2, A^3, ...\)</span>, for a diagonal matrix <span class="math inline">\(A\)</span>: <span class="math display">\[A =\begin{pmatrix}
a &amp;  0 \\
0 &amp;  b 
\end{pmatrix}\]</span></p>
<p><strong>Exercice </strong> Compute the scalar product <span class="math inline">\(x^*y\)</span> between <span class="math inline">\(x = (1 + 2i, 1 - i)^t\)</span> and <span class="math inline">\(y = (0.5 - i, -0.5)^t\)</span>.</p>
<p><strong>Exercice </strong> Now compute the scalar product <span class="math inline">\(y^*x\)</span> and compare with the result with the previous exercise.</p>
<p><strong>Exercice </strong> Compute the scalar product between <span class="math inline">\(z = (z_1, z_2)^t\)</span> and itself, if <span class="math inline">\(z\)</span> is a complex-valued vector. What can you say about the result?</p>
<hr />
<p><strong>Tips on eigenvalues</strong> Some matrices have special shapes that make it easier to compute the determinant, and the eigenvalues. These are called eigenvalue-revealing shapes.</p>
<ul>
<li><p>Diagonal matrices have their eigenvalues on the diagonal.</p></li>
<li><p><strong>Triangular matrices</strong>, i.e. matrices that have zeros above (lower-triangular matrix) or below (upper-triangular matrix) the main diagonal have also their eigenvalues on the diagonal.</p></li>
<li><p>A matrix with a row or a column of zeros has its determinant equal to zero. This implies that one of its eigenvalues is 0.</p></li>
</ul>
<hr />
<h1 id="eigenvalue-decomposition">Eigenvalue decomposition</h1>
<p>In many applications, it is useful to decompose a matrix into a form that makes it easier to operate complex operations on. For instance, we might want to compute the powers of a matrix <span class="math inline">\(A\)</span>: <span class="math inline">\(A^2\)</span>, <span class="math inline">\(A^3\)</span>, <span class="math inline">\(A^4\)</span>. Multiplying matrices are computationally intensive, especially when the size of the matrix becomes large. The <strong>power of a matrix</strong> is <span class="math inline">\(A^k = AA...A\)</span>, <span class="math inline">\(k\)</span> times. The zeroth power is the identity matrix: <span class="math inline">\(A^0 = I\)</span>.</p>
<p>The <strong>inverse</strong> of a matrix <span class="math inline">\(A\)</span>, denoted by <span class="math inline">\(A^{-1}\)</span> is the unique matrix such that <span class="math inline">\(AA^{-1} A{^-1}A = I\)</span>. The notation is self-consistent with the positive powers of <span class="math inline">\(A\)</span>. The inverse does not always exist. A matrix is <strong>invertible</strong> if and only if its determinant is not 0. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are invertible, then <span class="math inline">\(AB\)</span> is invertible, and <span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span>.</p>
<p>The <strong>eigenvalue decomposition</strong> is a decomposition of the form <span class="math inline">\(A = X D X^{-1}\)</span>, where <span class="math inline">\(D\)</span> is a diagonal matrix, and <span class="math inline">\(X\)</span> is an invertible matrix. If there exists such a decomposition for <span class="math inline">\(A\)</span>, then computing powers of <span class="math inline">\(A\)</span> becomes easy: <span class="math display">\[\begin{aligned}
A^k &amp; = (XDX^{-1})^k = XDX^{-1} \, XDX^{-1} \, ... XDX^{-1}, \\
    &amp; = XD(X^{-1}X)D(X^{-1}X) D ... (X^{-1}X) DX^{-1}, \\
    &amp; = XD^kX^{-1}.\end{aligned}\]</span></p>
<p>The eigenvalue decomposition does not always exists, because it is not always possible to find an invertible matrix <span class="math inline">\(X\)</span>. When it exists, though, the columns of the matrix <span class="math inline">\(X\)</span> is composed of the eigenvectors of <span class="math inline">\(A\)</span>. When <span class="math inline">\(A\)</span> is a <span class="math inline">\(2 \times 2\)</span> matrix, it is enough to find 2 linearly independent eigenvectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> for the matrix <span class="math display">\[X = \Biggl( \begin{array}{c|c} x_1 &amp; y_1 \\ x_2 &amp; y_2 \end{array} \Biggr)\]</span> to be invertible.</p>
<h2 id="eigenvectors">Eigenvectors</h2>
<p>The <strong>eigenvectors</strong> of a matrix <span class="math inline">\(A\)</span> are the <em>nonzero</em> vectors <span class="math inline">\(x\)</span> such that for an eigenvalue <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(A\)</span>, <span class="math display">\[Ax = \lambda x.\]</span> If <span class="math inline">\(x\)</span> is an eigenvector, so is any <span class="math inline">\(\alpha x\)</span> for any scalar value <span class="math inline">\(\alpha\)</span>. If there are two linearly independent eigenvectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> associated to an eigenvalue, <span class="math inline">\(\alpha x + \beta y\)</span> is also an eigenvector. There is at least one eigenvector for each distinct eigenvalue, but there may be more than one when the eigenvalue is repeated.</p>
<p><strong>Example </strong> Distinct, real eigenvalues The matrix <span class="math display">\[A =\begin{pmatrix}
-1 &amp; -2 \\
0  &amp;  1 
\end{pmatrix}\]</span></p>
<p>is upper-triangular; this is one of the eigenvalue-relealing shapes. The eigenvalues are <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. These are distinct eigenvalues, so each eigenvalue possesses a single eigenvector. The eigenvector <span class="math inline">\(x\)</span> associated to <span class="math inline">\(\lambda_1 = -1\)</span> is found by solving the eigensystem <span class="math display">\[Ax = (-1)x.\]</span></p>
<p>The unknown quantity <span class="math inline">\(x\)</span> appears on both sides of the equation. We can find a simpler form by noting that multiplying a vector by the identity matrix is neutral: <span class="math inline">\((-1)x = (-1) I x.\)</span> The eigenproblem becomes <span class="math display">\[\begin{aligned}
A x &amp; = (-1) I x, \\
A x - (-1) I x = 0, \\
\bigl( A - (-1) I \bigr) x = 0,\end{aligned}\]</span></p>
<p>that is, the eigenvector is a nonzero solution of the linear system <span class="math inline">\(\bigl( A - \lambda I \bigr) x = 0\)</span>. In general, if a matrix <span class="math inline">\(B\)</span> is invertible, the only solution to <span class="math inline">\(Bx=0\)</span> is <span class="math inline">\(x = 0\)</span> (the vector of zeroes). But, by construction, <span class="math inline">\(A - \lambda I\)</span> cannot be invertible if <span class="math inline">\(\lambda\)</span> is an eigenvalue: its determinant is exactly the characteristic polynomial evaluated at one of its roots, so it is zero. This is why the eigensystem has nonzero solutions. Now, because <span class="math inline">\(A - \lambda I\)</span> is not invertible, this means that a least one of its rows is a linear combination of the others. For <span class="math inline">\(2 \times 2\)</span> matrices, this implies that the two rows are colinear, or redundant. For our example, the eigensystem reads <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (-1) &amp; -2 \\
0  &amp;  1 - (-1) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
0  &amp; -2 \\
0  &amp;  2 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>we immediately see that the two rows <span class="math inline">\((0,-2)\)</span> and <span class="math inline">\((0,2)\)</span> are colinear, with a factor <span class="math inline">\(-1\)</span>. This leads to an underdetermined system: <span class="math inline">\(0 x_1 + -2 x_2 = 0\)</span>. The solution is <span class="math inline">\(x_2 = 0\)</span> and we can take <span class="math inline">\(x_1\)</span> to be any value, save 0. We choose <span class="math inline">\(x = (1, 0)^t\)</span>.</p>
<p>For the eigenvalue <span class="math inline">\(\lambda_2 = +1\)</span>, the eigensystem reads: <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (+1) &amp; -2 \\
0  &amp;  1 - (+1) 
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
-2  &amp; -2 \\
0  &amp;   0
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>Again, the second row <span class="math inline">\((0,0)\)</span> can be neglected, and the solution is <span class="math inline">\(-2 y_1 + 2 y_2 = 0\)</span>, or <span class="math inline">\(y_1 = y_2\)</span>. It is customary to choose an eigenvector with norm 1. The <strong>norm</strong> of a complex-valued vector <span class="math inline">\(y = (y_1, y_2)^t\)</span> is <span class="math display">\[||y|| = \sqrt{y^*y} = \sqrt{\bar y_1 y_1 + \bar y_2 y_2} = \sqrt{|y_1|^2 + |y_2|^2}.\]</span></p>
<p>Here, the eigenvector is <span class="math inline">\(y = (y_1, y_1)^t\)</span>, so <span class="math inline">\(||y|| = \sqrt{|y_1|^2 + |y_1|^2} = \sqrt{2}\sqrt{|y_1|^2} = \sqrt{2}|y_1|.\)</span> Taking <span class="math inline">\(||y|| = 1\)</span> solves <span class="math inline">\(|y_1| = 1/\sqrt{2}.\)</span> This means that we could take a negative, or a complex value for <span class="math inline">\(y_1\)</span>, as long as the <span class="math inline">\(|y_1| = 1/\sqrt{2}.\)</span> Going for simplicity, we take <span class="math inline">\(y_1 = 1/\sqrt{2}\)</span>.</p>
<p><strong>Example </strong> Complex eigenvalues</p>
<p>The matrix <span class="math display">\[A =\begin{pmatrix}
0  &amp; -1 \\
1  &amp;  0 
\end{pmatrix}\]</span> is <em>not</em> diagonal, so we have to compute the eigenvalues by hand. The trace of <span class="math inline">\(A\)</span> is zero, the determinant is <span class="math inline">\(0 - (1)(-1) = 1\)</span>, and the discriminant is <span class="math inline">\(-4\)</span>. A negative discriminant implies complex eigenvalues, <span class="math display">\[\lambda_{1,2} = \frac 12 \bigl( 0 \pm \sqrt{-4} \bigr) = \pm i.\]</span> For the eigenvalue <span class="math inline">\(\lambda_1 = +i\)</span>, the eigensystem reads: <span class="math display">\[\begin{aligned}
\begin{pmatrix}
- (+i) &amp;  -1 \\
1      &amp;  - (+i) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
-i  &amp; -1 \\
1  &amp;  -i
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>The two rows <span class="math inline">\((-i,1)\)</span> and <span class="math inline">\((1,-i)\)</span> should be colinear, but this is not obvious with the complex coefficients. Multiplying the first row by <span class="math inline">\(i\)</span> gives <span class="math inline">\(i(-i, -1) = (- i^2, - i) = (-(-1), -i) = (1, -i)\)</span>, the second row, ok. Having confirmed that the system is indeed underdetermined, we can week a solution to <span class="math inline">\(-i x_1 - x_2 = 0\)</span>. Solving for <span class="math inline">\(x_2 = -i x_1\)</span>, we obtain the eigenvector <span class="math inline">\(x = (x_1, -i x_2)^t\)</span>. Normalization of <span class="math inline">\(x\)</span> imposes <span class="math display">\[||x|| = \sqrt{|x_1|^2 + |-ix_1|^2} = \sqrt{|x_1|^2 + |x_1|^2} = \sqrt{2}|x_1| = 1.\]</span> As in the previous example, we can choose <span class="math inline">\(x_1 = 1/\sqrt{2}.\)</span></p>
<p>The second eigenvectors, associated <span class="math inline">\(\lambda_2 = -i\)</span>, solves the eigensystem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
- (-i) &amp;  -1 \\
1      &amp;  - (-i) 
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
i  &amp; -1 \\
1  &amp;  i
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>The first row yields <span class="math inline">\(iy_1 - y_2 = 0\)</span>, so <span class="math inline">\(y = (y_1, iy_2)^t\)</span>. A normalized eigenvector can be <span class="math inline">\(y = (1/\sqrt{2}, i/\sqrt{2})^t\)</span>. We could also have chosen <span class="math inline">\(y = (i/\sqrt{2}, -1/\sqrt{2})^t\)</span>.</p>
<p><strong>Example </strong> Repeated eigenvalues 1</p>
<p>The matrix <span class="math display">\[\begin{pmatrix}
-1  &amp;   0 \\
 2  &amp;  -1 
\end{pmatrix}\]</span></p>
<p>is lower-trianglar, with repeated eigenvalues on the diagonal, <span class="math inline">\(\lambda_{1,2} = -1\)</span>. The eigenvectors associated with <span class="math inline">\(-1\)</span> satisfy the eigenproblem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (-1) &amp;  0 \\
 2      &amp;  -1 - (-1) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
 0  &amp;  0 \\
 2 &amp;   0
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>The first row vanishes, and the second row means that <span class="math inline">\(x_1 = 0\)</span>, leaving for instance <span class="math inline">\(x_2 = 1\)</span>, and <span class="math inline">\(x = (0,1)^t\)</span>. There are no other linearly independent eigenvectors. This is not always the case, repeated eigenvalues can have more than one independent eigenvector, as in the next example.</p>
<p><strong>Example </strong> Repeated eigenvalues 2</p>
<p>The matrix <span class="math display">\[\begin{pmatrix}
-1  &amp;   0 \\
 0  &amp;  -1 
\end{pmatrix}\]</span> is diagonal, with repeated eigenvalues on the diagonal, <span class="math inline">\(\lambda_{1,2} = -1\)</span>. The eigenvectors associated with <span class="math inline">\(-1\)</span> satisfy the eigenproblem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (-1) &amp;  0 \\
 0      &amp;  -1 - (-1) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
 0  &amp;  0 \\
 0 &amp;   0
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span> Now, the two rows vanished, leaving no condition at all on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. This means that all the vectors are eigenvectors! How many linearly independent eigenvectors can we find? Vectors of size 2 live in a vector space of dimension 2; we can find at most 2 linearly independent vectors. We can choose for instance the canonical basis: <span class="math inline">\(x = (1,0)^t\)</span> and <span class="math inline">\(y = (0,1)^t\)</span>.</p>
<hr />
<p><strong>Tips on eigenvalue decomposition</strong></p>
<ul>
<li><p>A <span class="math inline">\(2 \times 2\)</span> matrix (or any square matrix) admits an eigenvalue decomposition if all the eigenvalues are distinct. For <span class="math inline">\(2 \times 2\)</span> matrices, eigenvalues are distinct if and only if the discriminant <span class="math inline">\(\Delta \neq 0\)</span>.</p></li>
<li><p>If the matrix has a repeated eigenvalue, it will admit an eigenvalue decomposition if the number of (linearly independent) eigenvectors is equal to the number of times the eigenvalue is repeated. The number of eigenvectors is called geometric multiplicity, and the number of repeats is called algebraic multiplicity.</p></li>
<li><p>The eigenproblem should be underdetermined; you should always be able to eliminate at least one row by linear combination. If you cannot, this means that there is a error, possibly an incorrect eigenvalue, or a arithmetic mistake in computing <span class="math inline">\(A - \lambda I\)</span>.</p></li>
<li><p>Because eigenvalues are in general complex, the eigenvectors will also be complex.</p></li>
<li><p>The eigenvector matrix <span class="math inline">\(X\)</span> needs to be inverted. When the eigenvectors can be chosen so that they are orthogonal and normalized, the inverse <span class="math inline">\(X^{-1} = X^*\)</span> (i.e. the conjugate transpose of <span class="math inline">\(X\)</span>). Symmetric matrices have orthogonal eigenvalues, so this class of matrices are especially easy to diagonalise.</p></li>
<li><p>Eigenvalue decomposition and invertibility are two different concepts. A matrix can be invertible without admitting an eigenvalue decomposition, and vice versa.</p></li>
<li><p>When a matrix does not admit an eigenvalue decomposition, it still can be triangularised. One such triangularisation is the Jordan decomposition: <span class="math inline">\(A = P(D+S)P^{-1}\)</span>, where <span class="math inline">\(P\)</span> is invertible, <span class="math inline">\(D\)</span> is the diagonal matrix of eigenvalues, and <span class="math inline">\(S\)</span> is a <strong>nilpotent</strong> matrix, i.e. a nonzero matrix such that <span class="math inline">\(S^k = 0\)</span> for <span class="math inline">\(k \geq k_0 &gt; 1\)</span>.</p></li>
</ul>
<hr />
<h2 id="exercises-on-eigenvalues-decomposition">Exercises on eigenvalues decomposition</h2>
<p><strong>Exercice </strong> Find, if there is any, an eigenvalue decomposition of <span class="math display">\[A = \begin{pmatrix}
-1  &amp;   2 \\
 2  &amp;  -1 
\end{pmatrix}\]</span> To compute <span class="math inline">\(X^{-1}\)</span>, you can use the fact that because <span class="math inline">\(A\)</span> is real and symmetrical, the eigenvectors are orthogonal, meaning that <span class="math inline">\(X^{-1} = X^t\)</span>, if the eignevectors are normalized.</p>
<h1 id="linearisation-of-functions-mathbbr2-to-mathbbr2">Linearisation of functions <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span></h1>
<p>Nonlinear systems of ordinary differential equations are used to describe the <strong>dynamics</strong> (evolution in time) of concentration of biochemical species, population densities in ecological systems, of the electrophyiology of neurons.</p>
<p>Two dimensional systems are described by two ordinary differential equations (ODEs) <span class="math display">\[\begin{aligned}
\frac{dx_1}{dt} &amp; = f_1(x_1,x_2), \\
\frac{dx_2}{dt} &amp; = f_2(x_1,x_2), \\\end{aligned}\]</span> The variables <span class="math inline">\(x_1, x_2\)</span> are functions of time: <span class="math inline">\(x_1(t)\)</span>, <span class="math inline">\(x_2(t)\)</span>, and <span class="math inline">\(f_1, f_2\)</span> are the derivatives. We define the two-dimensional vectors <span class="math inline">\(\boldsymbol{x} = (x_1,x_2)^t\)</span> (here we will use <strong>bold</strong> for vectors), and <span class="math inline">\(\boldsymbol{f} = (f_1, f_2)^t\)</span>. The ODEs can now be represented in vector format, <span class="math display">\[\frac{d\boldsymbol{x}}{dt} = \boldsymbol{f}(\boldsymbol{x}).\]</span> Here we assume that there exists a point in the 2D plane <span class="math inline">\(\bar{\boldsymbol{x}}\)</span> such that the derivative <span class="math inline">\(\boldsymbol{f}(\bar{\boldsymbol{x}}) = 0.\)</span> This point is called a <strong>steady state</strong> because the derivatives are all zeros; the steady state is therefore a solution to the system of ODE.</p>
<p>We are interested in how <span class="math inline">\(\boldsymbol{f}\)</span> is behaving around the steady state. To do that we linearize the function <span class="math inline">\(\boldsymbol{f}\)</span> at the steady state. <strong>Linearisation</strong> is a first-order expansion. For a function from <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span>, a first-order expansion around a point <span class="math inline">\(\boldsymbol{x}_0\)</span> is <span class="math display">\[\begin{aligned}
\boldsymbol{f}(\boldsymbol{x}) \approx \boldsymbol{f}(\boldsymbol{x}_0) + 
  \boldsymbol{D}\boldsymbol{f}(\boldsymbol{x}_0) (\boldsymbol{x}-\boldsymbol{x}_0) \end{aligned}\]</span> When expanding around a steady state, the constant term <span class="math inline">\(\boldsymbol{f}(\bar{\boldsymbol{x}}) = 0\)</span>. In the second term, <span class="math inline">\(\boldsymbol{D}\boldsymbol{f}\)</span> is a <span class="math inline">\(2 \times 2\)</span> matrix, called the Jacobian matrix, and often denoted <span class="math inline">\(\boldsymbol{J}\)</span>. The <strong>Jacobian matrix</strong> for the function <span class="math inline">\(\boldsymbol{f}\)</span> is defined as <span class="math display">\[\begin{aligned}
\boldsymbol{J} = \boldsymbol{D}\boldsymbol{f} = 
\begin{pmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} 
\end{pmatrix}.\end{aligned}\]</span> When evaluated at a steady state, the Jacobian matrix can provide information on the dynamics of the nonlinear ODE system. More precisely, the eigenvalues of the Jacobian matrix can determine whether the steady state is stable (attracts solutions) or is unstable. <strong>Linearisation around a steady state means computing the Jacbian matrix at the steady state.</strong></p>
<p><strong>Example </strong> Linearisation around a steady state</p>
<p>The Lotka-Volterra equations is a classical ODE system mathematical biology. The equations reads <span class="math display">\[\begin{aligned}
\frac{dx}{dt} = ax - xy, \\
\frac{dy}{dt} = xy - by, \\\end{aligned}\]</span> for <span class="math inline">\(a, b\)</span> positive constants. The solution vector is <span class="math inline">\(\boldsymbol{x} = (x,y)^t\)</span> and the derivatives are <span class="math inline">\(f_1(x,y) = ax - xy\)</span> and <span class="math inline">\(f_2 = xy - by\)</span>. We first look for steady states <span class="math display">\[f_1 = ax - xy = 0, \quad f_2 = x y - b y.\]</span> If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are not zero, we have <span class="math inline">\(x = b\)</span> and <span class="math inline">\(y = a\)</span>. If <span class="math inline">\(x = 0\)</span>, the second equation implies <span class="math inline">\(y = 0\)</span>. If <span class="math inline">\(y = 0\)</span>, the first equation implies <span class="math inline">\(x = 0\)</span>. Therefore there are two steady states, <span class="math inline">\(\bar{\boldsymbol{x}} = (b,a)^t\)</span> and <span class="math inline">\(\hat{\boldsymbol{x}} = (0,0)^t\)</span>.</p>
<p>We have the following derivatives <span class="math display">\[\begin{aligned}
\frac{\partial f_1}{\partial x} (x,y) &amp; = a - y, \\
\frac{\partial f_1}{\partial y} (x,y) &amp; = - x, \\
\frac{\partial f_2}{\partial x} (x,y) &amp; =   y, \\
\frac{\partial f_2}{\partial y} (x,y) &amp; = x - b, \\\end{aligned}\]</span> The Jacobian matrix is <span class="math display">\[\begin{aligned}
\boldsymbol{J} = 
\begin{pmatrix}
 a - y &amp; - x \\
  y    &amp; x - b 
\end{pmatrix}.\end{aligned}\]</span> Evaluated at the steady state <span class="math inline">\(\bar{\boldsymbol{x}} = (b,a)^t\)</span> and <span class="math inline">\(\hat{\boldsymbol{x}} = (0,0)^t\)</span>, the Jacobian matrices are <span class="math display">\[\begin{aligned}
\boldsymbol{J}(\bar{\boldsymbol{x}}) = 
\begin{pmatrix}
        0  &amp; - b \\
        a  &amp;   0 
\end{pmatrix}, \quad
\boldsymbol{J}(\hat{\boldsymbol{x}}) = 
\begin{pmatrix}
        a  &amp;  0  \\
        0  &amp; -b 
\end{pmatrix}.\end{aligned}\]</span></p>
<h2 id="exercises-on-linearisation">Exercises on linearisation </h2>
<p><strong>Exercice </strong> Let the function <span class="math inline">\(\boldsymbol{f} = (f_1, f_2)^t\)</span>, with <span class="math display">\[f_1(x,y) = -d x + x \exp(-a xy), \quad f_2(x,y) = x - y,\]</span> <span class="math inline">\(d &lt; 1\)</span>, <span class="math inline">\(a, d\)</span> positive. Find the steady states (by solving the equations <span class="math inline">\(f_1 = 0, f_2 = 0\)</span>). Compute the Jacobian matrix, and evaluate the Jacobian matrix at each steady state.</p>
<p><strong>Exercice </strong> Compute the Jacobian matrices of each of the following functions of <span class="math inline">\((x,y)\)</span>. All parameters are constants. You do not need to compute the steady states just the matrices.</p>
<ul>
<li><p>van der Pol oscillator <span class="math display">\[f_1(x,y) = \mu\bigl( (1-x^2)y - x \bigr), \; f_2(x,y) = y.\]</span></p></li>
<li><p>Two-compartment pharmacokinetics <span class="math display">\[f_1(x,y) = a - k_{12} x + k_{21} y - k_1 x, \; f_2(x,y) = k_{12} x - k_{21} y.\]</span></p></li>
<li><p>SI epidemiological model <span class="math display">\[f_1(x,y) = - \beta x y, \; f_2(x,y) = \beta x y - \gamma y.\]</span></p></li>
</ul>
<h1 id="solution-of-systems-of-linear-differential-equations-in-dimension-2">Solution of systems of linear differential equations in dimension 2</h1>
<p>Linear differential equations have linear derivative parts, which can be represented in matrix-vector format <span class="math display">\[\frac{d\boldsymbol{x}(t)}{dt} = \boldsymbol{A} \boldsymbol{x}(t),\]</span> for a vector <span class="math inline">\(\boldsymbol{x}\)</span> square matrix <span class="math inline">\(\boldsymbol{A}\)</span>. For initial conditions <span class="math inline">\(\boldsymbol{x}(t) = \boldsymbol{x}_0\)</span>, the <strong>solution of the linear system of ODEs</strong> is <span class="math display">\[\begin{aligned}
\boldsymbol{x}(t) = e^{\boldsymbol{A}t} \boldsymbol{x}_0.\end{aligned}\]</span> If we have at our disposal an eigenvalue decomposition of <span class="math inline">\(\boldsymbol{A} = \boldsymbol{X} \boldsymbol{D} \boldsymbol{X}^{-1}\)</span>, the <strong>exponential of the matrix</strong> is <span class="math display">\[\begin{aligned}
e^{\boldsymbol{A}t} &amp; = \boldsymbol{X} e^{\boldsymbol{D}t} \boldsymbol{X}^{-1}, \\
                    &amp; = \boldsymbol{X} 
                    \begin{pmatrix}
                      e^{ \lambda_1 t } &amp; 0 \\
                      0 &amp; e^{ \lambda_2 t }   
                    \end{pmatrix}
                    \boldsymbol{X}^{-1}.\end{aligned}\]</span> Therefore, the long-time behavior of the exponential is controlled by the eigenvalues <span class="math inline">\(\lambda_{1,2}\)</span>.</p>
<p><strong>Example </strong> Solution of a linear system of ODEs</p>
<p>Consider the linear system of ODEs given by the Lotka-Volterra model linearised at its nonzero steady state <span class="math inline">\(\bar{\boldsymbol{x}} = (b,a)^t\)</span> is <span class="math display">\[\begin{aligned}
\begin{pmatrix}
\frac{dx}{dt}  \\
\frac{dy}{dt}  
\end{pmatrix}
=
\begin{pmatrix}
   0 &amp; - b \\
   a &amp;  0 
\end{pmatrix}
\begin{pmatrix}
x  \\
y  
\end{pmatrix}, \quad 
\begin{pmatrix}
x(0)  \\
y(0)  
\end{pmatrix}
=
\begin{pmatrix}
x_0  \\
y_0  
\end{pmatrix}.
\label{eq_linearequation}\end{aligned}\]</span> This system approximates the nonlinear version near the steady state. In this linear system, variables <span class="math inline">\((x,y)\)</span> are deviations from the steady state; their solutions are "centered" around 0. To solve this linear system, we will diagonalise the matrix <span class="math display">\[\begin{aligned}
A = \begin{pmatrix}
  0 &amp; - b \\
  a &amp;  0 
\end{pmatrix}.\end{aligned}\]</span> The goal is to go slowly through every step once for this system. In general it is not necessary to solve the system completely by hand; knowledge of the eigenvalues is often sufficient in many applications.</p>
<p>We have <span class="math inline">\(\det A = 0 - a(-b) = ab &gt; 0\)</span>, <span class="math inline">\(\mathrm{tr}\,A = 0\)</span> and <span class="math inline">\(\Delta = 0 - 4ab = -4ab &lt; 0\)</span>. The eigenvalues are therefore complex conjugates: <span class="math inline">\(\lambda_{1,2} = \pm i \sqrt{ab}.\)</span> Distinct eigenvalues means that <span class="math inline">\(A\)</span> is diagonalisable. The eigenvector associated to <span class="math inline">\(\lambda_1 = i\sqrt{ab}\)</span> is given by the system <span class="math display">\[\begin{aligned}
\Biggl( 
\begin{array}{cc|c}
-i\sqrt{ab} &amp; -b          &amp; 0 \\
    a       &amp; -i\sqrt{ab} &amp; 0
\end{array}
\Biggr)\end{aligned}\]</span> We have from the first row <span class="math inline">\(-i\sqrt{ab} x = b y\)</span>. Letting <span class="math inline">\(x = b\)</span> and <span class="math inline">\(y = -i\sqrt{ab}\)</span>, we obtain the non-normalized eigenvector <span class="math inline">\(\tilde x_1 = (b, -i\sqrt{ab})^t\)</span>. Normalization is done by dividing by <span class="math display">\[||\tilde x || = \sqrt{b^2 + (-i\sqrt{ab})^2} = \sqrt{b^2 + ab},\]</span> to obtain the first eigenvector <span class="math display">\[x = 
\begin{pmatrix} 
\frac{b}{\sqrt{b^2 + ab}} \\
\frac{-i\sqrt{ab}}{\sqrt{b^2 + ab}}
\end{pmatrix}
=
\begin{pmatrix} 
\frac{b}{\sqrt{b}\sqrt{b + a}} \\
\frac{-i\sqrt{a}\sqrt{b}}{\sqrt{b}\sqrt{b + a}}
\end{pmatrix}
=
\begin{pmatrix} 
\frac{\sqrt{b}}{\sqrt{b + a}} \\
\frac{-i\sqrt{a}}{\sqrt{b + a}}
\end{pmatrix}.\]</span> The second eigenvector is computed the same way (watch out for the slightly different signs!). The eigenproblem for the eigenvalue <span class="math inline">\(\lambda = -i\sqrt{ab}\)</span> is <span class="math display">\[\begin{aligned}
\Biggl( 
\begin{array}{cc|c}
+i\sqrt{ab} &amp; -b          &amp; 0 \\
    a       &amp; +i\sqrt{ab} &amp; 0
\end{array}
\Biggr)\end{aligned}\]</span> Given that the only change is <span class="math inline">\(-i \to +i\)</span>, the second eigenvector is <span class="math display">\[x_2 = 
\begin{pmatrix} 
\frac{\sqrt{b}}{\sqrt{b + a}} \\
\frac{i\sqrt{a}}{\sqrt{b + a}}
\end{pmatrix}.\]</span> The solution to the linear ODE is <span class="math display">\[\begin{pmatrix}
x(t) \\ y(t)
\end{pmatrix}
=
\boldsymbol{X} e^{\boldsymbol{D}t} \boldsymbol{X}^{-1} 
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix},\]</span> with <span class="math display">\[\begin{aligned}
\boldsymbol{X} =
\frac{1}{\sqrt{b + a}}
\begin{pmatrix}
\sqrt{b}  &amp; \sqrt{b} \\
-i\sqrt{a} &amp; i\sqrt{a}
\end{pmatrix}, \quad
\boldsymbol{D} = 
\begin{pmatrix}
+i\sqrt{ab}  &amp; 0 \\
 0 &amp; -i\sqrt{ab}
\end{pmatrix}\end{aligned}\]</span> The <strong>inverse of a <span class="math inline">\(2 \times 2\)</span> matrix</strong> with coefficients <span class="math inline">\(a,b,c,d\)</span> is <span class="math display">\[\begin{aligned}
\begin{pmatrix}
a  &amp; b \\
c &amp;  d
\end{pmatrix}^{-1} =
\frac{1}{ad-bc}
\begin{pmatrix}
d  &amp; -b \\
-c &amp;  a
\end{pmatrix}.\end{aligned}\]</span> This is conditional to <span class="math inline">\(\det = ad - bc \neq 0\)</span>, of course. With this formula, the inverse of <span class="math inline">\(\boldsymbol{X}\)</span> is <span class="math display">\[\boldsymbol{X}^{-1} =
\frac{1}{\sqrt{b + a}} \frac{1}{\det \boldsymbol{X}}
\begin{pmatrix}
i \sqrt{a} &amp; -\sqrt{b} \\
i \sqrt{a} &amp; \sqrt{b}
\end{pmatrix}.\]</span> The determinant <span class="math inline">\(\det \boldsymbol{X} = \frac{i \sqrt{b} \sqrt{a}}{b + a} + \frac{i \sqrt{a} \sqrt{b}}{b + a} = 2i \frac{\sqrt{ab}}{b + a}.\)</span> The inverse reduces to <span class="math display">\[\frac{1}{\sqrt{b + a}} \frac{a + b}{2i\sqrt{ab}}
\begin{pmatrix}
i \sqrt{a} &amp; -\sqrt{b} \\
i \sqrt{a} &amp; \sqrt{b}
\end{pmatrix} 
=
\frac{-i \sqrt{b + a}}{2\sqrt{ab}}
\begin{pmatrix}
i \sqrt{a} &amp; -\sqrt{b} \\
i \sqrt{a} &amp; \sqrt{b}
\end{pmatrix} 
=
\frac{\sqrt{b + a}}{2\sqrt{ab}}
\begin{pmatrix}
 \sqrt{a} &amp;  i\sqrt{b} \\
 \sqrt{a} &amp; -i\sqrt{b}
\end{pmatrix}.\]</span> We have now obtained the eigenvalue decompostion of <span class="math inline">\(\boldsymbol{A} = \boldsymbol{X} \boldsymbol{D} \boldsymbol{X}^{-1}\)</span>. To solve the linear ODE, we need to compute the product <span class="math display">\[\begin{aligned}
\begin{pmatrix}
x(t) \\ y(t)
\end{pmatrix}
 &amp; =
\boldsymbol{X} e^{\boldsymbol{D}t} \boldsymbol{X}^{-1} 
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}, \\
 &amp; =
\frac{1}{\sqrt{b + a}}
\begin{pmatrix}
\sqrt{b}  &amp; \sqrt{b} \\
-i\sqrt{a} &amp; i\sqrt{a}
\end{pmatrix}
\begin{pmatrix}
e^{+i\sqrt{ab}t}  &amp; 0 \\
 0 &amp; e^{-i\sqrt{ab}t}
\end{pmatrix}
\frac{\sqrt{b + a}}{2\sqrt{ab}}
\begin{pmatrix}
 \sqrt{a} &amp;  i\sqrt{b} \\
 \sqrt{a} &amp; -i\sqrt{b}
\end{pmatrix}
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}, \\
 &amp; =
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  &amp; \sqrt{b} \\
-i\sqrt{a} &amp; i\sqrt{a}
\end{pmatrix}
\begin{pmatrix}
e^{+i\sqrt{ab}t}  &amp; 0 \\
 0 &amp; e^{-i\sqrt{ab}t}
\end{pmatrix}
\begin{pmatrix}
 \sqrt{a} &amp;  i\sqrt{b} \\
 \sqrt{a} &amp; -i\sqrt{b}
\end{pmatrix}
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}, \\
 &amp; =
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b} e^{+i\sqrt{ab}t}  &amp; \sqrt{b} e^{-i\sqrt{ab}t}\\
-i\sqrt{a} e^{+i\sqrt{ab}t}  &amp; i\sqrt{a} e^{-i\sqrt{ab}t}
\end{pmatrix}
\begin{pmatrix}
 \sqrt{a}x_0+i\sqrt{b}y_0 \\
 \sqrt{a}x_0-i\sqrt{b}y_0
\end{pmatrix}. \\\end{aligned}\]</span> To simplify the last steps of the calculation, we will introduce the following notation. Using Euler’s formula, <span class="math inline">\(e^{\pm i\sqrt{ab}t} = \cos(\sqrt{ab}t) \pm i \sin(\sqrt{ab}t)\)</span>. Let <span class="math inline">\(c = \cos(\sqrt{ab}t)\)</span>, <span class="math inline">\(s = \sin(\sqrt{ab}t)\)</span>, and <span class="math inline">\(C_1 = \sqrt{a}x_0+i\sqrt{b}y_0\)</span>, <span class="math inline">\(C_2 = \sqrt{a}x_0-i\sqrt{b}y_0\)</span>. The solution reads <span class="math display">\[\begin{aligned}
\begin{pmatrix}
x(t) \\ y(t) 
\end{pmatrix} &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  e^{i\sqrt{ab}t} C_1 + \sqrt{b} e^{-i\sqrt{ab}t} C_2 \\
-i\sqrt{a}e^{i\sqrt{ab}t} C_1 + i\sqrt{a}e^{-i\sqrt{ab}t} C_2  
\end{pmatrix}, \\
 &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  (c + is) C_1 + \sqrt{b} (c - is) C_2 \\
-i\sqrt{a}(c + is) C_1 + i\sqrt{a}(c - is) C_2  
\end{pmatrix}, \\
 &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  c (C_1 + C_2) + i \sqrt{b} s (C_1 - C_2) \\
\sqrt{a}  s (C_1 + C_2) + i \sqrt{a} c (-C_1 + C_2)  
\end{pmatrix}, \\
 &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
2 \sqrt{ab} \cos(\sqrt{ab}t) x_0 - 2 b \sin(\sqrt{ab}t) y_0 \\
2 a \sin(\sqrt{ab}t)x_0  + 2 \sqrt{ab} \cos(\sqrt{ab}t) y_0  
\end{pmatrix}, \\
 &amp; = 
\begin{pmatrix}
\cos(\sqrt{ab}t) x_0 - \sqrt{b/a} \sin(\sqrt{ab}t) y_0 \\
\sqrt{a/b} \sin(\sqrt{ab}t)x_0  +  \cos(\sqrt{ab}t) y_0  
\end{pmatrix}.\end{aligned}\]</span> And that’s it! We have obtained a solution to the linear ODE (Figure <a href="#f_sol_linear_ode" data-reference-type="ref" data-reference="f_sol_linear_ode">8</a>).</p>
<figure>
<img src="htmltmp/tikzblocks_29.pdf.png" id="f_sol_linear_ode" alt="Solution of the linear system of ODEs ([eq_linearequation]), with a = 0.1, b = 0.4. " /><figcaption aria-hidden="true">Solution of the linear system of ODEs (<a href="#eq_linearequation" data-reference-type="ref" data-reference="eq_linearequation">[eq_linearequation]</a>), with <span class="math inline">\(a = 0.1\)</span>, <span class="math inline">\(b = 0.4\)</span>. </figcaption>
</figure>
<h1 id="glossary">Glossary</h1>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">French</th>
<th style="text-align: left;">English</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">dérivable</td>
<td style="text-align: left;">differentiable</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">matrice jacobienne</td>
<td style="text-align: left;">Jacobian matrix</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">rang</td>
<td style="text-align: left;">rank</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">noyau</td>
<td style="text-align: left;">kernel</td>
<td style="text-align: left;">notation: <span class="math inline">\(\ker\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ensemble</td>
<td style="text-align: left;">set</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">espace vectoriel</td>
<td style="text-align: left;">vector space</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">sous-espace vectoriel</td>
<td style="text-align: left;">linear subspace</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">valeur propre</td>
<td style="text-align: left;">eigenvalue</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vecteur propre</td>
<td style="text-align: left;">eigenvector</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">sous-espace propre</td>
<td style="text-align: left;">eigenspace</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">décomposition en valeurs propres</td>
<td style="text-align: left;">eigenvalue decomposition</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">décomposition en valeurs singulières</td>
<td style="text-align: left;">singular value decomposition</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">valeur singulière</td>
<td style="text-align: left;">singular value</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">trace</td>
<td style="text-align: left;">trace</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">déterminant</td>
<td style="text-align: left;">determinant</td>
<td style="text-align: left;"><span class="math inline">\(\det\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">base</td>
<td style="text-align: left;">basis</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">application linéaire</td>
<td style="text-align: left;">linear map</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">application</td>
<td style="text-align: left;">map</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">dimension</td>
<td style="text-align: left;">dimension</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">moindres carrés</td>
<td style="text-align: left;">least-squares</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">produit scalaire</td>
<td style="text-align: left;">scalar product</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Vect</td>
<td style="text-align: left;">Span</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">famille libre</td>
<td style="text-align: left;">linearly independent set</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">famille génératrice</td>
<td style="text-align: left;">spanning set</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>bernard@math.univ-lyon1.fr<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>pujo@math.univ-lyon1.fr<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
