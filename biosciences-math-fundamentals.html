<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Samuel Bernard" />
  <meta name="author" content="Laurent Pujo-Menjouet" />
  <meta name="dcterms.date" content="2021-09-27" />
  <title>Elements of Maths for Biology</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Elements of Maths for Biology</h1>
<p class="author">Samuel Bernard<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p class="author">Laurent Pujo-Menjouet<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p class="date">20210927</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#readme">README</a></li>
<li><a href="#fonctions-maps">Fonctions, maps</a>
<ul>
<li><a href="#usual-maps">Usual maps</a></li>
<li><a href="#exercises-on-functions">Exercises on functions</a></li>
</ul></li>
<li><a href="#derivatives">Derivatives</a>
<ul>
<li><a href="#list-of-common-derivatives">List of common derivatives</a></li>
<li><a href="#exercises-on-derivatives">Exercises on derivatives</a></li>
</ul></li>
<li><a href="#integrals-and-primitives">Integrals and primitives</a></li>
<li><a href="#complex-numbers">Complex numbers</a>
<ul>
<li><a href="#roots-of-a-complex-number">Roots of a complex number</a></li>
<li><a href="#exercises-on-complex-numbers">Exercises on complex numbers</a></li>
</ul></li>
<li><a href="#matrices-in-dimension-2">Matrices in dimension 2</a>
<ul>
<li><a href="#eigenvalues-of-a-2-times-2-matrix">Eigenvalues of a <span class="math inline">\(2 \times 2\)</span> matrix</a>
<ul>
<li><a href="#exercises-on-eigenvalues">Exercises on eigenvalues</a></li>
</ul></li>
<li><a href="#matrix-vector-operations">Matrix-vector operations </a>
<ul>
<li><a href="#exercises-on-matrix-vector-and-matrix-matrix-operations">Exercises on Matrix-vector and matrix-matrix operations</a></li>
</ul></li>
</ul></li>
<li><a href="#eigenvalue-decomposition">Eigenvalue decomposition</a>
<ul>
<li><a href="#eigenvectors">Eigenvectors</a></li>
<li><a href="#exercises-on-eigenvalues-decomposition">Exercises on eigenvalues decomposition</a></li>
</ul></li>
<li><a href="#linearisation-of-functions-mathbbr2-to-mathbbr2">Linearisation of functions <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span></a>
<ul>
<li><a href="#exercises-on-linearisation">Exercises on linearisation </a></li>
</ul></li>
<li><a href="#solution-of-systems-of-linear-differential-equations-in-dimension-2">Solution of systems of linear differential equations in dimension 2</a></li>
<li><a href="#glossary">Glossary</a></li>
</ul>
</nav>
<h1 id="readme">README</h1>
<p>This document is intended to serve bith for training and for future reference. As a reference document, you may find it useful for the first biomaths class (3BS, Fall semester) and for linear algebra (3BIM, Winter Semester).</p>
<p>When important concepts are encoutered for the first time, they highlighted in <strong>bold</strong> next to their definition. Exercises are important, they can introduce theory or techniques that will be prove useful. We tried to make the examples as complete as possible. This means that they are long, you could probably solve them faster.</p>
<h1 id="fonctions-maps">Fonctions, maps</h1>
<p>A <strong>function</strong> is a relation, denoted in general <span class="math inline">\(f\)</span>, that associate an element <span class="math inline">\(x\)</span> belonging to a <strong>domain</strong> <span class="math inline">\(I\)</span>, and at most an element <span class="math inline">\(y\)</span> of the <strong>image</strong> <span class="math inline">\(J\)</span>. The domain <span class="math inline">\(I\)</span> and <span class="math inline">\(J\)</span> are sets, usually <span class="math inline">\(I, J \in \mathbb{R}\)</span>.</p>
<figure>
<img src="htmltmp/tikzblocks_0.pdf.png" id="f_functions" alt="Functions. (A) Function f. (B) Not a function. " /><figcaption aria-hidden="true">Functions. (A) Function <span class="math inline">\(f\)</span>. (B) Not a function. </figcaption>
</figure>
<p>A <strong>map</strong> is a relation that associate <em>each</em> element of its domain to exactly one element of its image. Maps and functions are related but slightly different concepts. A function <span class="math inline">\(f\)</span> is a map if it is defined for all elements of of its domain <span class="math inline">\(I\)</span>. A map is always a function, but the term can also be used when the domain or the image are not numbers (Figure <a href="#f_functions" data-reference-type="ref" data-reference="f_functions">1</a>).</p>
<p>The <strong>graph</strong> of a function <span class="math inline">\(f\)</span>, denoted <span class="math inline">\(\mathcal{G}(f)\)</span> is the set of all pairs <span class="math inline">\((x, f(x))\)</span> in the <span class="math inline">\(I \times J\)</span> plane. For real-valued functions, the graph is represented in the Cartesian plane.</p>
<p>Functions are not numbers. Do not confuse</p>
<ul>
<li><p><span class="math inline">\(f\)</span> the function</p></li>
<li><p><span class="math inline">\(f(x)\)</span> the evaluation of <span class="math inline">\(f\)</span> at element <span class="math inline">\(x\)</span>; <span class="math inline">\(f(x)\)</span> is an element of the image (usually a number)</p></li>
<li><p><span class="math inline">\(\mathcal{G}(f)\)</span> the graph of <span class="math inline">\(f\)</span>.</p></li>
</ul>
<p>Consequently, never write</p>
<ul>
<li><p><span class="math inline">\(f(x)\)</span> is increasing... but write <span class="math inline">\(f\)</span> is increasing...</p></li>
<li><p><span class="math inline">\(f(x)\)</span> is decreasing... but write <span class="math inline">\(f\)</span> is decreasing...</p></li>
<li><p><span class="math inline">\(f(x)\)</span> is continuous... but write <span class="math inline">\(f\)</span> continuous...</p></li>
</ul>
<h2 id="usual-maps">Usual maps</h2>
<ul>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to k\)</span>, <span class="math inline">\(k \in \mathbb{R}\)</span> constant; <span class="math inline">\(x \to x\)</span>, identity map.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_1.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_2.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}\backslash\{0\} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \frac 1x\)</span>, inverse.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_3.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to x^2\)</span>, parabola; <span class="math inline">\(x \to x^3\)</span>, cubic function.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_4.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_5.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}^+ \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \sqrt{x} = x^{\frac 12}\)</span>, square root; more generally with <span class="math inline">\(x \to x^{\frac pq} = {}^q\sqrt{x^p}\)</span>, fractional power.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_6.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_7.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}\backslash\{-d/c\} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \frac{ax + b}{cx + d}\)</span>.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_8.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \exp(x)\)</span>, exponential.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_9.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R}^+\backslash\{0\} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \ln(x)\)</span>, natural logarithm.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_10.pdf.png" alt="image" /></p>
</div>
<p>On logarithms: For <span class="math inline">\(a, b &gt;0\)</span>, <span class="math inline">\(n\)</span> positive integer, <span class="math inline">\(\ln(ab) = \ln(a) + \ln(b)\)</span>, <span class="math inline">\(\ln(a^n) = n \ln(a)\)</span>, <span class="math inline">\(\ln(a/b) = \ln(a) - \ln(b)\)</span>.</p></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \cos(x)\)</span>, cosine; <span class="math inline">\(x \to \sin(x)\)</span>, sine; <span class="math inline">\(x \to \tan(x)\)</span>, tangent.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_11.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_12.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_13.pdf.png" alt="image" /></p>
</div>
<p>On trigonometric functions. In the diagram below is shown the relationship between sine, cosine and tangent, of a angle <span class="math inline">\(\theta\)</span>.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_14.pdf.png" alt="image" /></p>
</div></li>
<li><p><span class="math inline">\(f:\mathbb{R} \to \mathbb{R}\)</span>, with <span class="math inline">\(x \to \cosh(x) = \frac 12 \bigl( e^{x} + e^{-x} \bigr)\)</span>, hyperbolic cosine; <span class="math inline">\(x \to \sinh(x) = \frac 12 \bigl( e^{x} - e^{-x} \bigr)\)</span>, hyperbolic sine; <span class="math inline">\(x \to \tanh(x) = \frac{\sinh(x)}{\cosh(x)}\)</span>, hyperbolic tangent.</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_15.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_16.pdf.png" alt="image" /> <img src="htmltmp/tikzblocks_17.pdf.png" alt="image" /></p>
</div></li>
</ul>
<h2 id="exercises-on-functions">Exercises on functions</h2>
<h1 id="derivatives">Derivatives</h1>
<p>We call the <strong>derivative</strong> of the function <span class="math inline">\(f:I \to J\)</span> (<span class="math inline">\(I,J \subset \mathbb{R}),\)</span>, at point <span class="math inline">\(a \in I\)</span> the limit, if it exists, <span class="math display">\[\lim_{x \to a} \frac{f(x) - f(a)}{x - a}.\]</span> The derivative is denoted <span class="math inline">\(f&#39;(a)\)</span>. An alternative representation of the limit is obtained by setting <span class="math inline">\(h = x - a\)</span>, <span class="math display">\[f&#39;(a) = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h}.\]</span> If the derivative exists for all elements <span class="math inline">\(a \in I\)</span>, we say that <strong>differentiable</strong> on <span class="math inline">\(I\)</span>.</p>
<ul>
<li><p>If <span class="math inline">\(f\)</span> is differentiable on <span class="math inline">\(I\)</span>, and <span class="math inline">\(f&#39;(x) &gt; 0\)</span>, then <span class="math inline">\(f\)</span> is strictly increasing on <span class="math inline">\(I\)</span>.</p></li>
<li><p>If <span class="math inline">\(f\)</span> is differentiable on <span class="math inline">\(I\)</span>, and <span class="math inline">\(f&#39;(x) &lt; 0\)</span>, then <span class="math inline">\(f\)</span> is strictly decreasing on <span class="math inline">\(I\)</span>.</p></li>
</ul>
<p>However, if <span class="math inline">\(f\)</span> is strictly increasing, it does not mean that <span class="math inline">\(f&#39;(x) &gt; 0\)</span>. For example the function <span class="math inline">\(f\)</span> with <span class="math inline">\(f(x) = x^3\)</span> is strictly increasing on <span class="math inline">\(\mathbb{R}\)</span>, but <span class="math inline">\(f&#39;(0) = 0\)</span>. Where the derivative exists, we can define the derivative function <span class="math inline">\(f&#39;:I \to \mathbb{R}\)</span> of <span class="math inline">\(f\)</span>.</p>
<p>The <strong>second derivative</strong> of a function <span class="math inline">\(f\)</span>, denoted <span class="math inline">\(f&#39;&#39;\)</span> is the derivative of <span class="math inline">\(f&#39;\)</span>, where defined. If <span class="math inline">\(f&#39;&#39;(x)\)</span> exists and <span class="math inline">\(f&#39;&#39;(x) &gt; 0\)</span> for all <span class="math inline">\(x \in I\)</span>, we say that <span class="math inline">\(f\)</span> is <strong>convex</strong> (U-shaped). If <span class="math inline">\(f&#39;(x) = 0\)</span> and <span class="math inline">\(f&#39;&#39;(x) &gt; 0\)</span>, the point <span class="math inline">\(x\)</span> is a <strong>minimum</strong>. If <span class="math inline">\(f&#39;(x)\)</span> and <span class="math inline">\(f&#39;&#39;(x) &lt; 0\)</span>, the point <span class="math inline">\(x\)</span> is a <strong>maxmimum</strong>. Maxima and minima are <strong>extrema</strong>. If <span class="math inline">\(f&#39;&#39;(0) = 0\)</span>, the point <span class="math inline">\(x\)</span> is an <strong>inflection point</strong> (Figure <a href="#f_extrema" data-reference-type="ref" data-reference="f_extrema">2</a>).</p>
<figure>
<img src="htmltmp/tikzblocks_18.pdf.png" id="f_extrema" alt="Extrema, inflection points of the polynomial f(x) = (x+0.8)(x-0.5)(x-0.8). " /><figcaption aria-hidden="true">Extrema, inflection points of the polynomial <span class="math inline">\(f(x) = (x+0.8)(x-0.5)(x-0.8).\)</span> </figcaption>
</figure>
<h2 id="list-of-common-derivatives">List of common derivatives</h2>
<p>The derivative is linear. If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are differentiable on <span class="math inline">\(I\)</span>, and <span class="math inline">\(a \in \mathbb{R}\)</span>,</p>
<ul>
<li><p><span class="math inline">\((f + g)&#39; = f&#39; + g&#39;\)</span>.</p></li>
<li><p><span class="math inline">\((af)&#39; = a (f&#39;)\)</span>.</p></li>
<li><p><span class="math inline">\((af + g)&#39;  = a (f&#39;) + g&#39;\)</span>.</p></li>
</ul>
<p>The derivative follow the <strong>rule of composed functions</strong>. If <span class="math inline">\(g:I \to J\)</span> and <span class="math inline">\(f:J \to K\)</span>, then <span class="math inline">\(f \circ g\)</span> is function <span class="math inline">\(x \to f(g(x))\)</span>. If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are differentiable, the derivative <span class="math display">\[\bigl( f \circ g \bigr)&#39;(x) = f&#39;(g(x))g&#39;(x).\]</span></p>
<p><strong>Example </strong> Let <span class="math inline">\(f:x \to x^2\)</span> and <span class="math inline">\(g:x \to 3x + 1\)</span>, two differentiable functions, with <span class="math inline">\(f&#39;(x) = 2x\)</span> and <span class="math inline">\(g&#39;(x) = 3\)</span>. The derivative of the composed function <span class="math inline">\(f \circ g\)</span> at <span class="math inline">\(x\)</span> is <span class="math display">\[f&#39;(g(x))g&#39;(x) = f&#39;(3x+1)g&#39;(x) = 2(3x+1) \cdot 3 = 6(3x+1) = 18x + 6.\]</span> The derivative could have been obtained by computing the composed function <span class="math inline">\(f(g(x)) = (3x+1)^2 =
9 x^2 + 6x + 1\)</span>.</p>
<p><strong>Example </strong> Compute the derivative of <span class="math inline">\(f: x \to \sin(1/x)\)</span>. The function <span class="math inline">\(f\)</span> is composed of a sine and an inverse function. To compute the derivative, we decomposed the function <span class="math inline">\(f\)</span> as <span class="math inline">\(f(x) = g(h(x))\)</span> with <span class="math inline">\(g(x) = \sin(x)\)</span> and <span class="math inline">\(h(x) = 1/x\)</span>. The derivatives <span class="math inline">\(g&#39;(x) = \cos(x)\)</span> and <span class="math inline">\(h&#39;(x) = -1/x^2\)</span>. <span class="math display">\[f&#39;(x) = g&#39;(h(x))h&#39;(x) = \cos(1/x) \Bigl( \frac{-1}{x^2} \Bigr) = - \frac{\cos(1/x)}{x^2}.\]</span></p>
<p><strong>Example </strong> A function <span class="math inline">\(f\)</span> is bijective (invertible) if there exists a function, denoted <span class="math inline">\(f^{-1}\)</span>, such that <span class="math inline">\(f \circ f^{-1} = f^{-1} \circ f\)</span> is the identity map. If <span class="math inline">\(f\)</span> is differentiable and invertible, what is the derivative of <span class="math inline">\(f^{-1}\)</span>?</p>
<p>We apply the derivative to <span class="math inline">\(f(f^{-1})\)</span>. Given that <span class="math inline">\(f(f^{-1}(x)) = x\)</span> by definition, we have <span class="math inline">\(\bigl( f(f^{-1}) \bigr)&#39; =  1,\)</span> and <span class="math display">\[\begin{aligned}
  \bigl( f(f^{-1}) \bigr)&#39;(x) &amp; = f&#39;(f^{-1}(x))(f^{-1})&#39;(x), \\
                              &amp; = 1, \\
            (f^{-1})&#39;(x)      &amp; = \frac{1}{f&#39;(f^{-1}(x))}.\end{aligned}\]</span> Take for instance <span class="math inline">\(f(x) = x^2\)</span> on <span class="math inline">\(x \in (0,1]\)</span>. The inverse is <span class="math inline">\(f^{-1}(x) = \sqrt{x}\)</span>. The derivative of <span class="math inline">\(f\)</span> is <span class="math inline">\(f(x) = 2x\)</span> and the derivative <span class="math display">\[f^{-1}(x) = \frac{1}{f&#39;(f^{-1}(x))} = \frac{1}{2(\sqrt{x})}.\]</span></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Derivative</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(x^a\)</span></td>
<td style="text-align: left;"><span class="math inline">\(ax^{a-1}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(a \in \mathbb{R}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\frac{1}{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{-1}{x^2}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(x^{\frac 12}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{2x^{\frac 12}}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\ln(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac 1x\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
<td style="text-align: left;"><span class="math inline">\(e^x\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\cosh(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sinh(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\sinh(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\cosh(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\cos(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(-\sin(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\sin(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\cos(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\dfrac{u(x)}{v(x)}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\dfrac{v(x)u&#39;(x) - u(x)v&#39;(x)}{v^2(x)}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(u(x) v(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(u&#39;(x)v(x) + u(x)v&#39;(x)\)</span></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h2 id="exercises-on-derivatives">Exercises on derivatives</h2>
<h1 id="integrals-and-primitives">Integrals and primitives</h1>
<h1 id="complex-numbers">Complex numbers</h1>
<p>A complex number is a number that can be expressed in the form <span class="math inline">\(a + i b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are real numbers, and the symbol <span class="math inline">\(i\)</span> is called <strong>imaginary unit</strong>. The imaginary unit satisfies the equation <span class="math inline">\(i^2 = -1\)</span>. Because no real number satisfies this equation, this number is called <em>imaginary</em>.</p>
<p>For the complex number <span class="math inline">\(z = a + i b\)</span>, <span class="math inline">\(a\)</span> is called the <strong>real part</strong> and <span class="math inline">\(b\)</span> is called the <strong>imaginary part</strong>. The real part of <span class="math inline">\(z\)</span> is denoted <span class="math inline">\(\Re(z)\)</span> (<code>\Re</code> in LaTeX) or just <span class="math inline">\(\mathrm{Re}(z).\)</span> The imaginary part of <span class="math inline">\(z\)</span> denoted <span class="math inline">\(\Im(z)\)</span> (<code>\Im</code> in LaTex) or just <span class="math inline">\(\mathrm{Im}(z)\)</span>. The set of all complex numbers is denoted <span class="math inline">\(\mathbb{C}\)</span> (<code>\mathbb{C}</code> in LaTeX).</p>
<p>We need complex numbers for solving polynomial equations. The fundamental theorem of algebra asserts that a polynomial equation of with real or complex coefficients has complex solutions. These polynomial equations arise when trying to compute the eigenvalues of matrices, something we need to do to solve linear differential equations for instance.</p>
<p>Arithmetic rules that apply on real numbers also apply on complex numbers, by using the rule <span class="math inline">\(i^2 = -1\)</span>: addition, subtraction, multiplication and division are associative, commutative and distributive.</p>
<p>Let <span class="math inline">\(u = a + i b\)</span> and <span class="math inline">\(v = c + i d\)</span> two complex numbers, with real coefficients <span class="math inline">\(a,b,c,d\)</span>. Then</p>
<ul>
<li><p><span class="math inline">\(u + v = a + i b + c + i d = (a+c) + i (b+d)\)</span>.</p></li>
<li><p><span class="math inline">\(uv = (a + i b)(c + i d) = ac + i a d + i b c + i^2 b d = ac - bd + i(ad + bc)\)</span>.</p></li>
<li><p><span class="math inline">\(\frac{1}{v} =  \frac{1}{c + i d} = \frac{c - id}{(c - id)(c + id)} = \frac{c - id}{c^2 + d^2} =
  \frac{c}{c^2 + d^2} - i \frac{d}{c^2+d^2}\)</span>.</p></li>
<li><p><span class="math inline">\(u = v\)</span> if and only if <span class="math inline">\(a = c\)</span> and <span class="math inline">\(b = d\)</span>.</p>
<p>It follows from the rule on <span class="math inline">\(i\)</span> that</p></li>
<li><p><span class="math inline">\(\frac 1i = -i.\)</span> (Proof: <span class="math inline">\(\frac 1i = \frac{i}{i^2} = \frac{i}{-1} = -i\)</span>.)</p></li>
</ul>
<p>Multiplying by the imaginary unit <span class="math inline">\(i\)</span> is equivalent to a counterclockwise rotation by <span class="math inline">\(\pi/2\)</span> (Figure <a href="#f_complex_rotation" data-reference-type="ref" data-reference="f_complex_rotation">3</a>) <span class="math display">\[ui = (a + ib)i = ia + i^2 b = -b + ia.\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_19.pdf.png" id="f_complex_rotation" alt="Rotation in the complex plane. Multiplication by i is a 90 degree counterclockwise rotation. " /><figcaption aria-hidden="true">Rotation in the complex plane. Multiplication by <span class="math inline">\(i\)</span> is a 90 degree counterclockwise rotation. </figcaption>
</figure>
<p>Let <span class="math inline">\(z = a + ib\)</span> a complex number with real <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The <strong>conjugate</strong> of <span class="math inline">\(z\)</span>, denoted <span class="math inline">\(\bar z\)</span>, is <span class="math inline">\(a - ib\)</span>. The conjugate of the conjugate of <span class="math inline">\(z\)</span> is <span class="math inline">\(z\)</span> (<em>reflection</em>, Figure <a href="#f_complex_rotation" data-reference-type="ref" data-reference="f_complex_rotation">3</a>). The <strong>modulus</strong> of <span class="math inline">\(z\)</span>, denoted <span class="math inline">\(|z|\)</span> is <span class="math inline">\(\sqrt{z \bar z}\)</span>. The product <span class="math inline">\(z \bar z = (a+ib)(a-ib)=a^2 + b^2 + i(-ab + ab) = a^2 + b^2\)</span>. The modulus is the complex version of the absolute value, for if <span class="math inline">\(z\)</span> (i.e. <span class="math inline">\(b = 0\)</span>), <span class="math inline">\(|z| = \sqrt{a^2} = |a|\)</span>. It is always a real, positive number, and <span class="math inline">\(|z| = 0\)</span> if and only if <span class="math inline">\(z = 0\)</span>. The modulus also has the property of being the <em>length</em> of the complex number <span class="math inline">\(z\)</span>, if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the sides of a rectangular triangle, then <span class="math inline">\(|z|\)</span> is the hypotenuse.</p>
<p>When simplifying a ratio involving a complex <span class="math inline">\(v\)</span> at the denominator, it is important to convert it to a real number by multiplying the ratio by <span class="math inline">\(\bar v/ \bar v\)</span>. For instance, if <span class="math inline">\(v \neq 0\)</span>, <span class="math display">\[\frac{u}{v} = \frac{u \bar v}{u \bar v} = \frac{u \bar v}{|v|^2}.\]</span> The denominator <span class="math inline">\(|v|^2\)</span> is always a positive real number.</p>
<p>By allowing complex values, nonlinear functions of real numbers like exponentials, logarithms and trigonometric functions can have their domain extended to all real and complex numbers. The most useful extension is the exponential function. Recall that the exponential function <span class="math inline">\(e^x\)</span>, where <span class="math inline">\(e \approx 2.71828\)</span> is Euler’s constant, satisfies the relation <span class="math inline">\(e^{x + y} = e^{x} e^{y}\)</span>. This remains true for complex numbers. The <strong>Euler’s formula</strong> relates the exponential of a imaginary number with trigonometric functions. For a real number <span class="math inline">\(y\)</span>, <span class="math display">\[e^{i y} = \cos(y) + i \sin(y).\]</span> Therefore, for any complex number <span class="math inline">\(z = a + i b\)</span>, the exponential <span class="math display">\[e^{z} = e^{a + ib} = e^a e^{ib} = e^a \bigl( \cos(b) + i \sin(b) \bigr).\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_20.pdf.png" id="f_complex_plane" alt="Complex plane" /><figcaption aria-hidden="true">Complex plane</figcaption>
</figure>
<hr />
<p><strong>Tips on complex numbers</strong></p>
<ul>
<li><p>If <span class="math inline">\(x\)</span> is real, <span class="math inline">\(ix\)</span> is pure imaginary. If <span class="math inline">\(y\)</span> is imaginary, <span class="math inline">\(iy\)</span> is real.</p></li>
<li><p><span class="math inline">\(|i| = 1\)</span>. For any real <span class="math inline">\(\theta\)</span>, <span class="math inline">\(|e^{i \theta}| = 1\)</span>.</p></li>
<li><p><span class="math inline">\(|z_1 z_2| = |z_1| |z_2|\)</span>.</p></li>
<li><p>In particular, <span class="math inline">\(|iz| = |i||z| = |z|\)</span>. (Multpliying by <span class="math inline">\(i\)</span> is a rotation in the complex plane, it does not change the modulus.)</p></li>
</ul>
<hr />
<h2 id="roots-of-a-complex-number">Roots of a complex number</h2>
<p>For complex numbers, the equation <span class="math inline">\(z^n = 1\)</span> has <span class="math inline">\(n\)</span> solutions. They are called the <strong>root of unity</strong>. For <span class="math inline">\(n=2\)</span>, we have the well-known roots <span class="math inline">\(z = \pm 1\)</span>, which are real. What are the roots of <span class="math inline">\(z^3 = 1\)</span>? To find them, we express <span class="math inline">\(z\)</span> in polar coordinates: <span class="math inline">\(z = r e^{i\theta}\)</span>. Then <span class="math display">\[z^3 = (r e^{i\theta})^3 = r^3 e^{i 3\theta} = 1.\]</span> The equation implies that <span class="math inline">\(z\)</span> has modulus 1, so <span class="math inline">\(r = 1\)</span>. The remaining term <span class="math inline">\(e^{i 3\theta} = 1\)</span> implies that <span class="math inline">\(3 \theta\)</span> is a multiple of <span class="math inline">\(2 \pi\)</span> because <span class="math inline">\(e^{i \omega} = 1\)</span> if and only if <span class="math inline">\(\omega = 2 k \pi\)</span>, for some integer <span class="math inline">\(k\)</span>. Therefore <span class="math inline">\(\theta = \frac{2}{3} k \pi\)</span>, for <span class="math inline">\(k = 0, 1, 2, ...\)</span>. How many distinct points do we have? Clearly, <span class="math inline">\(k = 3\)</span> is equivalent to <span class="math inline">\(k = 0\)</span>: <span class="math inline">\(e^{i \frac{2}{3} 3 \pi} = e^{i 2 \pi} = e{i 0}\)</span>. In the same way <span class="math inline">\(k = 4\)</span> is equivalent to <span class="math inline">\(k = 1\)</span>, and so on. Therefore, there are exactly three distinct solutions for <span class="math inline">\(\theta\)</span>: <span class="math inline">\(0, \frac{2}{3} \pi, \frac{4}{3} \pi\)</span> (Figure <a href="#f_roots_unity" data-reference-type="ref" data-reference="f_roots_unity">5</a>).</p>
<figure>
<img src="htmltmp/tikzblocks_21.pdf.png" id="f_roots_unity" alt="The roots of z^3 - 1." /><figcaption aria-hidden="true">The roots of <span class="math inline">\(z^3 - 1\)</span>.</figcaption>
</figure>
<h2 id="exercises-on-complex-numbers">Exercises on complex numbers</h2>
<p><strong>Exercice </strong> Let the complex number <span class="math inline">\(z = 2 + 3 i\)</span>. Compute <span class="math inline">\(\bar z\)</span>, <span class="math inline">\(|z|\)</span>, <span class="math inline">\(|\bar z|\)</span> (compare with <span class="math inline">\(|z|\)</span>), <span class="math inline">\(z^2\)</span>, <span class="math inline">\(\Re(\bar z)\)</span> , <span class="math inline">\(\Im(\bar z)\)</span>, <span class="math inline">\(\frac{z + \bar z}{2}\)</span> , <span class="math inline">\(\frac{z - \bar z}{2}\)</span> , <span class="math inline">\(-z\)</span>, <span class="math inline">\(iz\)</span>.</p>
<p><strong>Correction</strong> <span class="math inline">\(\bar z = 2 - 3i\)</span>, <span class="math inline">\(|z| = \sqrt{2^2 + 3^2} = \sqrt{13}\)</span>, <span class="math inline">\(|\bar z| = \sqrt{2^2 + (-3)^2} = \sqrt{13}\)</span>, we see that <span class="math inline">\(|z| = |\bar z|\)</span>, <span class="math inline">\(\Re(\bar z) = 2\)</span>, <span class="math inline">\(\Im(\bar z) = -3\)</span>, <span class="math inline">\(\frac{z + \bar z}{2} = (2 + 3i + (2 - 3i))/2 = 2\)</span>, <span class="math inline">\(\frac{z - \bar z}{2} = ((2 + 3i - (2 - 3i))/2 = 3i\)</span>, <span class="math inline">\(-z = -2 - 3i\)</span>, <span class="math inline">\(iz = 2i + 3i^2 = -3 + 2i\)</span>.</p>
<p><strong>Exercice </strong> Any complex number can be represented in <strong>polar form</strong>: <span class="math inline">\(z = r ( \cos(\theta) + i \sin(\theta) )\)</span>.</p>
<ul>
<li><p>Show that <span class="math inline">\(|z| = r\)</span></p></li>
<li><p>Show that <span class="math inline">\(z = r e^{i \theta}\)</span></p></li>
<li><p>Conclude that for any complex number <span class="math inline">\(z\)</span>, <span class="math inline">\(|z| = 1\)</span> if and only if <span class="math inline">\(z\)</span> can be expressed as <span class="math inline">\(z = e^{i \theta}\)</span> for a real <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p><strong>Correction</strong> The modulus of <span class="math inline">\(z\)</span> is <span class="math inline">\(|z| = \sqrt{r^2 \cos^2 (\theta) + r^2 \sin^2 (\theta)} = \sqrt{r^2} = r.\)</span> From Euler’s formula, we have <span class="math inline">\(\cos(\theta) + i\sin(\theta) = e^{i\theta}\)</span>, so <span class="math inline">\(z = re^{i\theta}.\)</span> Therefore, for any complex number <span class="math inline">\(z = r e^{i\theta}\)</span>, <span class="math inline">\(|z| = 1\)</span> if and only if <span class="math inline">\(r = 1\)</span>.</p>
<p><strong>Exercice </strong> Using Euler’s formula, show that <span class="math inline">\(\cos(a)\cos(b) - \sin(a)\sin(b) = \cos(a+b)\)</span>. <em>(Use the property that <span class="math inline">\(e^{ia + ib} = e^{ia} e^{ib}\)</span> and apply Euler’s Formula)</em>.</p>
<p><strong>Correction</strong> All trigonometric identities can be obtained by applying Euler’s formula. Here we start from <span class="math inline">\(e^{ia + ib} = \cos(a+b) + i \sin(a+b)\)</span>. We only want the real part, <span class="math display">\[\begin{aligned}
  \cos(a + b)  &amp; = &amp;&amp; \frac{e^{ia + ib} + e^{-ia - ib}}{2} \\
               &amp; = &amp;&amp; \frac{e^{ia} e^{ib} + e^{-ia} e^{-ib}}{2} \\
               &amp; = &amp;&amp; \frac{(\cos(a) + i \sin(a))(\cos(b)+ i \sin(b)) + (\cos(a) - i \sin(a))(\cos(b) - i \sin(b))}{2} \\
               &amp; = &amp;&amp; \frac{\cos(a)\cos(b) + i^2 \sin(a)\sin(b) + i \cos(a)\sin(b) + i \cos(b)\sin(a)}{2} \\ 
               &amp;   &amp;&amp;  + \frac{\cos(a)\cos(b) + i^2 \sin(a)\sin(b) - i \cos(a)\sin(b) - i \cos(b)\sin(a)}{2} \end{aligned}\]</span> The mixed cosine-sine terms cancel each other while the other ones add up, resuling in <span class="math display">\[\begin{aligned}
 \cos(a + b) = \cos(a)\cos(b) - \sin(a)\sin(b).\end{aligned}\]</span></p>
<p><strong>Exercice </strong> Show Euler’s identity: <span class="math inline">\(e^{i \pi} = -1\)</span>.</p>
<p><strong>Correction</strong> This is a direct application of Euler’s formula: <span class="math inline">\(e^{i \pi} = \cos(\pi) + i \sin(\pi) = -1.\)</span></p>
<p><strong>Exercice </strong> What are the roots of the equation <span class="math inline">\(z^6 = 1\)</span>?</p>
<p><strong>Correction</strong> The roots must satisfy <span class="math inline">\(e^{i 6 \theta} = 1\)</span>. This means that <span class="math inline">\(\theta = \frac{2}{6} k \pi\)</span>, for <span class="math inline">\(k = 0, 1, ..., 5\)</span>. There are six distinct roots.</p>
<p><strong>Exercice </strong> For a complex <span class="math inline">\(z\)</span>, find necessary and sufficient conditions for <span class="math inline">\(e^{z t}\)</span>, <span class="math inline">\(t &gt; 0\)</span>, to converge to 0.</p>
<p><strong>Correction</strong> The exponential converges to zero if and only if <span class="math inline">\(\Re(z) &lt; 0\)</span>. A complex number is close to zero if and only if its modulus is close to zero. Therefore, to show that a quantity converges to zero, it is necessary and sufficient to show that its modulus converges to zero. If <span class="math inline">\(z = a + ib\)</span>, the exponential <span class="math inline">\(e^{z t} = e^{(a + ib)t} = e^{at} e^{ibt}.\)</span> The modulus <span class="math inline">\(|e^{ibt}| = 1\)</span>, so <span class="math inline">\(|e^{zt}| = e^{at}\)</span> (no need for absolute values, the exponential of a real number is always positive). The condition for convergence to zero is therefore a condition on the real part of <span class="math inline">\(z\)</span>: <span class="math inline">\(e^{at} \to 0\)</span> when <span class="math inline">\(t \to \infty\)</span> if and only if <span class="math inline">\(a &lt; 0\)</span>.</p>
<p><strong>Exercice </strong> Let the complex number <span class="math inline">\(z = a + ib\)</span> with real <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Compute <span class="math inline">\(\sqrt{z}\)</span> (that is, express <span class="math inline">\(s = \sqrt{z}\)</span> as <span class="math inline">\(s = \alpha + i \beta\)</span>, with real <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>)</p>
<div class="center">
<p><img src="htmltmp/tikzblocks_22.pdf.png" alt="image" /></p>
</div>
<p><strong>Correction</strong> The square root of a complex number always exists. Express <span class="math inline">\(z\)</span> in polar form <span class="math inline">\(z = re^{i\theta}\)</span>, <span class="math inline">\(r \geq 0, \theta \in [0, 2\pi]\)</span>. The square root <span class="math inline">\(\sqrt{z} = \sqrt{r}\sqrt{e^{i\theta}} = \sqrt{r} e^{\frac 12 i \theta}\)</span>. Using Euler’s formula, <span class="math inline">\(\sqrt{z} = \sqrt{r} \cos(\theta/2) + i \sqrt{r} \sqrt{\theta/2}\)</span>. That is, the square root is obtained by taking the square root of the modulus <span class="math inline">\(r\)</span>, and dividing the angle (the <strong>argument</strong>) by 2. There is a problem with this solution, because <span class="math inline">\(z\)</span> can also be represented by <span class="math inline">\(re^{i\theta + 2\pi}\)</span>, giving <span class="math inline">\(\sqrt{z} = \sqrt{r} e^{\frac 12 i\theta + \pi}\)</span>, which is equivalent to dividing the angle by two in the other direction. We define the <strong>principal square root</strong> as the solution that makes the smallest change in angle: <span class="math inline">\(\sqrt{z} = \sqrt{r} e^{\frac 12 i\theta}\)</span> if <span class="math inline">\(\theta \in [0,\pi]\)</span>, and <span class="math inline">\(\sqrt{z} = \sqrt{r} e^{\frac 12 i\theta + \pi}\)</span> if <span class="math inline">\(\theta \in (\pi,2\pi]\)</span> To express the solution in terms of the original form of <span class="math inline">\(z = a + i b\)</span>, we express the square root <span class="math inline">\(s = \alpha + i \beta\)</span>. Then <span class="math inline">\(s^2 = \alpha^2 - \beta^2 + 2i \alpha \beta = z = a + ib.\)</span> By identifying the real and imaginary parts, we get two equations: <span class="math inline">\(\alpha^2 - \beta^2 = a\)</span> and <span class="math inline">\(2 i \alpha \beta = b.\)</span> Denoting the modulus of <span class="math inline">\(z\)</span> by <span class="math inline">\(r = \sqrt{a^2 + b^2}\)</span>, we can obtain the solutions <span class="math display">\[\alpha = \frac{1}{\sqrt{2}}\sqrt{a + r}, \quad \beta = \mathrm{sign}(b) \frac{1}{\sqrt{2}} \sqrt{-a + r}.\]</span></p>
<h1 id="matrices-in-dimension-2">Matrices in dimension 2</h1>
<h2 id="eigenvalues-of-a-2-times-2-matrix">Eigenvalues of a <span class="math inline">\(2 \times 2\)</span> matrix</h2>
<p>A <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(A\)</span> is an array with 2 rows and 2 columns: <span class="math display">\[A = \begin{pmatrix}
a &amp; b \\
c &amp; d 
\end{pmatrix}.\]</span> Usually, the <strong>coefficients</strong> <span class="math inline">\(a, b, c, d\)</span> are real numbers. The <strong>identity</strong> matrix is the matrix <span class="math display">\[I = \begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 
\end{pmatrix}.\]</span></p>
<p>The <strong>determinant</strong> of <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(\det A\)</span> or <span class="math inline">\(|A|\)</span> is the number <span class="math inline">\(ad - bc\)</span>. The <strong>trace</strong> of <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(\mathrm{tr}\,A\)</span>, is the sum of the main <strong>diagonal</strong> of <span class="math inline">\(A\)</span>: <span class="math inline">\(a + d\)</span>.</p>
<p>The <strong>characteristic polynomial</strong> of <span class="math inline">\(A\)</span> is the second order polynomial in <span class="math inline">\(\lambda\)</span> obtained by computing the determinant of the matrix <span class="math inline">\(A - \lambda I\)</span> (Figure <a href="#f_characteristic_polynomial" data-reference-type="ref" data-reference="f_characteristic_polynomial">6</a>), <span class="math display">\[\det ( A - \lambda I ) = (a-\lambda)(d-\lambda) - bc = ad - bc - \lambda ( a + d ) + \lambda^2.\]</span> The characteristic polynomial <span class="math inline">\(p_A(\lambda)\)</span> of <span class="math inline">\(A\)</span> can be expressed in terms of its determinant and trace: <span class="math display">\[p_A(\lambda) = \det A - \mathrm{tr}\,A \lambda + \lambda^2.\]</span> The <strong>eigenvalues</strong> of <span class="math inline">\(A\)</span> are the roots of the characteristic polynomial. By the fundamental theorem of algebra, we know that the characteristic polynomial has exactly two roots, counting multiple roots. These roots can be real, or complex. The eigenvalues of <span class="math inline">\(A\)</span> are calculated using the quadratic formula: <span class="math display">\[\lambda_{1,2} = \frac{1}{2} \Bigl( \mathrm{tr}\,A \pm \sqrt{ (\mathrm{tr}\,A)^2 - 4 \det A } \Bigr).\]</span></p>
<figure>
<img src="htmltmp/tikzblocks_23.pdf.png" id="f_characteristic_polynomial" alt="Graph of the characteristic polynomial. (a, green) polynomial with two negative roots, p(\lambda) = 0.08 + 0.8\lambda + \lambda^2. (b, gray) polynomial with two complex roots, p(\lambda) = 0.1  - 0.3\lambda + \lambda^2. (c, red) polynomial with two positive roots, p(\lambda) = 0.05 + 0.5\lambda + \lambda^2. (d, purple) polynomial with a negative and a positive root, p(\lambda) = -0.1 - 0.3\lambda + \lambda^2. " /><figcaption aria-hidden="true">Graph of the characteristic polynomial. (<em>a, green</em>) polynomial with two negative roots, <span class="math inline">\(p(\lambda) = 0.08 + 0.8\lambda + \lambda^2\)</span>. (<em>b, gray</em>) polynomial with two complex roots, <span class="math inline">\(p(\lambda) = 0.1  - 0.3\lambda + \lambda^2\)</span>. (<em>c, red</em>) polynomial with two positive roots, <span class="math inline">\(p(\lambda) = 0.05 + 0.5\lambda + \lambda^2\)</span>. (<em>d, purple</em>) polynomial with a negative and a positive root, <span class="math inline">\(p(\lambda) = -0.1 - 0.3\lambda + \lambda^2\)</span>. </figcaption>
</figure>
<p>From this formula, we can classify the eigenvalues of <span class="math inline">\(A\)</span>. Let <span class="math display">\[\Delta = (\mathrm{tr}\,A)^2 - 4 \det A\]</span> the <strong>discriminant</strong> of the quadratic formula. The two eigenvalues of <span class="math inline">\(A\)</span> are real if and only if <span class="math inline">\(\Delta \geq 0\)</span>, i.e. <span class="math inline">\(\mathrm{tr}\,A)^2 \geq 4 \det A\)</span> Then we have the following properties (Figure <a href="#f_eigenvalues" data-reference-type="ref" data-reference="f_eigenvalues">7</a>):</p>
<ol>
<li><p><span class="math inline">\(\Delta &lt; 0\)</span>, complex eigenvalues</p>
<ul>
<li><p>The two eigenvalues are complex conjugate: <span class="math inline">\(\lambda_1 = \bar \lambda_2\)</span></p></li>
<li><p>Their real part <span class="math inline">\(\Re(\lambda) = \frac{1}{2} \mathrm{tr}\,A\)</span>.</p></li>
</ul></li>
<li><p><span class="math inline">\(\Delta = 0\)</span>, there is a single root of multiplicity 2: <span class="math inline">\(\lambda = \frac{1}{2} \mathrm{tr}\,A\)</span>.</p></li>
<li><p><span class="math inline">\(\Delta &gt; 0, \det A &gt; 0\)</span>, real, distinct eigenvalues of the same sign.</p>
<ul>
<li><p><span class="math inline">\(\mathrm{tr}\,A &gt; 0\)</span> and <span class="math inline">\(\det A &gt; 0\)</span>. Then <span class="math inline">\(\lambda_{1,2}\)</span> are distinct and positive.</p></li>
<li><p><span class="math inline">\(\mathrm{tr}\,A &lt; 0\)</span> and <span class="math inline">\(\det A &gt; 0\)</span>. Then <span class="math inline">\(\lambda_{1,2}\)</span> are distinct and negative.</p></li>
</ul></li>
<li><p><span class="math inline">\(\det A &lt; 0\)</span>, real distinct eigenvalues of opposite sign.</p>
<ul>
<li><p><span class="math inline">\(\lambda_1 &lt; 0 &lt; \lambda_2\)</span>.</p></li>
</ul></li>
<li><p><span class="math inline">\(\det A = 0\)</span> one of the eigenvalue is zero, the other eigenvalue is <span class="math inline">\(\mathrm{tr}\,A\)</span>.</p></li>
</ol>
<figure>
<img src="htmltmp/tikzblocks_24.pdf.png" id="f_eigenvalues" alt="Properties of the eigenvalues of a 2 \times 2 matrix A with respect to \det A and \mathrm{tr}\,A." /><figcaption aria-hidden="true">Properties of the eigenvalues of a <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(A\)</span> with respect to <span class="math inline">\(\det A\)</span> and <span class="math inline">\(\mathrm{tr}\,A\)</span>.</figcaption>
</figure>
<h3 id="exercises-on-eigenvalues">Exercises on eigenvalues</h3>
<p><strong>Exercice </strong> Properties of the eigenvalues of <span class="math inline">\(2 \times 2\)</span> matrices. For each <span class="math inline">\(2 \times 2\)</span> matrix, compute the determinant, the trace, and the discriminant, and determine whether the eigenvalues are real, complex, distinct, and the sign (negative, positive, or zero) of the real parts.</p>
<p><span class="math display">\[A_1 = \begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}, \quad
A_2 = \begin{pmatrix}
-2 &amp;  1 \\
 1 &amp; -2 
\end{pmatrix}, \quad
A_3 = \begin{pmatrix}
 1 &amp; -2 \\
 0 &amp;  1 
\end{pmatrix}, \quad
A_4 = \begin{pmatrix}
-1 &amp;  2 \\
1/2&amp;  2 
\end{pmatrix}.\]</span></p>
<h2 id="matrix-vector-operations">Matrix-vector operations </h2>
<p>A matrix defines a linear transformation between vector spaces. Given a vector <span class="math inline">\(x\)</span>, the product <span class="math inline">\(Ax\)</span> is vector composed of linear combinations of the coefficients of <span class="math inline">\(x\)</span>. For a matrix <span class="math inline">\(2 \times 2\)</span>, the vector <span class="math inline">\(x\)</span> must be a vector of size 2, and the product <span class="math inline">\(Ax\)</span> is a vector of size two. If <span class="math inline">\(x = (x_1, x2)^t\)</span> (the <span class="math inline">\({}^t\)</span> stands for the transpose, because <span class="math inline">\(x\)</span> must be a column vector), and <span class="math inline">\(A = [ a_{ij} ]_{i=1,2, \, j=1,2}\)</span>, then <span class="math display">\[Ax = 
\begin{pmatrix}
a_{11} &amp;  a_{12} \\
a_{21} &amp;  a_{22} 
\end{pmatrix}
\begin{pmatrix}
x_{1}  \\
x_{2}  
\end{pmatrix}
=
\begin{pmatrix}
a_{11} x_{1} + a_{12} x_2  \\
a_{21} x_{1} + a_{22} x_2 
\end{pmatrix}.\]</span> Successive linear transformations can be accomplished by applying several matrices. Given two matrices <span class="math inline">\(A, B\)</span>, the matrix product <span class="math inline">\(C = AB\)</span> is also a matrix. The matrix <span class="math inline">\(C\)</span> is the linear transformation that first applies <span class="math inline">\(B\)</span>, then <span class="math inline">\(A\)</span>. Matrix product is *not* commutative is general: <span class="math inline">\(AB \neq BA\)</span>. *(If <span class="math inline">\(B\)</span> means ’put on socks’ and <span class="math inline">\(A\)</span> means ’put on shoes’, then <span class="math inline">\(BA\)</span> does not have the expected result.)* The product of two matrices <span class="math inline">\(A = [ a_{ij} ]_{i=1,2, \, j=1,2}\)</span> and <span class="math inline">\(B = [ b_{ij} ]_{i=1,2, \, j=1,2}\)</span> is <span class="math display">\[AB = 
\begin{pmatrix}
a_{11} &amp;  a_{12} \\
a_{21} &amp;  a_{22} 
\end{pmatrix}
\begin{pmatrix}
b_{11} &amp;  b_{12} \\
b_{21} &amp;  b_{22} 
\end{pmatrix}
=
\begin{pmatrix}
a_{11} b_{11} + a_{12}b_{21} &amp; a_{11}b_{12} + a_{12}b_{22} \\
a_{21} b_{11} + a_{22}b_{21} &amp; a_{21}b_{12} + a_{22}b_{22}
\end{pmatrix}.\]</span> The <strong>sum of two matrices</strong> <span class="math inline">\(A+B\)</span> is performed element-wise: <span class="math inline">\(A+B = [a_{ij} + b_{ij}]_{i=1,2, \, j=1,2}\)</span>. The <strong>sum of two vectors</strong> is defined similarly. Addition is commutative. Matrix operations are associative and distributive. <span class="math display">\[\begin{aligned}
A + B &amp; = B + A, \\
A(B + C) &amp; = AB + BC, \\
A(BC)  &amp; = (AB)C. \end{aligned}\]</span> Matrices and vectors can be multiplied by a scalar value (real or complex). <strong>Multiplication by a scalar</strong> is associative, distributive, and commutative. The result of the multiplication by a scalar is to multiply each coefficient of the matrix or vector by the scalar. For example, if <span class="math inline">\(\lambda, \mu\)</span> are scalars, <span class="math display">\[\begin{aligned}
\lambda A &amp; = A (\lambda I) = A \lambda, \\
\lambda ( A + B ) &amp; = \lambda A + \lambda B, \\
(\lambda A) B &amp; = \lambda (AB) = A (\lambda B), \\
(\mu + \lambda) A &amp; = \mu A + \lambda A, \\
\mu (\lambda A) &amp; = (\mu \lambda) A, ...\end{aligned}\]</span> The product between two column vectors is not defined, because the sizes do not match. However, we can define the <strong>scalar product</strong> between two column vectors <span class="math inline">\(x, y\)</span> in the same way matrix product is defined: <span class="math display">\[x^ty \equiv x_1 y_1 + x_2 y_2.\]</span></p>
<p>If the vectors are complex-valued, we need also to conjugate the transposed vector <span class="math inline">\(x^t\)</span>. The conjugate-transpose is called the <strong>adjoint</strong> and is denoted <span class="math inline">\({}^*\)</span>. Thus, if <span class="math inline">\(x\)</span> is complex-valued, the adjoint <span class="math inline">\(x^*\)</span> is the row vector <span class="math inline">\((\bar x_1, \bar x_2)\)</span>. The scalar product for complex-valued vectors is denoted <span class="math inline">\(x^*y\)</span>. Since this notation also works for real-valued vector, we will used most of the time.</p>
<p>Two vectors are <strong>orthogonal</strong> if their scalar product is 0. In the plane, this means that they are oriented at 90 degree apart. Orthogonal vectors are super important because they can be used to build orthogonal bases that are necessary for solving all sorts of *linear problems*.</p>
<h3 id="exercises-on-matrix-vector-and-matrix-matrix-operations">Exercises on Matrix-vector and matrix-matrix operations</h3>
<p><strong>Exercice </strong> Compute matrix-vector product <span class="math display">\[\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}.\]</span> What is the transformation given by this matrix.</p>
<p><strong>Correction</strong> <span class="math display">\[\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
= 
\begin{pmatrix}
-x_2 \\ x_1
\end{pmatrix}.\]</span> The transformation is a 90 degree counterclockwise rotation.</p>
<p><strong>Exercice </strong> Compute the matrix-matrix product <span class="math display">\[\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}
\begin{pmatrix}
1 &amp;  0 \\
0 &amp; -1 
\end{pmatrix}.\]</span> Can you tell what transformation this is?</p>
<p><strong>Correction</strong> <span class="math display">\[\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}
\begin{pmatrix}
1 &amp;  0 \\
0 &amp; -1 
\end{pmatrix} = 
\begin{pmatrix}
0 &amp;  1 \\
1 &amp;  0 
\end{pmatrix}.\]</span> This matrix exchanges the coordinates of a vector, this is a reflection through the axis <span class="math inline">\(x = y\)</span>.</p>
<p><strong>Exercice </strong> Now compute the product of the same matrices, but in the inverse order <span class="math display">\[\begin{pmatrix}
1 &amp;  0 \\
0 &amp; -1 
\end{pmatrix}
\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix}.\]</span> Compare with the solution found in the previous exercise. What is this transformation?</p>
<p><strong>Correction</strong> <span class="math display">\[\begin{pmatrix}
1 &amp;  0 \\
0 &amp; -1 
\end{pmatrix}
\begin{pmatrix}
0 &amp; -1 \\
1 &amp;  0 
\end{pmatrix} =
\begin{pmatrix}
0 &amp; -1 \\
-1 &amp; 0 
\end{pmatrix}.\]</span> The pruduct is not the same, the matrices do not commute. The transformation is now a reflection through <span class="math inline">\(y = -x\)</span>.</p>
<p><strong>Exercice </strong> Find the matrix that takes a vector <span class="math inline">\(x = (x_1,x_2)^t\)</span> and returns <span class="math inline">\((a x_1, b x_2)^t\)</span>.</p>
<p><strong>Correction</strong> The matrix is <span class="math display">\[\begin{pmatrix}
a &amp;  0 \\
0  &amp; b 
\end{pmatrix}.\]</span></p>
<p><strong>Exercice </strong> Find the matrix that takes a vector <span class="math inline">\(x = (x_1,x_2)^t\)</span> and returns <span class="math inline">\((x_2, x_1)^t\)</span>.</p>
<p><strong>Correction</strong> The matrix is <span class="math display">\[\begin{pmatrix}
  0  &amp; 1 \\
  1  &amp; 0
\end{pmatrix}.\]</span></p>
<p><strong>Exercice </strong> Find the matrix that takes a vector <span class="math inline">\(x = (x_1,x_2)^t\)</span> and returns <span class="math inline">\((x_2, 0)^t\)</span>.</p>
<p><strong>Correction</strong> The matrix is <span class="math display">\[\begin{pmatrix}
  0  &amp; 1 \\
  0  &amp; 0
\end{pmatrix}.\]</span></p>
<p><strong>Exercice </strong> Compute the successive powers <span class="math inline">\(A, A^2, A^3, ...\)</span>, for a diagonal matrix <span class="math inline">\(A\)</span>: <span class="math display">\[A =\begin{pmatrix}
a &amp;  0 \\
0 &amp;  b 
\end{pmatrix}\]</span></p>
<p><strong>Correction</strong> The power of a diagonal matrix is a diagonal matrix <span class="math display">\[A^k = 
\begin{pmatrix}
  a^k  &amp; 0 \\
  0  &amp; b^k
\end{pmatrix}.\]</span></p>
<p><strong>Exercice </strong> Compute the scalar product <span class="math inline">\(x^*y\)</span> between <span class="math inline">\(x = (1 + 2i, 1 - i)^t\)</span> and <span class="math inline">\(y = (0.5 - i, -0.5)^t\)</span>.</p>
<p><strong>Correction</strong> The scalar product is <span class="math display">\[\begin{aligned}
  x^*y &amp; = (1 - 2i, 1 + i) (0.5 - i, -0.5)^t \\ 
       &amp; = (1 - 2i)(0.5 - i) + (1 + i)(-0.5) \\
       &amp; = 0.5 + 2i^2 - 2(0.5)i - i -0.5 - 0.5i \\
       &amp; = (0.5 - 0.5 + 2i^2) + (-2(0.5) - 1 - 0.5)i \\
       &amp; = -2 - 2.5i.\end{aligned}\]</span></p>
<p><strong>Exercice </strong> Now compute the scalar product <span class="math inline">\(y^*x\)</span> and compare with the result with the previous exercise.</p>
<p><strong>Correction</strong> The scalar product is <span class="math display">\[\begin{aligned}
  y^*x &amp; = (0.5 + i, -0.5) (1 + 2i, 1 - i)^t \\ 
       &amp; = (0.5 + i)(1 + 2i) + (-0.5)(1 - i) \\
       &amp; = 0.5 + 2i^2 + i + 2(0.5)i - 0.5 + 0.5i \\
       &amp; = -2 + 2.5i\end{aligned}\]</span> This is the conjugate: <span class="math inline">\(x^*y = (y^*x)^*.\)</span></p>
<p><strong>Exercice </strong> Compute the scalar product between <span class="math inline">\(z = (z_1, z_2)^t\)</span> and itself, if <span class="math inline">\(z\)</span> is a complex-valued vector. What can you say about the result?</p>
<p><strong>Correction</strong> The scalar product <span class="math inline">\(z^*z = (\bar z_1, \bar z_2) (z_1, z_2)^t = \bar z_1 z_1 + \bar z_2 z_2 = |z_1|^2 + |z_2|^2.\)</span> The scalar product is the square of the norm of the vector <span class="math inline">\(z\)</span>.</p>
<hr />
<p><strong>Tips on eigenvalues</strong> Some matrices have special shapes that make it easier to compute the determinant, and the eigenvalues. These are called eigenvalue-revealing shapes.</p>
<ul>
<li><p>Diagonal matrices have their eigenvalues on the diagonal.</p></li>
<li><p><strong>Triangular matrices</strong>, i.e. matrices that have zeros above (lower-triangular matrix) or below (upper-triangular matrix) the main diagonal have also their eigenvalues on the diagonal.</p></li>
<li><p>A matrix with a row or a column of zeros has its determinant equal to zero. This implies that one of its eigenvalues is 0.</p></li>
</ul>
<hr />
<h1 id="eigenvalue-decomposition">Eigenvalue decomposition</h1>
<p>In many applications, it is useful to decompose a matrix into a form that makes it easier to operate complex operations on. For instance, we might want to compute the powers of a matrix <span class="math inline">\(A\)</span>: <span class="math inline">\(A^2\)</span>, <span class="math inline">\(A^3\)</span>, <span class="math inline">\(A^4\)</span>. Multiplying matrices are computationally intensive, especially when the size of the matrix becomes large. The <strong>power of a matrix</strong> is <span class="math inline">\(A^k = AA...A\)</span>, <span class="math inline">\(k\)</span> times. The zeroth power is the identity matrix: <span class="math inline">\(A^0 = I\)</span>.</p>
<p>The <strong>inverse</strong> of a matrix <span class="math inline">\(A\)</span>, denoted by <span class="math inline">\(A^{-1}\)</span> is the unique matrix such that <span class="math inline">\(AA^{-1} A{^-1}A = I\)</span>. The notation is self-consistent with the positive powers of <span class="math inline">\(A\)</span>. The inverse does not always exist. A matrix is <strong>invertible</strong> if and only if its determinant is not 0. If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are invertible, then <span class="math inline">\(AB\)</span> is invertible, and <span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span>.</p>
<p>The <strong>eigenvalue decomposition</strong> is a decomposition of the form <span class="math inline">\(A = X D X^{-1}\)</span>, where <span class="math inline">\(D\)</span> is a diagonal matrix, and <span class="math inline">\(X\)</span> is an invertible matrix. If there exists such a decomposition for <span class="math inline">\(A\)</span>, then computing powers of <span class="math inline">\(A\)</span> becomes easy: <span class="math display">\[\begin{aligned}
A^k &amp; = (XDX^{-1})^k = XDX^{-1} \, XDX^{-1} \, ... XDX^{-1}, \\
    &amp; = XD(X^{-1}X)D(X^{-1}X) D ... (X^{-1}X) DX^{-1}, \\
    &amp; = XD^kX^{-1}.\end{aligned}\]</span></p>
<p>The eigenvalue decomposition does not always exists, because it is not always possible to find an invertible matrix <span class="math inline">\(X\)</span>. When it exists, though, the columns of the matrix <span class="math inline">\(X\)</span> is composed of the eigenvectors of <span class="math inline">\(A\)</span>. When <span class="math inline">\(A\)</span> is a <span class="math inline">\(2 \times 2\)</span> matrix, it is enough to find 2 linearly independent eigenvectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> for the matrix <span class="math display">\[X = \Biggl( \begin{array}{c|c} x_1 &amp; y_1 \\ x_2 &amp; y_2 \end{array} \Biggr)\]</span> to be invertible.</p>
<h2 id="eigenvectors">Eigenvectors</h2>
<p>The <strong>eigenvectors</strong> of a matrix <span class="math inline">\(A\)</span> are the *nonzero* vectors <span class="math inline">\(x\)</span> such that for an eigenvalue <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(A\)</span>, <span class="math display">\[Ax = \lambda x.\]</span> If <span class="math inline">\(x\)</span> is an eigenvector, so is any <span class="math inline">\(\alpha x\)</span> for any scalar value <span class="math inline">\(\alpha\)</span>. If there are two linearly independent eigenvectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> associated to an eigenvalue, <span class="math inline">\(\alpha x + \beta y\)</span> is also an eigenvector. There is at least one eigenvector for each distinct eigenvalue, but there may be more than one when the eigenvalue is repeated.</p>
<p><strong>Example </strong> Distinct, real eigenvalues The matrix <span class="math display">\[A =\begin{pmatrix}
-1 &amp; -2 \\
0  &amp;  1 
\end{pmatrix}\]</span></p>
<p>is upper-triangular; this is one of the eigenvalue-relealing shapes. The eigenvalues are <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. These are distinct eigenvalues, so each eigenvalue possesses a single eigenvector. The eigenvector <span class="math inline">\(x\)</span> associated to <span class="math inline">\(\lambda_1 = -1\)</span> is found by solving the eigensystem <span class="math display">\[Ax = (-1)x.\]</span></p>
<p>The unknown quantity <span class="math inline">\(x\)</span> appears on both sides of the equation. We can find a simpler form by noting that multiplying a vector by the identity matrix is neutral: <span class="math inline">\((-1)x = (-1) I x.\)</span> The eigenproblem becomes <span class="math display">\[\begin{aligned}
A x &amp; = (-1) I x, \\
A x - (-1) I x = 0, \\
\bigl( A - (-1) I \bigr) x = 0,\end{aligned}\]</span></p>
<p>that is, the eigenvector is a nonzero solution of the linear system <span class="math inline">\(\bigl( A - \lambda I \bigr) x = 0\)</span>. In general, if a matrix <span class="math inline">\(B\)</span> is invertible, the only solution to <span class="math inline">\(Bx=0\)</span> is <span class="math inline">\(x = 0\)</span> (the vector of zeroes). But, by construction, <span class="math inline">\(A - \lambda I\)</span> cannot be invertible if <span class="math inline">\(\lambda\)</span> is an eigenvalue: its determinant is exactly the characteristic polynomial evaluated at one of its roots, so it is zero. This is why the eigensystem has nonzero solutions. Now, because <span class="math inline">\(A - \lambda I\)</span> is not invertible, this means that a least one of its rows is a linear combination of the others. For <span class="math inline">\(2 \times 2\)</span> matrices, this implies that the two rows are colinear, or redundant. For our example, the eigensystem reads <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (-1) &amp; -2 \\
0  &amp;  1 - (-1) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
0  &amp; -2 \\
0  &amp;  2 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>we immediately see that the two rows <span class="math inline">\((0,-2)\)</span> and <span class="math inline">\((0,2)\)</span> are colinear, with a factor <span class="math inline">\(-1\)</span>. This leads to an underdetermined system: <span class="math inline">\(0 x_1 + -2 x_2 = 0\)</span>. The solution is <span class="math inline">\(x_2 = 0\)</span> and we can take <span class="math inline">\(x_1\)</span> to be any value, save 0. We choose <span class="math inline">\(x = (1, 0)^t\)</span>.</p>
<p>For the eigenvalue <span class="math inline">\(\lambda_2 = +1\)</span>, the eigensystem reads: <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (+1) &amp; -2 \\
0  &amp;  1 - (+1) 
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
-2  &amp; -2 \\
0  &amp;   0
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>Again, the second row <span class="math inline">\((0,0)\)</span> can be neglected, and the solution is <span class="math inline">\(-2 y_1 + 2 y_2 = 0\)</span>, or <span class="math inline">\(y_1 = y_2\)</span>. It is customary to choose an eigenvector with norm 1. The <strong>norm</strong> of a complex-valued vector <span class="math inline">\(y = (y_1, y_2)^t\)</span> is <span class="math display">\[||y|| = \sqrt{y^*y} = \sqrt{\bar y_1 y_1 + \bar y_2 y_2} = \sqrt{|y_1|^2 + |y_2|^2}.\]</span></p>
<p>Here, the eigenvector is <span class="math inline">\(y = (y_1, y_1)^t\)</span>, so <span class="math inline">\(||y|| = \sqrt{|y_1|^2 + |y_1|^2} = \sqrt{2}\sqrt{|y_1|^2} = \sqrt{2}|y_1|.\)</span> Taking <span class="math inline">\(||y|| = 1\)</span> solves <span class="math inline">\(|y_1| = 1/\sqrt{2}.\)</span> This means that we could take a negative, or a complex value for <span class="math inline">\(y_1\)</span>, as long as the <span class="math inline">\(|y_1| = 1/\sqrt{2}.\)</span> Going for simplicity, we take <span class="math inline">\(y_1 = 1/\sqrt{2}\)</span>.</p>
<p><strong>Example </strong> Complex eigenvalues</p>
<p>The matrix <span class="math display">\[A =\begin{pmatrix}
0  &amp; -1 \\
1  &amp;  0 
\end{pmatrix}\]</span> is *not* diagonal, so we have to compute the eigenvalues by hand. The trace of <span class="math inline">\(A\)</span> is zero, the determinant is <span class="math inline">\(0 - (1)(-1) = 1\)</span>, and the discriminant is <span class="math inline">\(-4\)</span>. A negative discriminant implies complex eigenvalues, <span class="math display">\[\lambda_{1,2} = \frac 12 \bigl( 0 \pm \sqrt{-4} \bigr) = \pm i.\]</span> For the eigenvalue <span class="math inline">\(\lambda_1 = +i\)</span>, the eigensystem reads: <span class="math display">\[\begin{aligned}
\begin{pmatrix}
- (+i) &amp;  -1 \\
1      &amp;  - (+i) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
-i  &amp; -1 \\
1  &amp;  -i
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>The two rows <span class="math inline">\((-i,1)\)</span> and <span class="math inline">\((1,-i)\)</span> should be colinear, but this is not obvious with the complex coefficients. Multiplying the first row by <span class="math inline">\(i\)</span> gives <span class="math inline">\(i(-i, -1) = (- i^2, - i) = (-(-1), -i) = (1, -i)\)</span>, the second row, ok. Having confirmed that the system is indeed underdetermined, we can week a solution to <span class="math inline">\(-i x_1 - x_2 = 0\)</span>. Solving for <span class="math inline">\(x_2 = -i x_1\)</span>, we obtain the eigenvector <span class="math inline">\(x = (x_1, -i x_2)^t\)</span>. Normalization of <span class="math inline">\(x\)</span> imposes <span class="math display">\[||x|| = \sqrt{|x_1|^2 + |-ix_1|^2} = \sqrt{|x_1|^2 + |x_1|^2} = \sqrt{2}|x_1| = 1.\]</span> As in the previous example, we can choose <span class="math inline">\(x_1 = 1/\sqrt{2}.\)</span></p>
<p>The second eigenvectors, associated <span class="math inline">\(\lambda_2 = -i\)</span>, solves the eigensystem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
- (-i) &amp;  -1 \\
1      &amp;  - (-i) 
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
i  &amp; -1 \\
1  &amp;  i
\end{pmatrix}
\begin{pmatrix}
y_1  \\
y_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>The first row yields <span class="math inline">\(iy_1 - y_2 = 0\)</span>, so <span class="math inline">\(y = (y_1, iy_2)^t\)</span>. A normalized eigenvector can be <span class="math inline">\(y = (1/\sqrt{2}, i/\sqrt{2})^t\)</span>. We could also have chosen <span class="math inline">\(y = (i/\sqrt{2}, -1/\sqrt{2})^t\)</span>.</p>
<p><strong>Example </strong> Repeated eigenvalues 1</p>
<p>The matrix <span class="math display">\[\begin{pmatrix}
-1  &amp;   0 \\
 2  &amp;  -1 
\end{pmatrix}\]</span></p>
<p>is lower-trianglar, with repeated eigenvalues on the diagonal, <span class="math inline">\(\lambda_{1,2} = -1\)</span>. The eigenvectors associated with <span class="math inline">\(-1\)</span> satisfy the eigenproblem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (-1) &amp;  0 \\
 2      &amp;  -1 - (-1) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
 0  &amp;  0 \\
 2 &amp;   0
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span></p>
<p>The first row vanishes, and the second row means that <span class="math inline">\(x_1 = 0\)</span>, leaving for instance <span class="math inline">\(x_2 = 1\)</span>, and <span class="math inline">\(x = (0,1)^t\)</span>. There are no other linearly independent eigenvectors. This is not always the case, repeated eigenvalues can have more than one independent eigenvector, as in the next example.</p>
<p><strong>Example </strong> Repeated eigenvalues 2</p>
<p>The matrix <span class="math display">\[\begin{pmatrix}
-1  &amp;   0 \\
 0  &amp;  -1 
\end{pmatrix}\]</span> is diagonal, with repeated eigenvalues on the diagonal, <span class="math inline">\(\lambda_{1,2} = -1\)</span>. The eigenvectors associated with <span class="math inline">\(-1\)</span> satisfy the eigenproblem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
-1 - (-1) &amp;  0 \\
 0      &amp;  -1 - (-1) 
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}, \\ 
\begin{pmatrix}
 0  &amp;  0 \\
 0 &amp;   0
\end{pmatrix}
\begin{pmatrix}
x_1  \\
x_2  \\
\end{pmatrix}
 &amp; = 
\begin{pmatrix}
0  \\
0  \\
\end{pmatrix}.\end{aligned}\]</span> Now, the two rows vanished, leaving no condition at all on <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. This means that all the vectors are eigenvectors! How many linearly independent eigenvectors can we find? Vectors of size 2 live in a vector space of dimension 2; we can find at most 2 linearly independent vectors. We can choose for instance the canonical basis: <span class="math inline">\(x = (1,0)^t\)</span> and <span class="math inline">\(y = (0,1)^t\)</span>.</p>
<hr />
<p><strong>Tips on eigenvalue decomposition</strong></p>
<ul>
<li><p>A <span class="math inline">\(2 \times 2\)</span> matrix (or any square matrix) admits an eigenvalue decomposition if all the eigenvalues are distinct. For <span class="math inline">\(2 \times 2\)</span> matrices, eigenvalues are distinct if and only if the discriminant <span class="math inline">\(\Delta \neq 0\)</span>.</p></li>
<li><p>If the matrix has a repeated eigenvalue, it will admit an eigenvalue decomposition if the number of (linearly independent) eigenvectors is equal to the number of times the eigenvalue is repeated. The number of eigenvectors is called geometric multiplicity, and the number of repeats is called algebraic multiplicity.</p></li>
<li><p>The eigenproblem should be underdetermined; you should always be able to eliminate at least one row by linear combination. If you cannot, this means that there is a error, possibly an incorrect eigenvalue, or a arithmetic mistake in computing <span class="math inline">\(A - \lambda I\)</span>.</p></li>
<li><p>Because eigenvalues are in general complex, the eigenvectors will also be complex.</p></li>
<li><p>The eigenvector matrix <span class="math inline">\(X\)</span> needs to be inverted. When the eigenvectors can be chosen so that they are orthogonal and normalized, the inverse <span class="math inline">\(X^{-1} = X^*\)</span> (i.e. the conjugate transpose of <span class="math inline">\(X\)</span>). Symmetric matrices have orthogonal eigenvalues, so this class of matrices are especially easy to diagonalise.</p></li>
<li><p>Eigenvalue decomposition and invertibility are two different concepts. A matrix can be invertible without admitting an eigenvalue decomposition, and vice versa.</p></li>
<li><p>When a matrix does not admit an eigenvalue decomposition, it still can be triangularised. One such triangularisation is the Jordan decomposition: <span class="math inline">\(A = P(D+S)P^{-1}\)</span>, where <span class="math inline">\(P\)</span> is invertible, <span class="math inline">\(D\)</span> is the diagonal matrix of eigenvalues, and <span class="math inline">\(S\)</span> is a <strong>nilpotent</strong> matrix, i.e. a nonzero matrix such that <span class="math inline">\(S^k = 0\)</span> for <span class="math inline">\(k \geq k_0 &gt; 1\)</span>.</p></li>
</ul>
<hr />
<h2 id="exercises-on-eigenvalues-decomposition">Exercises on eigenvalues decomposition</h2>
<p><strong>Exercice </strong> Find, if there is any, an eigenvalue decomposition of <span class="math display">\[A = \begin{pmatrix}
-1  &amp;   2 \\
 2  &amp;  -1 
\end{pmatrix}\]</span> To compute <span class="math inline">\(X^{-1}\)</span>, you can use the fact that because <span class="math inline">\(A\)</span> is real and symmetrical, the eigenvectors are orthogonal, meaning that <span class="math inline">\(X^{-1} = X^t\)</span>, if the eignevectors are normalized.</p>
<p><strong>Correction</strong> We have <span class="math inline">\(\det A = (-1)(-1) - (2)(2) = 1 - 4 = -3 &lt; 0,\)</span> <span class="math inline">\(\mathrm{tr}\,A = -1 -1 = -2,\)</span> and <span class="math inline">\(\Delta = (-2)^2 - 4 (-3) = 4 + 12 = 16.\)</span> The eigenvalues are <span class="math display">\[\lambda_{1,2} = \frac 12 \bigl( -2 \pm \sqrt{16} \bigr) = -1 \pm 2 = 1, -3.\]</span> The two eigenvalues are distinct, so the matrix <span class="math inline">\(A\)</span> is diagonalisable. The eigenvector associated with the eigenvalue <span class="math inline">\(\lambda_1 = 1\)</span> is solution to the eigenproblem <span class="math inline">\(\bigl( A - \lambda_1 I ) x = 0.\)</span> We look for a solution <span class="math display">\[\begin{aligned}
\begin{pmatrix}
  -1 - (1) &amp;  2 \\
  2  &amp;  -1 - (1)
\end{pmatrix}
\begin{pmatrix}
 x_1 \\
 x_2
\end{pmatrix}
 &amp; = 0, \\
\begin{pmatrix}
  -2 &amp;  2 \\
  2  &amp; -2
\end{pmatrix}
\begin{pmatrix}
 x_1 \\
 x_2
\end{pmatrix}
 &amp; = 0.\end{aligned}\]</span> (We check that the two rows are colinear.) The first row gives <span class="math inline">\(-2x_1 + 2x_2 = 0\)</span>, or <span class="math inline">\(x_1 = x_2\)</span>. The norm of the eigenvector <span class="math inline">\(x = (x_1, x_2)^t = \sqrt{x_1^2 + x_2^2} = \sqrt{2}|x_1|.\)</span> We choose<span class="math inline">\(x_1 = 1/\sqrt{2}\)</span> to have a normalized eigenvector. We know that the second eigenvector is orthogonal to <span class="math inline">\(x\)</span>, so we can take <span class="math inline">\(y = (1/\sqrt{2},-1/\sqrt{2})^t\)</span> for the eigenvector associated to <span class="math inline">\(\lambda_2 = -3\)</span>. To check that this is indeed an eigenvector, we solve to eigenproblem <span class="math display">\[\begin{aligned}
\begin{pmatrix}
  -1 - (-3) &amp;  2 \\
  2  &amp;  -1 - (-3)
\end{pmatrix}
\begin{pmatrix}
 y_1 \\
 y_2
\end{pmatrix}
 &amp; = 0, \\
\begin{pmatrix}
  2 &amp;  2 \\
  2  &amp; 2
\end{pmatrix}
\begin{pmatrix}
 y_1 \\
 y_2
\end{pmatrix}
 &amp; = 0.\end{aligned}\]</span> The solutions are vectors that satisfy <span class="math inline">\(y_1 = - y_2\)</span>; this is the case for <span class="math inline">\(y\)</span>. The matrix <span class="math inline">\(X\)</span> is composed of the column vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(X = \bigl( x | y \bigr)\)</span> and its inverse is <span class="math display">\[X^{-1} = X^t = \begin{pmatrix} x_1^t \\ \hline x_2^t \end{pmatrix}
  = 
  \frac{1}{\sqrt{2}}
  \begin{pmatrix} 
    1 &amp;  1 \\ 
    1 &amp; -1    
  \end{pmatrix}.\]</span></p>
<h1 id="linearisation-of-functions-mathbbr2-to-mathbbr2">Linearisation of functions <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span></h1>
<p>Nonlinear systems of ordinary differential equations are used to describe the <strong>dynamics</strong> (evolution in time) of concentration of biochemical species, population densities in ecological systems, of the electrophyiology of neurons.</p>
<p>Two dimensional systems are described by two ordinary differential equations (ODEs) <span class="math display">\[\begin{aligned}
\frac{dx_1}{dt} &amp; = f_1(x_1,x_2), \\
\frac{dx_2}{dt} &amp; = f_2(x_1,x_2), \\\end{aligned}\]</span> The variables <span class="math inline">\(x_1, x_2\)</span> are functions of time: <span class="math inline">\(x_1(t)\)</span>, <span class="math inline">\(x_2(t)\)</span>, and <span class="math inline">\(f_1, f_2\)</span> are the derivatives. We define the two-dimensional vectors <span class="math inline">\(\boldsymbol{x} = (x_1,x_2)^t\)</span> (here we will use <strong>bold</strong> for vectors), and <span class="math inline">\(\boldsymbol{f} = (f_1, f_2)^t\)</span>. The ODEs can now be represented in vector format, <span class="math display">\[\frac{d\boldsymbol{x}}{dt} = \boldsymbol{f}(\boldsymbol{x}).\]</span> Here we assume that there exists a point in the 2D plane <span class="math inline">\(\bar{\boldsymbol{x}}\)</span> such that the derivative <span class="math inline">\(\boldsymbol{f}(\bar{\boldsymbol{x}}) = 0.\)</span> This point is called a <strong>steady state</strong> because the derivatives are all zeros; the steady state is therefore a solution to the system of ODE.</p>
<p>We are interested in how <span class="math inline">\(\boldsymbol{f}\)</span> is behaving around the steady state. To do that we linearize the function <span class="math inline">\(\boldsymbol{f}\)</span> at the steady state. <strong>Linearisation</strong> is a first-order expansion. For a function from <span class="math inline">\(\mathbb{R}^2 \to \mathbb{R}^2\)</span>, a first-order expansion around a point <span class="math inline">\(\boldsymbol{x}_0\)</span> is <span class="math display">\[\begin{aligned}
\boldsymbol{f}(\boldsymbol{x}) \approx \boldsymbol{f}(\boldsymbol{x}_0) + 
  \boldsymbol{D}\boldsymbol{f}(\boldsymbol{x}_0) (\boldsymbol{x}-\boldsymbol{x}_0) \end{aligned}\]</span> When expanding around a steady state, the constant term <span class="math inline">\(\boldsymbol{f}(\bar{\boldsymbol{x}}) = 0\)</span>. In the second term, <span class="math inline">\(\boldsymbol{D}\boldsymbol{f}\)</span> is a <span class="math inline">\(2 \times 2\)</span> matrix, called the Jacobian matrix, and often denoted <span class="math inline">\(\boldsymbol{J}\)</span>. The <strong>Jacobian matrix</strong> for the function <span class="math inline">\(\boldsymbol{f}\)</span> is defined as <span class="math display">\[\begin{aligned}
\boldsymbol{J} = \boldsymbol{D}\boldsymbol{f} = 
\begin{pmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} 
\end{pmatrix}.\end{aligned}\]</span> When evaluated at a steady state, the Jacobian matrix can provide information on the dynamics of the nonlinear ODE system. More precisely, the eigenvalues of the Jacobian matrix can determine whether the steady state is stable (attracts solutions) or is unstable. <strong>Linearisation around a steady state means computing the Jacbian matrix at the steady state.</strong></p>
<p><strong>Example </strong> Linearisation around a steady state</p>
<p>The Lotka-Volterra equations is a classical ODE system mathematical biology. The equations reads <span class="math display">\[\begin{aligned}
\frac{dx}{dt} = ax - xy, \\
\frac{dy}{dt} = xy - by, \\\end{aligned}\]</span> for <span class="math inline">\(a, b\)</span> positive constants. The solution vector is <span class="math inline">\(\boldsymbol{x} = (x,y)^t\)</span> and the derivatives are <span class="math inline">\(f_1(x,y) = ax - xy\)</span> and <span class="math inline">\(f_2 = xy - by\)</span>. We first look for steady states <span class="math display">\[f_1 = ax - xy = 0, \quad f_2 = x y - b y.\]</span> If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are not zero, we have <span class="math inline">\(x = b\)</span> and <span class="math inline">\(y = a\)</span>. If <span class="math inline">\(x = 0\)</span>, the second equation implies <span class="math inline">\(y = 0\)</span>. If <span class="math inline">\(y = 0\)</span>, the first equation implies <span class="math inline">\(x = 0\)</span>. Therefore there are two steady states, <span class="math inline">\(\bar{\boldsymbol{x}} = (b,a)^t\)</span> and <span class="math inline">\(\hat{\boldsymbol{x}} = (0,0)^t\)</span>.</p>
<p>We have the following derivatives <span class="math display">\[\begin{aligned}
\frac{\partial f_1}{\partial x} (x,y) &amp; = a - y, \\
\frac{\partial f_1}{\partial y} (x,y) &amp; = - x, \\
\frac{\partial f_2}{\partial x} (x,y) &amp; =   y, \\
\frac{\partial f_2}{\partial y} (x,y) &amp; = x - b, \\\end{aligned}\]</span> The Jacobian matrix is <span class="math display">\[\begin{aligned}
\boldsymbol{J} = 
\begin{pmatrix}
 a - y &amp; - x \\
  y    &amp; x - b 
\end{pmatrix}.\end{aligned}\]</span> Evaluated at the steady state <span class="math inline">\(\bar{\boldsymbol{x}} = (b,a)^t\)</span> and <span class="math inline">\(\hat{\boldsymbol{x}} = (0,0)^t\)</span>, the Jacobian matrices are <span class="math display">\[\begin{aligned}
\boldsymbol{J}(\bar{\boldsymbol{x}}) = 
\begin{pmatrix}
        0  &amp; - b \\
        a  &amp;   0 
\end{pmatrix}, \quad
\boldsymbol{J}(\hat{\boldsymbol{x}}) = 
\begin{pmatrix}
        a  &amp;  0  \\
        0  &amp; -b 
\end{pmatrix}.\end{aligned}\]</span></p>
<h2 id="exercises-on-linearisation">Exercises on linearisation </h2>
<p><strong>Exercice </strong> Let the function <span class="math inline">\(\boldsymbol{f} = (f_1, f_2)^t\)</span>, with <span class="math display">\[f_1(x,y) = -d x + x \exp(-a xy), \quad f_2(x,y) = x - y,\]</span> <span class="math inline">\(d &lt; 1\)</span>, <span class="math inline">\(a, d\)</span> positive. Find the steady states (by solving the equations <span class="math inline">\(f_1 = 0, f_2 = 0\)</span>). Compute the Jacobian matrix, and evaluate the Jacobian matrix at each steady state.</p>
<p><strong>Correction</strong> The steady states are found by solving <span class="math display">\[\begin{aligned}
  f_1(x,y) = -d x + x \exp(-a xy) &amp; = 0, \\
  f_2(x,y) = x - y &amp; = 0.\end{aligned}\]</span> From the second equation, we have <span class="math inline">\(x = y\)</span>. The first equation is equivalent to <span class="math inline">\(d x = x \exp( - a xy).\)</span> We need to distinguish two cases: (i) <span class="math inline">\(x = 0\)</span>, and (ii) <span class="math inline">\(x \neq 0\)</span>. Case (i) leads to the solution <span class="math inline">\(\boldsymbol{x}^* = (0, 0)^t\)</span>, our first steady state. Case (ii) means that we can simplify <span class="math inline">\(x\)</span> in the first equation: <span class="math inline">\(d = \exp(- a xy).\)</span> Replacing <span class="math inline">\(y = x\)</span>, and solving for <span class="math inline">\(x\)</span>: <span class="math display">\[\begin{aligned}
  d &amp; = \exp(-a x y), \\
  d &amp; = \exp(-a x^2), \\
  \ln d &amp; = - a x^2, \\
  - \frac{\ln d}{a} &amp; = x^2, \quad (a &gt; 0) \end{aligned}\]</span> The hypothesis <span class="math inline">\(d&lt;1\)</span> ensures that <span class="math inline">\(\ln d &lt; 0\)</span> and <span class="math inline">\(- \ln d &gt; 0\)</span>. There are therefore two real solutions for <span class="math inline">\(x\)</span>: <span class="math display">\[x = \pm \sqrt{- \frac{\ln d}{a}}.\]</span> The two additional steady states are <span class="math display">\[\bar{\boldsymbol{x}}_{1,2} =  
\begin{pmatrix}
  \pm \sqrt{- \frac{\ln d}{a}} \\  \pm \sqrt{- \frac{\ln d}{a}}
\end{pmatrix}.\]</span></p>
<p>The Jacobian matrix of <span class="math inline">\(\boldsymbol{f}\)</span> is computed from the partial derivatives <span class="math display">\[\begin{aligned}
  \frac{\partial f_1}{\partial x} (x,y) &amp; = -d + (-a y) \exp ( - axy ), \\
  \frac{\partial f_1}{\partial y} (x,y) &amp; = (-ax) \exp ( -axy ), \\
  \frac{\partial f_2}{\partial x} (x,y) &amp; = 1,     \\
  \frac{\partial f_2}{\partial y} (x,y) &amp; = -1.    \\
  \boldsymbol{J} &amp; = 
  \begin{pmatrix}
    -d - a y \exp ( - axy ) &amp; -ax \exp ( -axy ) \\
     1  &amp; -1
  \end{pmatrix}.\end{aligned}\]</span> The function <span class="math inline">\(f_2\)</span> is linear. This is reflected in the Jacobian matrix, which as constant coefficients on the second row. The evaluation of the Jacobian matrix at steady state <span class="math inline">\(\boldsymbol{x}^* = (0,0)^t\)</span> is <span class="math display">\[\boldsymbol{J}(\boldsymbol{x}^*) = 
  \begin{pmatrix}
    -d  &amp;  0 \\
     1  &amp; -1
  \end{pmatrix}.\]</span> The evaluation of Jacobian matrix at steady state <span class="math inline">\(\bar{\boldsymbol{x}}_1 = \bigl( \sqrt{- \frac{\ln d}{a}}, \sqrt{- \frac{\ln d}{a}} \bigr)^t\)</span> is <span class="math display">\[\boldsymbol{J}(\bar{\boldsymbol{x}}_1) = 
  \begin{pmatrix}
    -d - a \bar y_1 \exp( -a \bar x_1 \bar y_1 ) &amp;  - a \bar x_1 \exp ( - a \bar x_1 \bar y_1 ) \\
     1  &amp; -1
  \end{pmatrix}.\]</span> Here, we use the fact that steady states satisfy the equation <span class="math inline">\(\exp( -a xy ) = d\)</span> to simplify the exponential terms <span class="math display">\[\boldsymbol{J}(\bar{\boldsymbol{x}}_1) = 
  \begin{pmatrix}
    -d - a \bar y_1 d &amp;  - a \bar x_1 d \\
     1  &amp; -1
  \end{pmatrix}.\]</span> Replacing <span class="math inline">\(y_1\)</span> and <span class="math inline">\(x_1\)</span> by <span class="math inline">\(\sqrt{- \frac{\ln d}{a}}\)</span>, we obtain <span class="math display">\[\begin{aligned}
  \boldsymbol{J}(\bar{\boldsymbol{x}}_1) &amp; = 
  \begin{pmatrix}
    -d - a  y_1 d &amp;  - a \bar x_1 d \\
     1  &amp; -1
  \end{pmatrix}, \\
  &amp; = 
  \begin{pmatrix}
    -d - a \sqrt{- \frac{\ln d}{a}} d &amp;  - a \sqrt{- \frac{\ln d}{a}} d \\
     1  &amp; -1
  \end{pmatrix}, \\
  &amp; = 
  \begin{pmatrix}
    -d \Bigl( 1 + a \sqrt{- \frac{\ln d}{a}} \Bigr) &amp;  - d \sqrt{- a^2 \frac{\ln d}{a}}  \\
     1  &amp; -1
  \end{pmatrix}, \\
  &amp; = 
  \begin{pmatrix}
    -d \Bigl( 1 + \sqrt{- a \ln d} \Bigr) &amp;  - d \sqrt{- a \ln d}  \\
     1  &amp; -1
  \end{pmatrix}.\end{aligned}\]</span> The same lines of calculations for the steady state <span class="math inline">\(\bar{\boldsymbol{x}}_2\)</span> leads to <span class="math display">\[\begin{aligned}
  \boldsymbol{J}(\bar{\boldsymbol{x}}_2) &amp; = 
  \begin{pmatrix}
    -d \Bigl( 1 - \sqrt{- a \ln d} \Bigr) &amp;   d \sqrt{- a \ln d}  \\
     1  &amp; -1
  \end{pmatrix}.\end{aligned}\]</span></p>
<p><strong>Exercice </strong> Compute the Jacobian matrices of each of the following functions of <span class="math inline">\((x,y)\)</span>. All parameters are constants. You do not need to compute the steady states just the matrices.</p>
<ul>
<li><p>van der Pol oscillator <span class="math display">\[f_1(x,y) = \mu\bigl( (1-x^2)y - x \bigr), \; f_2(x,y) = y.\]</span></p></li>
<li><p>Two-compartment pharmacokinetics <span class="math display">\[f_1(x,y) = a - k_{12} x + k_{21} y - k_1 x, \; f_2(x,y) = k_{12} x - k_{21} y.\]</span></p></li>
<li><p>SI epidemiological model <span class="math display">\[f_1(x,y) = - \beta x y, \; f_2(x,y) = \beta x y - \gamma y.\]</span></p></li>
</ul>
<h1 id="solution-of-systems-of-linear-differential-equations-in-dimension-2">Solution of systems of linear differential equations in dimension 2</h1>
<p>Linear differential equations have linear derivative parts, which can be represented in matrix-vector format <span class="math display">\[\frac{d\boldsymbol{x}(t)}{dt} = \boldsymbol{A} \boldsymbol{x}(t),\]</span> for a vector <span class="math inline">\(\boldsymbol{x}\)</span> square matrix <span class="math inline">\(\boldsymbol{A}\)</span>. For initial conditions <span class="math inline">\(\boldsymbol{x}(t) = \boldsymbol{x}_0\)</span>, the <strong>solution of the linear system of ODEs</strong> is <span class="math display">\[\begin{aligned}
\boldsymbol{x}(t) = e^{\boldsymbol{A}t} \boldsymbol{x}_0.\end{aligned}\]</span> If we have at our disposal an eigenvalue decomposition of <span class="math inline">\(\boldsymbol{A} = \boldsymbol{X} \boldsymbol{D} \boldsymbol{X}^{-1}\)</span>, the <strong>exponential of the matrix</strong> is <span class="math display">\[\begin{aligned}
e^{\boldsymbol{A}t} &amp; = \boldsymbol{X} e^{\boldsymbol{D}t} \boldsymbol{X}^{-1}, \\
                    &amp; = \boldsymbol{X} 
                    \begin{pmatrix}
                      e^{ \lambda_1 t } &amp; 0 \\
                      0 &amp; e^{ \lambda_2 t }   
                    \end{pmatrix}
                    \boldsymbol{X}^{-1}.\end{aligned}\]</span> Therefore, the long-time behavior of the exponential is controlled by the eigenvalues <span class="math inline">\(\lambda_{1,2}\)</span>.</p>
<p><strong>Example </strong> Solution of a linear system of ODEs</p>
<p>Consider the linear system of ODEs given by the Lotka-Volterra model linearised at its nonzero steady state <span class="math inline">\(\bar{\boldsymbol{x}} = (b,a)^t\)</span> is <span class="math display">\[\begin{aligned}
\begin{pmatrix}
\frac{dx}{dt}  \\
\frac{dy}{dt}  
\end{pmatrix}
=
\begin{pmatrix}
   0 &amp; - b \\
   a &amp;  0 
\end{pmatrix}
\begin{pmatrix}
x  \\
y  
\end{pmatrix}, \quad 
\begin{pmatrix}
x(0)  \\
y(0)  
\end{pmatrix}
=
\begin{pmatrix}
x_0  \\
y_0  
\end{pmatrix}.
\label{eq_linearequation}\end{aligned}\]</span> This system approximates the nonlinear version near the steady state. In this linear system, variables <span class="math inline">\((x,y)\)</span> are deviations from the steady state; their solutions are "centered" around 0. To solve this linear system, we will diagonalise the matrix <span class="math display">\[\begin{aligned}
A = \begin{pmatrix}
  0 &amp; - b \\
  a &amp;  0 
\end{pmatrix}.\end{aligned}\]</span> The goal is to go slowly through every step once for this system. In general it is not necessary to solve the system completely by hand; knowledge of the eigenvalues is often sufficient in many applications.</p>
<p>We have <span class="math inline">\(\det A = 0 - a(-b) = ab &gt; 0\)</span>, <span class="math inline">\(\mathrm{tr}\,A = 0\)</span> and <span class="math inline">\(\Delta = 0 - 4ab = -4ab &lt; 0\)</span>. The eigenvalues are therefore complex conjugates: <span class="math inline">\(\lambda_{1,2} = \pm i \sqrt{ab}.\)</span> Distinct eigenvalues means that <span class="math inline">\(A\)</span> is diagonalisable. The eigenvector associated to <span class="math inline">\(\lambda_1 = i\sqrt{ab}\)</span> is given by the system <span class="math display">\[\begin{aligned}
\Biggl( 
\begin{array}{cc|c}
-i\sqrt{ab} &amp; -b          &amp; 0 \\
    a       &amp; -i\sqrt{ab} &amp; 0
\end{array}
\Biggr)\end{aligned}\]</span> We have from the first row <span class="math inline">\(-i\sqrt{ab} x = b y\)</span>. Letting <span class="math inline">\(x = b\)</span> and <span class="math inline">\(y = -i\sqrt{ab}\)</span>, we obtain the non-normalized eigenvector <span class="math inline">\(\tilde x_1 = (b, -i\sqrt{ab})^t\)</span>. Normalization is done by dividing by <span class="math display">\[||\tilde x || = \sqrt{b^2 + (-i\sqrt{ab})^2} = \sqrt{b^2 + ab},\]</span> to obtain the first eigenvector <span class="math display">\[x = 
\begin{pmatrix} 
\frac{b}{\sqrt{b^2 + ab}} \\
\frac{-i\sqrt{ab}}{\sqrt{b^2 + ab}}
\end{pmatrix}
=
\begin{pmatrix} 
\frac{b}{\sqrt{b}\sqrt{b + a}} \\
\frac{-i\sqrt{a}\sqrt{b}}{\sqrt{b}\sqrt{b + a}}
\end{pmatrix}
=
\begin{pmatrix} 
\frac{\sqrt{b}}{\sqrt{b + a}} \\
\frac{-i\sqrt{a}}{\sqrt{b + a}}
\end{pmatrix}.\]</span> The second eigenvector is computed the same way (watch out for the slightly different signs!). The eigenproblem for the eigenvalue <span class="math inline">\(\lambda = -i\sqrt{ab}\)</span> is <span class="math display">\[\begin{aligned}
\Biggl( 
\begin{array}{cc|c}
+i\sqrt{ab} &amp; -b          &amp; 0 \\
    a       &amp; +i\sqrt{ab} &amp; 0
\end{array}
\Biggr)\end{aligned}\]</span> Given that the only change is <span class="math inline">\(-i \to +i\)</span>, the second eigenvector is <span class="math display">\[x_2 = 
\begin{pmatrix} 
\frac{\sqrt{b}}{\sqrt{b + a}} \\
\frac{i\sqrt{a}}{\sqrt{b + a}}
\end{pmatrix}.\]</span> The solution to the linear ODE is <span class="math display">\[\begin{pmatrix}
x(t) \\ y(t)
\end{pmatrix}
=
\boldsymbol{X} e^{\boldsymbol{D}t} \boldsymbol{X}^{-1} 
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix},\]</span> with <span class="math display">\[\begin{aligned}
\boldsymbol{X} =
\frac{1}{\sqrt{b + a}}
\begin{pmatrix}
\sqrt{b}  &amp; \sqrt{b} \\
-i\sqrt{a} &amp; i\sqrt{a}
\end{pmatrix}, \quad
\boldsymbol{D} = 
\begin{pmatrix}
+i\sqrt{ab}  &amp; 0 \\
 0 &amp; -i\sqrt{ab}
\end{pmatrix}\end{aligned}\]</span> The <strong>inverse of a <span class="math inline">\(2 \times 2\)</span> matrix</strong> with coefficients <span class="math inline">\(a,b,c,d\)</span> is <span class="math display">\[\begin{aligned}
\begin{pmatrix}
a  &amp; b \\
c &amp;  d
\end{pmatrix}^{-1} =
\frac{1}{ad-bc}
\begin{pmatrix}
d  &amp; -b \\
-c &amp;  a
\end{pmatrix}.\end{aligned}\]</span> This is conditional to <span class="math inline">\(\det = ad - bc \neq 0\)</span>, of course. With this formula, the inverse of <span class="math inline">\(\boldsymbol{X}\)</span> is <span class="math display">\[\boldsymbol{X}^{-1} =
\frac{1}{\sqrt{b + a}} \frac{1}{\det \boldsymbol{X}}
\begin{pmatrix}
i \sqrt{a} &amp; -\sqrt{b} \\
i \sqrt{a} &amp; \sqrt{b}
\end{pmatrix}.\]</span> The determinant <span class="math inline">\(\det \boldsymbol{X} = \frac{i \sqrt{b} \sqrt{a}}{b + a} + \frac{i \sqrt{a} \sqrt{b}}{b + a} = 2i \frac{\sqrt{ab}}{b + a}.\)</span> The inverse reduces to <span class="math display">\[\frac{1}{\sqrt{b + a}} \frac{a + b}{2i\sqrt{ab}}
\begin{pmatrix}
i \sqrt{a} &amp; -\sqrt{b} \\
i \sqrt{a} &amp; \sqrt{b}
\end{pmatrix} 
=
\frac{-i \sqrt{b + a}}{2\sqrt{ab}}
\begin{pmatrix}
i \sqrt{a} &amp; -\sqrt{b} \\
i \sqrt{a} &amp; \sqrt{b}
\end{pmatrix} 
=
\frac{\sqrt{b + a}}{2\sqrt{ab}}
\begin{pmatrix}
 \sqrt{a} &amp;  i\sqrt{b} \\
 \sqrt{a} &amp; -i\sqrt{b}
\end{pmatrix}.\]</span> We have now obtained the eigenvalue decompostion of <span class="math inline">\(\boldsymbol{A} = \boldsymbol{X} \boldsymbol{D} \boldsymbol{X}^{-1}\)</span>. To solve the linear ODE, we need to compute the product <span class="math display">\[\begin{aligned}
\begin{pmatrix}
x(t) \\ y(t)
\end{pmatrix}
 &amp; =
\boldsymbol{X} e^{\boldsymbol{D}t} \boldsymbol{X}^{-1} 
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}, \\
 &amp; =
\frac{1}{\sqrt{b + a}}
\begin{pmatrix}
\sqrt{b}  &amp; \sqrt{b} \\
-i\sqrt{a} &amp; i\sqrt{a}
\end{pmatrix}
\begin{pmatrix}
e^{+i\sqrt{ab}t}  &amp; 0 \\
 0 &amp; e^{-i\sqrt{ab}t}
\end{pmatrix}
\frac{\sqrt{b + a}}{2\sqrt{ab}}
\begin{pmatrix}
 \sqrt{a} &amp;  i\sqrt{b} \\
 \sqrt{a} &amp; -i\sqrt{b}
\end{pmatrix}
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}, \\
 &amp; =
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  &amp; \sqrt{b} \\
-i\sqrt{a} &amp; i\sqrt{a}
\end{pmatrix}
\begin{pmatrix}
e^{+i\sqrt{ab}t}  &amp; 0 \\
 0 &amp; e^{-i\sqrt{ab}t}
\end{pmatrix}
\begin{pmatrix}
 \sqrt{a} &amp;  i\sqrt{b} \\
 \sqrt{a} &amp; -i\sqrt{b}
\end{pmatrix}
\begin{pmatrix} x_0 \\ y_0 \end{pmatrix}, \\
 &amp; =
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b} e^{+i\sqrt{ab}t}  &amp; \sqrt{b} e^{-i\sqrt{ab}t}\\
-i\sqrt{a} e^{+i\sqrt{ab}t}  &amp; i\sqrt{a} e^{-i\sqrt{ab}t}
\end{pmatrix}
\begin{pmatrix}
 \sqrt{a}x_0+i\sqrt{b}y_0 \\
 \sqrt{a}x_0-i\sqrt{b}y_0
\end{pmatrix}. \\\end{aligned}\]</span> To simplify the last steps of the calculation, we will introduce the following notation. Using Euler’s formula, <span class="math inline">\(e^{\pm i\sqrt{ab}t} = \cos(\sqrt{ab}t) \pm i \sin(\sqrt{ab}t)\)</span>. Let <span class="math inline">\(c = \cos(\sqrt{ab}t)\)</span>, <span class="math inline">\(s = \sin(\sqrt{ab}t)\)</span>, and <span class="math inline">\(C_1 = \sqrt{a}x_0+i\sqrt{b}y_0\)</span>, <span class="math inline">\(C_2 = \sqrt{a}x_0-i\sqrt{b}y_0\)</span>. The solution reads <span class="math display">\[\begin{aligned}
\begin{pmatrix}
x(t) \\ y(t) 
\end{pmatrix} &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  e^{i\sqrt{ab}t} C_1 + \sqrt{b} e^{-i\sqrt{ab}t} C_2 \\
-i\sqrt{a}e^{i\sqrt{ab}t} C_1 + i\sqrt{a}e^{-i\sqrt{ab}t} C_2  
\end{pmatrix}, \\
 &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  (c + is) C_1 + \sqrt{b} (c - is) C_2 \\
-i\sqrt{a}(c + is) C_1 + i\sqrt{a}(c - is) C_2  
\end{pmatrix}, \\
 &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
\sqrt{b}  c (C_1 + C_2) + i \sqrt{b} s (C_1 - C_2) \\
\sqrt{a}  s (C_1 + C_2) + i \sqrt{a} c (-C_1 + C_2)  
\end{pmatrix}, \\
 &amp; = 
\frac{1}{2\sqrt{ab}}
\begin{pmatrix}
2 \sqrt{ab} \cos(\sqrt{ab}t) x_0 - 2 b \sin(\sqrt{ab}t) y_0 \\
2 a \sin(\sqrt{ab}t)x_0  + 2 \sqrt{ab} \cos(\sqrt{ab}t) y_0  
\end{pmatrix}, \\
 &amp; = 
\begin{pmatrix}
\cos(\sqrt{ab}t) x_0 - \sqrt{b/a} \sin(\sqrt{ab}t) y_0 \\
\sqrt{a/b} \sin(\sqrt{ab}t)x_0  +  \cos(\sqrt{ab}t) y_0  
\end{pmatrix}.\end{aligned}\]</span> And that’s it! We have obtained a solution to the linear ODE (Figure <a href="#f_sol_linear_ode" data-reference-type="ref" data-reference="f_sol_linear_ode">8</a>).</p>
<figure>
<img src="htmltmp/tikzblocks_25.pdf.png" id="f_sol_linear_ode" alt="Solution of the linear system of ODEs ([eq_linearequation]), with a = 0.1, b = 0.4. " /><figcaption aria-hidden="true">Solution of the linear system of ODEs (<a href="#eq_linearequation" data-reference-type="ref" data-reference="eq_linearequation">[eq_linearequation]</a>), with <span class="math inline">\(a = 0.1\)</span>, <span class="math inline">\(b = 0.4\)</span>. </figcaption>
</figure>
<h1 id="glossary">Glossary</h1>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">French</th>
<th style="text-align: left;">English</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">dérivable</td>
<td style="text-align: left;">differentiable</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">matrice jacobienne</td>
<td style="text-align: left;">Jacobian matrix</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">rang</td>
<td style="text-align: left;">rank</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">noyau</td>
<td style="text-align: left;">kernel</td>
<td style="text-align: left;">notation: <span class="math inline">\(\ker\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ensemble</td>
<td style="text-align: left;">set</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">espace vectoriel</td>
<td style="text-align: left;">vector space</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">sous-espace vectoriel</td>
<td style="text-align: left;">linear subspace</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">valeur propre</td>
<td style="text-align: left;">eigenvalue</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">vecteur propre</td>
<td style="text-align: left;">eigenvector</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">sous-espace propre</td>
<td style="text-align: left;">eigenspace</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">décomposition en valeurs propres</td>
<td style="text-align: left;">eigenvalue decomposition</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">décomposition en valeurs singulières</td>
<td style="text-align: left;">singular value decomposition</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">valeur singulière</td>
<td style="text-align: left;">singular value</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">trace</td>
<td style="text-align: left;">trace</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">déterminant</td>
<td style="text-align: left;">determinant</td>
<td style="text-align: left;"><span class="math inline">\(\det\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">base</td>
<td style="text-align: left;">basis</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">application linéaire</td>
<td style="text-align: left;">linear map</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">application</td>
<td style="text-align: left;">map</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">dimension</td>
<td style="text-align: left;">dimension</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">moindres carrés</td>
<td style="text-align: left;">least-squares</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">produit scalaire</td>
<td style="text-align: left;">scalar product</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Vect</td>
<td style="text-align: left;">Span</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">famille libre</td>
<td style="text-align: left;">linearly independent set</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">famille génératrice</td>
<td style="text-align: left;">spanning set</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>bernard@math.univ-lyon1.fr<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>pujo@math.univ-lyon1.fr<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
